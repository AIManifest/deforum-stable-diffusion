{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AIManifest/deforum-stable-diffusion/blob/main/Deforum_Stable_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByGXyiHZWM_q"
      },
      "source": [
        "# $ \\color{blue} {\\large \\textsf{Deforum Stable Diffusion v0.7}}$\n",
        "[Stable Diffusion](https://github.com/CompVis/stable-diffusion) by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer and the [Stability.ai](https://stability.ai/) Team. [K Diffusion](https://github.com/crowsonkb/k-diffusion) by [Katherine Crowson](https://twitter.com/RiversHaveWings). Notebook by [deforum](https://discord.gg/upmXXsrwZc)\n",
        "\n",
        "[Quick Guide](https://docs.google.com/document/d/1RrQv7FntzOuLg4ohjRZPVL7iptIyBhwwbcEYEW2OfcI/edit?usp=sharing) to Deforum v0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vohUiWo-I2HQ",
        "outputId": "2f615cd3-65f6-4c59-b875-81e6e3ae4a68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92mTesla T4, 15360 MiB, 15109 MiB\n"
          ]
        }
      ],
      "source": [
        "#@title $ \\color{blue} {\\large \\textsf{Environment Setup / NVIDIA GPU}}$\n",
        "import subprocess, os, sys\n",
        "from tqdm import tqdm\n",
        "from IPython import display\n",
        "sub_p_res = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.free', '--format=csv,noheader'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "print(f\"\\033[92m{sub_p_res[:-1]}\")\n",
        "import subprocess, time, gc, os, sys\n",
        "!pip install -qq -U kora\n",
        "def setup_environment():\n",
        "    start_time = time.time()\n",
        "    print_subprocess = False\n",
        "    use_xformers_for_colab = True\n",
        "    try:\n",
        "        ipy = get_ipython()\n",
        "    except:\n",
        "        ipy = 'could not get_ipython'\n",
        "    if 'google.colab' in str(ipy):\n",
        "        print(\"\\033[92m..setting up environment\")\n",
        "\n",
        "        # weird hack\n",
        "        import torch\n",
        "        \n",
        "        all_process = [\n",
        "            ['pip', 'install', 'facexlib>=0.2.5', 'gfpgan>=1.3.5', 'basicsr>=1.4.2', 'sk-video>=1.1.10', 'omegaconf', 'einops==0.4.1', 'pytorch-lightning==1.7.7', 'torchmetrics', 'transformers', 'safetensors', 'kornia'],\n",
        "            ['git', 'clone', 'https://github.com/AIManifest/deforum-stable-diffusion.git'],\n",
        "            ['git', 'clone', 'https://github.com/AIManifest/Practical-RIFE.git'],\n",
        "            ['git', 'clone', 'https://github.com/AIManifest/Real-ESRGAN.git'],\n",
        "            ['pip', 'install', '-qq', '--user', 'ffmpeg-python'],\n",
        "            ['pip', 'install', 'accelerate', 'ftfy', 'jsonmerge', 'matplotlib', 'resize-right', 'timm', 'torchdiffeq','scikit-learn','torchsde','open-clip-torch','numpngw'],\n",
        "        ]\n",
        "        for process in tqdm(all_process):\n",
        "            running = subprocess.run(process,stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "            if print_subprocess:\n",
        "                print(running)\n",
        "        with open('deforum-stable-diffusion/src/k_diffusion/__init__.py', 'w') as f:\n",
        "            f.write('')\n",
        "        sys.path.extend([\n",
        "            'deforum-stable-diffusion/',\n",
        "            'deforum-stable-diffusion/src',\n",
        "        ])\n",
        "        if use_xformers_for_colab:\n",
        "\n",
        "            print(\"\\033[92m..installing triton and xformers\")\n",
        "\n",
        "            all_process = [['pip', 'install', 'triton==2.0.0.dev20221202', 'xformers==0.0.16']]\n",
        "            for process in all_process:\n",
        "                running = subprocess.run(process,stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "                if print_subprocess:\n",
        "                    print(running)\n",
        "    else:\n",
        "        sys.path.extend([\n",
        "            'src'\n",
        "        ])\n",
        "    end_time = time.time()\n",
        "    print(f\"\\033[92m..environment set up in {end_time-start_time:.0f} seconds\")\n",
        "    return\n",
        "\n",
        "setup_environment()\n",
        "with open('deforum-stable-diffusion/src/k_diffusion/__init__.py', 'w') as f:\n",
        "  f.write('')\n",
        "sys.path.extend([\n",
        "  'deforum-stable-diffusion/',\n",
        "  'deforum-stable-diffusion/src',\n",
        "])\n",
        "import torch\n",
        "import random\n",
        "import clip\n",
        "import yaml\n",
        "from IPython.display import display\n",
        "from ipywidgets import widgets\n",
        "from IPython import display\n",
        "import glob\n",
        "from types import SimpleNamespace\n",
        "from helpers.save_images import get_output_folder\n",
        "from helpers.settings import load_args\n",
        "from helpers.render import render_animation, render_input_video, render_image_batch, render_interpolation\n",
        "from helpers.model_load import make_linear_decode, load_model, get_model_output_paths\n",
        "from helpers.aesthetics import load_aesthetics_model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2LR-dAkoEHd8"
      },
      "outputs": [],
      "source": [
        "#@title $\\color{blue} {\\large \\textsf{Path Setup}}$\n",
        "\n",
        "def Root():\n",
        "    models_path = \"models\" #@param {type:\"string\"}\n",
        "    configs_path = \"configs\" #@param {type:\"string\"}\n",
        "    output_path = \"outputs\" #@param {type:\"string\"}\n",
        "    mount_google_drive = True #@param {type:\"boolean\"}\n",
        "    models_path_gdrive = \"/content/drive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion\" #@param {type:\"string\"}\n",
        "    output_path_gdrive = \"/content/drive/MyDrive/AI/StableDiffusion\" #@param {type:\"string\"}\n",
        "\n",
        "    #@markdown **Model Setup**\n",
        "    map_location = \"cuda\" #@param [\"cpu\", \"cuda\"]\n",
        "    model_config = \"custom\" #@param [\"custom\",\"v2-inference.yaml\",\"v2-inference-v.yaml\",\"v1-inference.yaml\"]\n",
        "    model_checkpoint =  \"custom\" #@param [\"custom\",\"v2-1_768-ema-pruned.ckpt\",\"v2-1_512-ema-pruned.ckpt\",\"768-v-ema.ckpt\",\"512-base-ema.ckpt\",\"Protogen_V2.2.ckpt\",\"v1-5-pruned.ckpt\",\"v1-5-pruned-emaonly.ckpt\",\"sd-v1-4-full-ema.ckpt\",\"sd-v1-4.ckpt\",\"sd-v1-3-full-ema.ckpt\",\"sd-v1-3.ckpt\",\"sd-v1-2-full-ema.ckpt\",\"sd-v1-2.ckpt\",\"sd-v1-1-full-ema.ckpt\",\"sd-v1-1.ckpt\", \"robo-diffusion-v1.ckpt\",\"wd-v1-3-float16.ckpt\"]\n",
        "    custom_config_path = \"\" #@param {type:\"string\"}\n",
        "    custom_checkpoint_path = \"\" #@param {type:\"string\"}\n",
        "    embeddings_dir = \"/content/drive/MyDrive/AI/embeddings\" #@param{type:'string'}\n",
        "    hypernetwork_dir = \"/content/drive/MyDrive/AI/hypernetworks\" #@param{type:'string'}\n",
        "    data_dir = \"/content/drive/MyDrive/sd/stable-diffusion-webui\" #@param{type:'string'}\n",
        "    #@markdown **Don't use yet, still working on it**\n",
        "    use_xformers = True #@param{type:'boolean'}\n",
        "    use_sub_quad_attention = False #@param{type:'boolean'}\n",
        "    use_split_attention_v1 = False #@param{type:'boolean'}\n",
        "    use_split_cross_attention_forward_invokeAI = False #@param{type:'boolean'}\n",
        "    use_cross_attention_attnblock_forward = False #@param{type:'boolean'}\n",
        "\n",
        "    variables = locals()\n",
        "    with open(\"output.yaml\", \"w\") as file:\n",
        "      yaml.dump(variables, file)\n",
        "    return variables\n",
        "\n",
        "root = Root()\n",
        "root = SimpleNamespace(**root)\n",
        "\n",
        "root.models_path, root.output_path = get_model_output_paths(root)\n",
        "root.model, root.device = load_model(root, load_on_run_all=True, check_sha256=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DwoczrtsikSK"
      },
      "outputs": [],
      "source": [
        "#@title $\\color{blue} {\\large \\textsf{Load Textual Inversion!}}$\n",
        "model = root.model\n",
        "import os\n",
        "import sys\n",
        "import traceback\n",
        "import inspect\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "import tqdm\n",
        "import html\n",
        "import datetime\n",
        "import csv\n",
        "import safetensors.torch\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, PngImagePlugin\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "root.model_checkpoint = \"/content/drive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/Deliberate_Nova_Dream.ckpt\"\n",
        "checkpoint_name = os.path.splitext(os.path.basename(root.model_checkpoint))\n",
        "\n",
        "import hashlib\n",
        "print(\"..checking sha256\")\n",
        "with open(root.model_checkpoint, \"rb\") as f:\n",
        "  bytes = f.read() \n",
        "  ckpt_hash = hashlib.sha256(bytes).hexdigest()\n",
        "\n",
        "#learn_rate_scheduler\n",
        "\n",
        "import tqdm\n",
        "\n",
        "def autocast(disable=False):\n",
        "    \n",
        "    if disable:\n",
        "        return contextlib.nullcontext()\n",
        "\n",
        "    if dtype == torch.float32 or precision == \"full\":\n",
        "        return contextlib.nullcontext()\n",
        "\n",
        "    return torch.autocast(\"cuda\")\n",
        "    \n",
        "class LearnScheduleIterator:\n",
        "    def __init__(self, learn_rate, max_steps, cur_step=0):\n",
        "        \"\"\"\n",
        "        specify learn_rate as \"0.001:100, 0.00001:1000, 1e-5:10000\" to have lr of 0.001 until step 100, 0.00001 until 1000, and 1e-5 until 10000\n",
        "        \"\"\"\n",
        "        print(learn_rate)\n",
        "        pairs = learn_rate.split(',')\n",
        "        self.rates = []\n",
        "        self.it = 0\n",
        "        self.maxit = 0\n",
        "        try:\n",
        "            for i, pair in enumerate(pairs):\n",
        "                if not pair.strip():\n",
        "                    continue\n",
        "                tmp = pair.split(':')\n",
        "                if len(tmp) == 2:\n",
        "                    step = int(tmp[1])\n",
        "                    if step > cur_step:\n",
        "                        self.rates.append((float(tmp[0]), min(step, max_steps)))\n",
        "                        self.maxit += 1\n",
        "                        if step > max_steps:\n",
        "                            return\n",
        "                    elif step == -1:\n",
        "                        self.rates.append((float(tmp[0]), max_steps))\n",
        "                        self.maxit += 1\n",
        "                        return\n",
        "                else:\n",
        "                    self.rates.append((float(tmp[0]), max_steps))\n",
        "                    self.maxit += 1\n",
        "                    return\n",
        "            assert self.rates\n",
        "        except (ValueError, AssertionError):\n",
        "            raise Exception('Invalid learning rate schedule. It should be a number or, for example, like \"0.001:100, 0.00001:1000, 1e-5:10000\" to have lr of 0.001 until step 100, 0.00001 until 1000, and 1e-5 until 10000.')\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.it < self.maxit:\n",
        "            self.it += 1\n",
        "            return self.rates[self.it - 1]\n",
        "        else:\n",
        "            raise StopIteration\n",
        "\n",
        "\n",
        "class LearnRateScheduler:\n",
        "    def __init__(self, learn_rate, max_steps, cur_step=0, verbose=True):\n",
        "        self.schedules = LearnScheduleIterator(learn_rate, max_steps, cur_step)\n",
        "        (self.learn_rate,  self.end_step) = next(self.schedules)\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f'Training at rate of {self.learn_rate} until step {self.end_step}')\n",
        "\n",
        "        self.finished = False\n",
        "\n",
        "    def step(self, step_number):\n",
        "        if step_number < self.end_step:\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            (self.learn_rate, self.end_step) = next(self.schedules)\n",
        "        except StopIteration:\n",
        "            self.finished = True\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def apply(self, optimizer, step_number):\n",
        "        if not self.step(step_number):\n",
        "            return\n",
        "\n",
        "        if self.verbose:\n",
        "            tqdm.tqdm.write(f'Training at rate of {self.learn_rate} until step {self.end_step}')\n",
        "\n",
        "        for pg in optimizer.param_groups:\n",
        "            pg['lr'] = self.learn_rate\n",
        "\n",
        "#dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import PIL\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from torchvision import transforms\n",
        "from collections import defaultdict\n",
        "from random import shuffle, choices\n",
        "\n",
        "import random\n",
        "import tqdm\n",
        "import re\n",
        "\n",
        "from ldm.modules.distributions.distributions import DiagonalGaussianDistribution\n",
        "\n",
        "re_numbers_at_start = re.compile(r\"^[-\\d]+\\s*\")\n",
        "\n",
        "\n",
        "class DatasetEntry:\n",
        "    def __init__(self, filename=None, filename_text=None, latent_dist=None, latent_sample=None, cond=None, cond_text=None, pixel_values=None):\n",
        "        self.filename = filename\n",
        "        self.filename_text = filename_text\n",
        "        self.latent_dist = latent_dist\n",
        "        self.latent_sample = latent_sample\n",
        "        self.cond = cond\n",
        "        self.cond_text = cond_text\n",
        "        self.pixel_values = pixel_values\n",
        "\n",
        "dataset_filename_join_string= \" \"\n",
        "dataset_filename_word_regex = \"\"\n",
        "class PersonalizedBase(Dataset):\n",
        "    def __init__(self, data_root, width, height, repeats, flip_p=0.5, placeholder_token=\"*\", model=None, cond_model=None, device=None, template_file=None, include_cond=False, batch_size=1, gradient_step=1, shuffle_tags=False, tag_drop_out=0, latent_sampling_method='once', varsize=False):\n",
        "        re_word = re.compile(dataset_filename_word_regex) if len(dataset_filename_word_regex) > 0 else None\n",
        "\n",
        "        self.placeholder_token = placeholder_token\n",
        "\n",
        "        self.flip = transforms.RandomHorizontalFlip(p=flip_p)\n",
        "\n",
        "        self.dataset = []\n",
        "\n",
        "        with open(template_file, \"r\") as file:\n",
        "            lines = [x.strip() for x in file.readlines()]\n",
        "\n",
        "        self.lines = lines\n",
        "\n",
        "        assert data_root, 'dataset directory not specified'\n",
        "        assert os.path.isdir(data_root), \"Dataset directory doesn't exist\"\n",
        "        assert os.listdir(data_root), \"Dataset directory is empty\"\n",
        "\n",
        "        self.image_paths = [os.path.join(data_root, file_path) for file_path in os.listdir(data_root)]\n",
        "\n",
        "        self.shuffle_tags = shuffle_tags\n",
        "        self.tag_drop_out = tag_drop_out\n",
        "        groups = defaultdict(list)\n",
        "\n",
        "        print(\"Preparing dataset...\")\n",
        "        for path in tqdm.tqdm(self.image_paths):\n",
        "            # if shared.state.interrupted:\n",
        "            #     raise Exception(\"interrupted\")\n",
        "            try:\n",
        "                image = Image.open(path).convert('RGB')\n",
        "                if not varsize:\n",
        "                    image = image.resize((width, height), PIL.Image.BICUBIC)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            text_filename = os.path.splitext(path)[0] + \".txt\"\n",
        "            filename = os.path.basename(path)\n",
        "\n",
        "            if os.path.exists(text_filename):\n",
        "                with open(text_filename, \"r\", encoding=\"utf8\") as file:\n",
        "                    filename_text = file.read()\n",
        "            else:\n",
        "                filename_text = os.path.splitext(filename)[0]\n",
        "                filename_text = re.sub(re_numbers_at_start, '', filename_text)\n",
        "                if re_word:\n",
        "                    tokens = re_word.findall(filename_text)\n",
        "                    filename_text = (dataset_filename_join_string or \"\").join(tokens)\n",
        "\n",
        "            npimage = np.array(image).astype(np.uint8)\n",
        "            npimage = (npimage / 127.5 - 1.0).astype(np.float32)\n",
        "\n",
        "            torchdata = torch.from_numpy(npimage).permute(2, 0, 1).to(device=device, dtype=torch.float32)\n",
        "            latent_sample = None\n",
        "\n",
        "            with torch.autocast(\"cuda\"):\n",
        "                latent_dist = model.encode_first_stage(torchdata.unsqueeze(dim=0))\n",
        "\n",
        "            if latent_sampling_method == \"once\" or (latent_sampling_method == \"deterministic\" and not isinstance(latent_dist, DiagonalGaussianDistribution)):\n",
        "                latent_sample = model.get_first_stage_encoding(latent_dist).squeeze().to(\"cuda\")\n",
        "                latent_sampling_method = \"once\"\n",
        "                entry = DatasetEntry(filename=path, filename_text=filename_text, latent_sample=latent_sample)\n",
        "            elif latent_sampling_method == \"deterministic\":\n",
        "                # Works only for DiagonalGaussianDistribution\n",
        "                latent_dist.std = 0\n",
        "                latent_sample = model.get_first_stage_encoding(latent_dist).squeeze().to(\"cuda\")\n",
        "                entry = DatasetEntry(filename=path, filename_text=filename_text, latent_sample=latent_sample)\n",
        "            elif latent_sampling_method == \"random\":\n",
        "                entry = DatasetEntry(filename=path, filename_text=filename_text, latent_dist=latent_dist)\n",
        "\n",
        "            if not (self.tag_drop_out != 0 or self.shuffle_tags):\n",
        "                entry.cond_text = self.create_text(filename_text)\n",
        "\n",
        "            if include_cond and not (self.tag_drop_out != 0 or self.shuffle_tags):\n",
        "                with autocast():\n",
        "                    entry.cond = cond_model([entry.cond_text]).to(\"cuda\").squeeze(0)\n",
        "            groups[image.size].append(len(self.dataset))\n",
        "            self.dataset.append(entry)\n",
        "            del torchdata\n",
        "            del latent_dist\n",
        "            del latent_sample\n",
        "\n",
        "        self.length = len(self.dataset)\n",
        "        self.groups = list(groups.values())\n",
        "        assert self.length > 0, \"No images have been found in the dataset.\"\n",
        "        self.batch_size = min(batch_size, self.length)\n",
        "        self.gradient_step = min(gradient_step, self.length // self.batch_size)\n",
        "        self.latent_sampling_method = latent_sampling_method\n",
        "\n",
        "        if len(groups) > 1:\n",
        "            print(\"Buckets:\")\n",
        "            for (w, h), ids in sorted(groups.items(), key=lambda x: x[0]):\n",
        "                print(f\"  {w}x{h}: {len(ids)}\")\n",
        "            print()\n",
        "\n",
        "    def create_text(self, filename_text):\n",
        "        text = random.choice(self.lines)\n",
        "        tags = filename_text.split(',')\n",
        "        if self.tag_drop_out != 0:\n",
        "            tags = [t for t in tags if random.random() > self.tag_drop_out]\n",
        "        if self.shuffle_tags:\n",
        "            random.shuffle(tags)\n",
        "        text = text.replace(\"[filewords]\", ','.join(tags))\n",
        "        text = text.replace(\"[name]\", self.placeholder_token)\n",
        "        return text\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        entry = self.dataset[i]\n",
        "        if self.tag_drop_out != 0 or self.shuffle_tags:\n",
        "            entry.cond_text = self.create_text(entry.filename_text)\n",
        "        if self.latent_sampling_method == \"random\":\n",
        "            entry.latent_sample = model.get_first_stage_encoding(entry.latent_dist).to(\"cuda\")\n",
        "        return entry\n",
        "\n",
        "\n",
        "class GroupedBatchSampler(Sampler):\n",
        "    def __init__(self, data_source: PersonalizedBase, batch_size: int):\n",
        "        super().__init__(data_source)\n",
        "\n",
        "        n = len(data_source)\n",
        "        self.groups = data_source.groups\n",
        "        self.len = n_batch = n // batch_size\n",
        "        expected = [len(g) / n * n_batch * batch_size for g in data_source.groups]\n",
        "        self.base = [int(e) // batch_size for e in expected]\n",
        "        self.n_rand_batches = nrb = n_batch - sum(self.base)\n",
        "        self.probs = [e%batch_size/nrb/batch_size if nrb>0 else 0 for e in expected]\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __iter__(self):\n",
        "        b = self.batch_size\n",
        "\n",
        "        for g in self.groups:\n",
        "            shuffle(g)\n",
        "\n",
        "        batches = []\n",
        "        for g in self.groups:\n",
        "            batches.extend(g[i*b:(i+1)*b] for i in range(len(g) // b))\n",
        "        for _ in range(self.n_rand_batches):\n",
        "            rand_group = choices(self.groups, self.probs)[0]\n",
        "            batches.append(choices(rand_group, k=b))\n",
        "\n",
        "        shuffle(batches)\n",
        "\n",
        "        yield from batches\n",
        "\n",
        "\n",
        "class PersonalizedDataLoader(DataLoader):\n",
        "    def __init__(self, dataset, latent_sampling_method=\"once\", batch_size=1, pin_memory=False):\n",
        "        super(PersonalizedDataLoader, self).__init__(dataset, batch_sampler=GroupedBatchSampler(dataset, batch_size), pin_memory=pin_memory)\n",
        "        if latent_sampling_method == \"random\":\n",
        "            self.collate_fn = collate_wrapper_random\n",
        "        else:\n",
        "            self.collate_fn = collate_wrapper\n",
        "\n",
        "\n",
        "class BatchLoader:\n",
        "    def __init__(self, data):\n",
        "        self.cond_text = [entry.cond_text for entry in data]\n",
        "        self.cond = [entry.cond for entry in data]\n",
        "        self.latent_sample = torch.stack([entry.latent_sample for entry in data]).squeeze(1)\n",
        "        #self.emb_index = [entry.emb_index for entry in data]\n",
        "        #print(self.latent_sample.device)\n",
        "\n",
        "    def pin_memory(self):\n",
        "        self.latent_sample = self.latent_sample.pin_memory()\n",
        "        return self\n",
        "\n",
        "def collate_wrapper(batch):\n",
        "    return BatchLoader(batch)\n",
        "\n",
        "class BatchLoaderRandom(BatchLoader):\n",
        "    def __init__(self, data):\n",
        "        super().__init__(data)\n",
        "\n",
        "    def pin_memory(self):\n",
        "        return self\n",
        "\n",
        "def collate_wrapper_random(batch):\n",
        "    return BatchLoaderRandom(batch)\n",
        "\n",
        "TextualInversionTemplate = namedtuple(\"TextualInversionTemplate\", [\"name\", \"path\"])\n",
        "textual_inversion_templates = {}\n",
        "\n",
        "class Embedding:\n",
        "    def __init__(self, vec, name, step=None):\n",
        "        self.vec = vec\n",
        "        self.name = name\n",
        "        self.step = step\n",
        "        self.shape = None\n",
        "        self.vectors = 0\n",
        "        self.cached_checksum = None\n",
        "        self.sd_checkpoint = None\n",
        "        self.sd_checkpoint_name = None\n",
        "        self.optimizer_state_dict = None\n",
        "        self.filename = None\n",
        "\n",
        "    def save(self, filename):\n",
        "        embedding_data = {\n",
        "            \"string_to_token\": {\"*\": 265},\n",
        "            \"string_to_param\": {\"*\": self.vec},\n",
        "            \"name\": self.name,\n",
        "            \"step\": self.step,\n",
        "            \"sd_checkpoint\": self.sd_checkpoint,\n",
        "            \"sd_checkpoint_name\": self.sd_checkpoint_name,\n",
        "        }\n",
        "\n",
        "        torch.save(embedding_data, filename)\n",
        "\n",
        "        save_optimizer_state=True\n",
        "        if save_optimizer_state and self.optimizer_state_dict is not None:\n",
        "            optimizer_saved_dict = {\n",
        "                'hash': self.checksum(),\n",
        "                'optimizer_state_dict': self.optimizer_state_dict,\n",
        "            }\n",
        "            torch.save(optimizer_saved_dict, filename + '.optim')\n",
        "\n",
        "    def checksum(self):\n",
        "        if self.cached_checksum is not None:\n",
        "            return self.cached_checksum\n",
        "\n",
        "        def const_hash(a):\n",
        "            r = 0\n",
        "            for v in a:\n",
        "                r = (r * 281 ^ int(v) * 997) & 0xFFFFFFFF\n",
        "            return r\n",
        "\n",
        "        self.cached_checksum = f'{const_hash(self.vec.reshape(-1) * 100) & 0xffff:04x}'\n",
        "        return self.cached_checksum\n",
        "\n",
        "\n",
        "class DirWithTextualInversionEmbeddings:\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        self.mtime = None\n",
        "\n",
        "    def has_changed(self):\n",
        "        if not os.path.isdir(self.path):\n",
        "            return False\n",
        "\n",
        "        mt = os.path.getmtime(self.path)\n",
        "        if self.mtime is None or mt > self.mtime:\n",
        "            return True\n",
        "\n",
        "    def update(self):\n",
        "        if not os.path.isdir(self.path):\n",
        "            return\n",
        "\n",
        "        self.mtime = os.path.getmtime(self.path)\n",
        "\n",
        "\n",
        "class EmbeddingDatabase:\n",
        "    def __init__(self):\n",
        "        self.ids_lookup = {}\n",
        "        self.word_embeddings = {}\n",
        "        self.skipped_embeddings = {}\n",
        "        self.expected_shape = -1\n",
        "        self.embedding_dirs = {}\n",
        "\n",
        "    def add_embedding_dir(self, path):\n",
        "        self.embedding_dirs[path] = DirWithTextualInversionEmbeddings(path)\n",
        "\n",
        "    def clear_embedding_dirs(self):\n",
        "        self.embedding_dirs.clear()\n",
        "\n",
        "    def register_embedding(self, embedding, model):\n",
        "        self.word_embeddings[embedding.name] = embedding\n",
        "\n",
        "        ids = model.cond_stage_model.tokenize([embedding.name])[0]\n",
        "\n",
        "        first_id = ids[0]\n",
        "        if first_id not in self.ids_lookup:\n",
        "            self.ids_lookup[first_id] = []\n",
        "\n",
        "        self.ids_lookup[first_id] = sorted(self.ids_lookup[first_id] + [(ids, embedding)], key=lambda x: len(x[0]), reverse=True)\n",
        "\n",
        "        return embedding\n",
        "\n",
        "    def get_expected_shape(self):\n",
        "        vec = root.model.cond_stage_model.encode_embedding_init_text(\",\", 1)\n",
        "        return vec.shape[1]\n",
        "\n",
        "    def load_from_file(self, path, filename):\n",
        "        name, ext = os.path.splitext(filename)\n",
        "        ext = ext.upper()\n",
        "\n",
        "        if ext in ['.PNG', '.WEBP', '.JXL', '.AVIF']:\n",
        "            _, second_ext = os.path.splitext(name)\n",
        "            if second_ext.upper() == '.PREVIEW':\n",
        "                return\n",
        "\n",
        "            embed_image = Image.open(path)\n",
        "            if hasattr(embed_image, 'text') and 'sd-ti-embedding' in embed_image.text:\n",
        "                data = embedding_from_b64(embed_image.text['sd-ti-embedding'])\n",
        "                name = data.get('name', name)\n",
        "            else:\n",
        "                data = extract_image_data_embed(embed_image)\n",
        "                name = data.get('name', name)\n",
        "        elif ext in ['.BIN', '.PT']:\n",
        "            data = torch.load(path, map_location=\"cpu\")\n",
        "        elif ext in ['.SAFETENSORS']:\n",
        "            data = safetensors.torch.load_file(path, device=\"cpu\")\n",
        "        else:\n",
        "            return\n",
        "\n",
        "        # textual inversion embeddings\n",
        "        if 'string_to_param' in data:\n",
        "            param_dict = data['string_to_param']\n",
        "            if hasattr(param_dict, '_parameters'):\n",
        "                param_dict = getattr(param_dict, '_parameters')  # fix for torch 1.12.1 loading saved file from torch 1.11\n",
        "            assert len(param_dict) == 1, 'embedding file has multiple terms in it'\n",
        "            emb = next(iter(param_dict.items()))[1]\n",
        "        # diffuser concepts\n",
        "        elif type(data) == dict and type(next(iter(data.values()))) == torch.Tensor:\n",
        "            assert len(data.keys()) == 1, 'embedding file has multiple terms in it'\n",
        "\n",
        "            emb = next(iter(data.values()))\n",
        "            if len(emb.shape) == 1:\n",
        "                emb = emb.unsqueeze(0)\n",
        "        else:\n",
        "            raise Exception(f\"Couldn't identify {filename} as neither textual inversion embedding nor diffuser concept.\")\n",
        "\n",
        "        vec = emb.detach().to(root.device, dtype=torch.float32)\n",
        "        embedding = Embedding(vec, name)\n",
        "        embedding.step = data.get('step', None)\n",
        "        embedding.sd_checkpoint = data.get('sd_checkpoint', None)\n",
        "        embedding.sd_checkpoint_name = data.get('sd_checkpoint_name', None)\n",
        "        embedding.vectors = vec.shape[0]\n",
        "        embedding.shape = vec.shape[-1]\n",
        "        embedding.filename = path\n",
        "\n",
        "        if self.expected_shape == -1 or self.expected_shape == embedding.shape:\n",
        "            self.register_embedding(embedding, root.model)\n",
        "        else:\n",
        "            self.skipped_embeddings[name] = embedding\n",
        "\n",
        "    def load_from_dir(self, embdir):\n",
        "        if not os.path.isdir(embdir.path):\n",
        "            return\n",
        "\n",
        "        for root, dirs, fns in os.walk(embdir.path):\n",
        "            for fn in fns:\n",
        "                try:\n",
        "                    fullfn = os.path.join(root, fn)\n",
        "\n",
        "                    if os.stat(fullfn).st_size == 0:\n",
        "                        continue\n",
        "\n",
        "                    self.load_from_file(fullfn, fn)\n",
        "                except Exception:\n",
        "                    print(f\"Error loading embedding {fn}:\", file=sys.stderr)\n",
        "                    print(traceback.format_exc(), file=sys.stderr)\n",
        "                    continue\n",
        "\n",
        "    def load_textual_inversion_embeddings(self, force_reload=False):\n",
        "        if not force_reload:\n",
        "            need_reload = False\n",
        "            for path, embdir in self.embedding_dirs.items():\n",
        "                if embdir.has_changed():\n",
        "                    need_reload = True\n",
        "                    break\n",
        "\n",
        "            if not need_reload:\n",
        "                return\n",
        "\n",
        "        self.ids_lookup.clear()\n",
        "        self.word_embeddings.clear()\n",
        "        self.skipped_embeddings.clear()\n",
        "        self.expected_shape = self.get_expected_shape()\n",
        "\n",
        "        for path, embdir in self.embedding_dirs.items():\n",
        "            self.load_from_dir(embdir)\n",
        "            embdir.update()\n",
        "\n",
        "        print(f\"Textual inversion embeddings loaded({len(self.word_embeddings)}): {', '.join(self.word_embeddings.keys())}\")\n",
        "        if len(self.skipped_embeddings) > 0:\n",
        "            print(f\"Textual inversion embeddings skipped({len(self.skipped_embeddings)}): {', '.join(self.skipped_embeddings.keys())}\")\n",
        "\n",
        "    def find_embedding_at_position(self, tokens, offset):\n",
        "        token = tokens[offset]\n",
        "        possible_matches = self.ids_lookup.get(token, None)\n",
        "\n",
        "        if possible_matches is None:\n",
        "            return None, None\n",
        "\n",
        "        for ids, embedding in possible_matches:\n",
        "            if tokens[offset:offset + len(ids)] == ids:\n",
        "                return embedding, len(ids)\n",
        "\n",
        "        return None, None\n",
        "\n",
        "#train embeddings\n",
        "def autocast(disable=False):\n",
        "    \n",
        "    if disable:\n",
        "        return contextlib.nullcontext()\n",
        "\n",
        "    if dtype == torch.float32 or precision == \"full\":\n",
        "        return contextlib.nullcontext()\n",
        "\n",
        "    return torch.autocast(\"cuda\")\n",
        "\n",
        "##markdown $\\color{blue} {\\textsf{These Params are FOR TEXTUAL INVERSION TRAINING ONLY!}}$\n",
        "\n",
        "def create_embedding(name, num_vectors_per_token, overwrite_old, init_text='*'):\n",
        "    cond_model = model.cond_stage_model\n",
        "\n",
        "    with autocast():\n",
        "        cond_model([\"\"])  # will send cond model to GPU if lowvram/medvram is active\n",
        "\n",
        "    #cond_model expects at least some text, so we provide '*' as backup.\n",
        "    embedded = cond_model.encode_embedding_init_text(init_text or '*', num_vectors_per_token)\n",
        "    vec = torch.zeros((num_vectors_per_token, embedded.shape[1]), device=root.device)\n",
        "\n",
        "    #Only copy if we provided an init_text, otherwise keep vectors as zeros\n",
        "    if init_text:\n",
        "        for i in range(num_vectors_per_token):\n",
        "            vec[i] = embedded[i * int(embedded.shape[0]) // num_vectors_per_token]\n",
        "\n",
        "    # Remove illegal characters from name.\n",
        "    name = \"\".join( x for x in name if (x.isalnum() or x in \"._- \"))\n",
        "    fn = os.path.join(root.embeddings_dir, f\"{name}.pt\")\n",
        "    if not overwrite_old:\n",
        "        assert not os.path.exists(fn), f\"file {fn} already exists\"\n",
        "\n",
        "    embedding = Embedding(vec, name)\n",
        "    embedding.step = 0\n",
        "    embedding.save(fn)\n",
        "\n",
        "    return fn\n",
        "\n",
        "training_write_csv_every=0\n",
        "def write_loss(log_directory, filename, step, epoch_len, values):\n",
        "    if training_write_csv_every == 0:\n",
        "        return\n",
        "\n",
        "    if step % training_write_csv_every != 0:\n",
        "        return\n",
        "    write_csv_header = False if os.path.exists(os.path.join(log_directory, filename)) else True\n",
        "\n",
        "    with open(os.path.join(log_directory, filename), \"a+\", newline='') as fout:\n",
        "        csv_writer = csv.DictWriter(fout, fieldnames=[\"step\", \"epoch\", \"epoch_step\", *(values.keys())])\n",
        "\n",
        "        if write_csv_header:\n",
        "            csv_writer.writeheader()\n",
        "\n",
        "        epoch = (step - 1) // epoch_len\n",
        "        epoch_step = (step - 1) % epoch_len\n",
        "\n",
        "        csv_writer.writerow({\n",
        "            \"step\": step,\n",
        "            \"epoch\": epoch,\n",
        "            \"epoch_step\": epoch_step,\n",
        "            **values,\n",
        "        })\n",
        "\n",
        "def tensorboard_setup(log_directory):\n",
        "    os.makedirs(os.path.join(log_directory, \"tensorboard\"), exist_ok=True)\n",
        "    training_tensorboard_flush_every=120\n",
        "    return SummaryWriter(\n",
        "            log_dir=os.path.join(log_directory, \"tensorboard\"),\n",
        "            flush_secs=training_tensorboard_flush_every)\n",
        "\n",
        "def tensorboard_add(tensorboard_writer, loss, global_step, step, learn_rate, epoch_num):\n",
        "    tensorboard_add_scaler(tensorboard_writer, \"Loss/train\", loss, global_step)\n",
        "    tensorboard_add_scaler(tensorboard_writer, f\"Loss/train/epoch-{epoch_num}\", loss, step)\n",
        "    tensorboard_add_scaler(tensorboard_writer, \"Learn rate/train\", learn_rate, global_step)\n",
        "    tensorboard_add_scaler(tensorboard_writer, f\"Learn rate/train/epoch-{epoch_num}\", learn_rate, step)\n",
        "\n",
        "def tensorboard_add_scaler(tensorboard_writer, tag, value, step):\n",
        "    tensorboard_writer.add_scalar(tag=tag, \n",
        "        scalar_value=value, global_step=step)\n",
        "\n",
        "def tensorboard_add_image(tensorboard_writer, tag, pil_image, step):\n",
        "    # Convert a pil image to a torch tensor\n",
        "    img_tensor = torch.as_tensor(np.array(pil_image, copy=True))\n",
        "    img_tensor = img_tensor.view(pil_image.size[1], pil_image.size[0], \n",
        "        len(pil_image.getbands()))\n",
        "    img_tensor = img_tensor.permute((2, 0, 1))\n",
        "                \n",
        "    tensorboard_writer.add_image(tag, img_tensor, global_step=step)\n",
        "\n",
        "def validate_train_inputs(model_name, learn_rate, batch_size, gradient_step, data_root, template_file, template_filename, steps, save_model_every, create_image_every, log_directory, name=\"embedding\"):\n",
        "    assert model_name, f\"{name} not selected\"\n",
        "    assert learn_rate, \"Learning rate is empty or 0\"\n",
        "    assert isinstance(batch_size, int), \"Batch size must be integer\"\n",
        "    assert batch_size > 0, \"Batch size must be positive\"\n",
        "    assert isinstance(gradient_step, int), \"Gradient accumulation step must be integer\"\n",
        "    assert gradient_step > 0, \"Gradient accumulation step must be positive\"\n",
        "    assert data_root, \"Dataset directory is empty\"\n",
        "    assert os.path.isdir(data_root), \"Dataset directory doesn't exist\"\n",
        "    assert os.listdir(data_root), \"Dataset directory is empty\"\n",
        "    assert template_filename, \"Prompt template file not selected\"\n",
        "    assert template_file, f\"Prompt template file {template_filename} not found\"\n",
        "    assert os.path.isfile(template_file), f\"Prompt template file {template_filename} doesn't exist\"\n",
        "    assert steps, \"Max steps is empty or 0\"\n",
        "    assert isinstance(steps, int), \"Max steps must be integer\"\n",
        "    assert steps > 0, \"Max steps must be positive\"\n",
        "    assert isinstance(save_model_every, int), \"Save {name} must be integer\"\n",
        "    assert save_model_every >= 0, \"Save {name} must be positive or 0\"\n",
        "    assert isinstance(create_image_every, int), \"Create image must be integer\"\n",
        "    assert create_image_every >= 0, \"Create image must be positive or 0\"\n",
        "    if save_model_every or create_image_every:\n",
        "        assert log_directory, \"Log directory is empty\"\n",
        "\n",
        "import os\n",
        "def train_embedding(id_task, embedding_name, learn_rate, batch_size, gradient_step, data_root, log_directory, training_width, training_height, varsize, steps, clip_grad_mode, clip_grad_value, shuffle_tags, tag_drop_out, latent_sampling_method, create_image_every, save_embedding_every, template_filename, save_image_with_stored_embedding, preview_from_txt2img, preview_prompt, preview_negative_prompt, preview_steps, preview_sampler_index, preview_cfg_scale, preview_seed, preview_width, preview_height):\n",
        "    save_embedding_every = save_embedding_every or 0\n",
        "    create_image_every = create_image_every or 0\n",
        "    template_file = template_filename\n",
        "    validate_train_inputs(embedding_name, learn_rate, batch_size, gradient_step, data_root, template_file, template_filename, steps, save_embedding_every, create_image_every, log_directory, name=\"embedding\")\n",
        "    template_file = template_filename\n",
        "\n",
        "\n",
        "    job = \"train-embedding\"\n",
        "    textinfo = \"Initializing textual inversion training...\"\n",
        "    job_count = steps\n",
        "    import os\n",
        "    import datetime\n",
        "    filename = os.path.join(root.embeddings_dir, f'{embedding_name}.pt')\n",
        "\n",
        "    log_directory = os.path.join(log_directory, datetime.datetime.now().strftime(\"%Y-%m-%d\"), embedding_name)\n",
        "    unload_models_when_training=False\n",
        "    unload = unload_models_when_training\n",
        "\n",
        "    if save_embedding_every > 0:\n",
        "        embedding_dir = os.path.join(log_directory, \"embeddings\")\n",
        "        os.makedirs(embedding_dir, exist_ok=True)\n",
        "    else:\n",
        "        embedding_dir = None\n",
        "\n",
        "    if create_image_every > 0:\n",
        "        images_dir = os.path.join(log_directory, \"images\")\n",
        "        os.makedirs(images_dir, exist_ok=True)\n",
        "    else:\n",
        "        images_dir = None\n",
        "\n",
        "    if create_image_every > 0 and save_image_with_stored_embedding:\n",
        "        images_embeds_dir = os.path.join(log_directory, \"image_embeddings\")\n",
        "        os.makedirs(images_embeds_dir, exist_ok=True)\n",
        "    else:\n",
        "        images_embeds_dir = None\n",
        "\n",
        "    hijack = model_hijack\n",
        "\n",
        "    embedding = hijack.embedding_db.word_embeddings[embedding_name]\n",
        "    checkpoint = root.model\n",
        "\n",
        "    initial_step = embedding.step or 0\n",
        "    if initial_step >= steps:\n",
        "        extinfo = \"Model has already been trained beyond specified max steps\"\n",
        "        return embedding, filename\n",
        "    \n",
        "    scheduler = LearnRateScheduler(learn_rate, steps, initial_step)\n",
        "    clip_grad = torch.nn.utils.clip_grad_value_ if clip_grad_mode == \"value\" else \\\n",
        "        torch.nn.utils.clip_grad_norm_ if clip_grad_mode == \"norm\" else \\\n",
        "        None\n",
        "    if clip_grad:\n",
        "        clip_grad_sched = LearnRateScheduler(clip_grad_value, steps, initial_step, verbose=False)\n",
        "    # dataset loading may take a while, so input validations and early returns should be done before this\n",
        "    textinfo = f\"Preparing dataset from {html.escape(data_root)}...\"\n",
        "    parallel_processing_allowed=False\n",
        "    old_parallel_processing_allowed = parallel_processing_allowed\n",
        "    \n",
        "    training_enable_tensorboard=True\n",
        "    if training_enable_tensorboard:\n",
        "        tensorboard_writer = tensorboard_setup(log_directory)\n",
        "\n",
        "    pin_memory = False\n",
        "\n",
        "    ds = PersonalizedBase(data_root=data_root, width=training_width, height=training_height, repeats=1, placeholder_token=embedding_name, model=model, cond_model=model.cond_stage_model, device=root.device, template_file=template_file, batch_size=batch_size, gradient_step=gradient_step, shuffle_tags=shuffle_tags, tag_drop_out=tag_drop_out, latent_sampling_method=latent_sampling_method, varsize=varsize)\n",
        "\n",
        "    import datetime\n",
        "    import json\n",
        "    import os\n",
        "\n",
        "    saved_params_shared = {\"model_name\", \"model_hash\", \"initial_step\", \"num_of_dataset_images\", \"learn_rate\", \"batch_size\", \"clip_grad_mode\", \"clip_grad_value\", \"gradient_step\", \"data_root\", \"log_directory\", \"training_width\", \"training_height\", \"steps\", \"create_image_every\", \"template_file\", \"gradient_step\", \"latent_sampling_method\"}\n",
        "    saved_params_ti = {\"embedding_name\", \"num_vectors_per_token\", \"save_embedding_every\", \"save_image_with_stored_embedding\"}\n",
        "    saved_params_hypernet = {\"hypernetwork_name\", \"layer_structure\", \"activation_func\", \"weight_init\", \"add_layer_norm\", \"use_dropout\", \"save_hypernetwork_every\"}\n",
        "    saved_params_all = saved_params_shared | saved_params_ti | saved_params_hypernet\n",
        "    saved_params_previews = {\"preview_prompt\", \"preview_negative_prompt\", \"preview_steps\", \"preview_sampler_index\", \"preview_cfg_scale\", \"preview_seed\", \"preview_width\", \"preview_height\"}\n",
        "\n",
        "\n",
        "    def save_settings_to_file(log_directory, all_params):\n",
        "        now = datetime.datetime.now()\n",
        "        params = {\"datetime\": now.strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
        "\n",
        "        keys = saved_params_all\n",
        "        if all_params.get('preview_from_txt2img'):\n",
        "          keys = keys | saved_params_previews\n",
        "\n",
        "        params.update({k: v for k, v in all_params.items() if k in keys})\n",
        "\n",
        "        filename = f'settings-{now.strftime(\"%Y-%m-%d-%H-%M-%S\")}.json'\n",
        "        with open(os.path.join(log_directory, filename), \"w\") as file:\n",
        "            json.dump(params, file, indent=4)\n",
        "\n",
        "    save_training_settings_to_txt=True\n",
        "    if save_training_settings_to_txt:\n",
        "        save_settings_to_file(log_directory, {**dict(model_name=root.model_checkpoint, model_hash=ckpt_hash, num_of_dataset_images=len(ds), num_vectors_per_token=len(embedding.vec)), **locals()})\n",
        "\n",
        "    latent_sampling_method = ds.latent_sampling_method\n",
        "\n",
        "    dl = PersonalizedDataLoader(ds, latent_sampling_method=latent_sampling_method, batch_size=ds.batch_size, pin_memory=pin_memory)\n",
        "\n",
        "    if unload:\n",
        "        parallel_processing_allowed = False\n",
        "        model.first_stage_model.to(\"cuda\")\n",
        "\n",
        "    embedding.vec.requires_grad = True\n",
        "    optimizer = torch.optim.AdamW([embedding.vec], lr=scheduler.learn_rate, weight_decay=0.0)\n",
        "    save_optimizer_state=True\n",
        "    if save_optimizer_state:\n",
        "        optimizer_state_dict = None\n",
        "        if os.path.exists(filename + '.optim'):\n",
        "            optimizer_saved_dict = torch.load(filename + '.optim', map_location='cpu')\n",
        "            if embedding.checksum() == optimizer_saved_dict.get('hash', None):\n",
        "                optimizer_state_dict = optimizer_saved_dict.get('optimizer_state_dict', None)\n",
        "    \n",
        "        if optimizer_state_dict is not None:\n",
        "            optimizer.load_state_dict(optimizer_state_dict)\n",
        "            print(\"Loaded existing optimizer from checkpoint\")\n",
        "        else:\n",
        "            print(\"No saved optimizer exists in checkpoint\")\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    batch_size = ds.batch_size\n",
        "    gradient_step = ds.gradient_step\n",
        "    # n steps = batch_size * gradient_step * n image processed\n",
        "    steps_per_epoch = len(ds) // batch_size // gradient_step\n",
        "    max_steps_per_epoch = len(ds) // batch_size - (len(ds) // batch_size) % gradient_step\n",
        "    loss_step = 0\n",
        "    _loss_step = 0 #internal\n",
        "\n",
        "    last_saved_file = \"<none>\"\n",
        "    last_saved_image = \"<none>\"\n",
        "    forced_filename = \"<none>\"\n",
        "    embedding_yet_to_be_embedded = False\n",
        "\n",
        "    # is_training_inpainting_model = root.model.conditioning_key in {'hybrid', 'concat'}\n",
        "    img_c = None\n",
        "\n",
        "   ## hijack_checkpoint\n",
        "\n",
        "    from torch.utils.checkpoint import checkpoint\n",
        "\n",
        "    import ldm.modules.attention\n",
        "    import ldm.modules.diffusionmodules.openaimodel\n",
        "\n",
        "\n",
        "    def BasicTransformerBlock_forward(self, x, context=None):\n",
        "        return checkpoint(self._forward, x, context)\n",
        "\n",
        "\n",
        "    def AttentionBlock_forward(self, x):\n",
        "        return checkpoint(self._forward, x)\n",
        "\n",
        "\n",
        "    def ResBlock_forward(self, x, emb):\n",
        "        return checkpoint(self._forward, x, emb)\n",
        "\n",
        "\n",
        "    stored = []\n",
        "\n",
        "\n",
        "    def hijack_checkpoint_add():\n",
        "        if len(stored) != 0:\n",
        "            return\n",
        "\n",
        "        stored.extend([\n",
        "            ldm.modules.attention.BasicTransformerBlock.forward,\n",
        "            ldm.modules.diffusionmodules.openaimodel.ResBlock.forward,\n",
        "            ldm.modules.diffusionmodules.openaimodel.AttentionBlock.forward\n",
        "        ])\n",
        "\n",
        "        ldm.modules.attention.BasicTransformerBlock.forward = BasicTransformerBlock_forward\n",
        "        ldm.modules.diffusionmodules.openaimodel.ResBlock.forward = ResBlock_forward\n",
        "        ldm.modules.diffusionmodules.openaimodel.AttentionBlock.forward = AttentionBlock_forward\n",
        "\n",
        "\n",
        "    def hijack_checkpoint_remove():\n",
        "        if len(stored) == 0:\n",
        "            return\n",
        "\n",
        "        ldm.modules.attention.BasicTransformerBlock.forward = stored[0]\n",
        "        ldm.modules.diffusionmodules.openaimodel.ResBlock.forward = stored[1]\n",
        "        ldm.modules.diffusionmodules.openaimodel.AttentionBlock.forward = stored[2]\n",
        "\n",
        "        stored.clear()\n",
        "\n",
        "    pbar = tqdm.tqdm(total=steps - initial_step)\n",
        "    try:\n",
        "        hijack_checkpoint_add()\n",
        "\n",
        "        for i in range((steps-initial_step) * gradient_step):\n",
        "            # if scheduler.finished:\n",
        "            #     break\n",
        "            # if interrupted:\n",
        "            #     break\n",
        "            for j, batch in enumerate(dl):\n",
        "                # works as a drop_last=True for gradient accumulation\n",
        "                if j == max_steps_per_epoch:\n",
        "                    break\n",
        "                scheduler.apply(optimizer, embedding.step)\n",
        "                # if scheduler.finished:\n",
        "                #     break\n",
        "                # if shared.state.interrupted:\n",
        "                #     break\n",
        "\n",
        "                if clip_grad:\n",
        "                    clip_grad_sched.step(embedding.step)\n",
        "                with torch.autocast(\"cuda\"):\n",
        "                    x = batch.latent_sample.to(\"cuda\", non_blocking=pin_memory)\n",
        "                    c = model.cond_stage_model(batch.cond_text)\n",
        "                    is_training_inpainting_model=False\n",
        "                    if is_training_inpainting_model:\n",
        "                        if img_c is None:\n",
        "                            img_c = processing.txt2img_image_conditioning(shared.sd_model, c, training_width, training_height)\n",
        "\n",
        "                        cond = {\"c_concat\": [img_c], \"c_crossattn\": [c]}\n",
        "                    else:\n",
        "                        cond = c\n",
        "                    cond = cond.to(\"cuda\")\n",
        "                    x = x.to(\"cuda\")\n",
        "                    loss = model(x, cond)[0] / gradient_step\n",
        "                    del x\n",
        "\n",
        "                    _loss_step += loss.item()\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                # go back until we reach gradient accumulation steps\n",
        "                if (j + 1) % gradient_step != 0:\n",
        "                    continue\n",
        "                \n",
        "                if clip_grad:\n",
        "                    clip_grad(embedding.vec, clip_grad_sched.learn_rate)\n",
        "\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                embedding.step += 1\n",
        "                pbar.update()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                loss_step = _loss_step\n",
        "                _loss_step = 0\n",
        "\n",
        "                steps_done = embedding.step + 1\n",
        "\n",
        "                epoch_num = embedding.step // steps_per_epoch\n",
        "                epoch_step = embedding.step % steps_per_epoch\n",
        "\n",
        "                description = f\"Training textual inversion [Epoch {epoch_num}: {epoch_step+1}/{steps_per_epoch}] loss: {loss_step:.7f}\"\n",
        "                pbar.set_description(description)\n",
        "                if embedding_dir is not None and steps_done % save_embedding_every == 0:\n",
        "                    # Before saving, change name to match current checkpoint.\n",
        "                    embedding_name_every = f'{embedding_name}-{steps_done}'\n",
        "                    last_saved_file = os.path.join(embedding_dir, f'{embedding_name_every}.pt')\n",
        "                    save_embedding(embedding, optimizer, checkpoint, embedding_name_every, last_saved_file, remove_cached_checksum=True)\n",
        "                    embedding_yet_to_be_embedded = True\n",
        "\n",
        "                write_loss(log_directory, \"textual_inversion_loss.csv\", embedding.step, steps_per_epoch, {\n",
        "                    \"loss\": f\"{loss_step:.7f}\",\n",
        "                    \"learn_rate\": scheduler.learn_rate\n",
        "                })\n",
        "\n",
        "                training_tensorboard_save_images=False\n",
        "\n",
        "                if images_dir is not None and steps_done % create_image_every == 0:\n",
        "                    forced_filename = f'{embedding_name}-{steps_done}'\n",
        "                    last_saved_image = os.path.join(images_dir, forced_filename)\n",
        "\n",
        "                    model.first_stage_model.to(root.device)\n",
        "\n",
        "                    # p = processing.StableDiffusionProcessingTxt2Img(\n",
        "                    #     sd_model=shared.sd_model,\n",
        "                    #     do_not_save_grid=True,\n",
        "                    #     do_not_save_samples=True,\n",
        "                    #     do_not_reload_embeddings=True,\n",
        "                    # )\n",
        "\n",
        "                    if preview_from_txt2img:\n",
        "                        p.prompt = preview_prompt\n",
        "                        p.negative_prompt = preview_negative_prompt\n",
        "                        p.steps = preview_steps\n",
        "                        p.sampler_name = sd_samplers.samplers[preview_sampler_index].name\n",
        "                        p.cfg_scale = preview_cfg_scale\n",
        "                        p.seed = preview_seed\n",
        "                        p.width = preview_width\n",
        "                        p.height = preview_height\n",
        "                    else:\n",
        "                        prompt = batch.cond_text[0]\n",
        "                        steps = 20\n",
        "                        width = training_width\n",
        "                        height = training_height\n",
        "\n",
        "                    preview_text = prompt\n",
        "\n",
        "                    # processed = processing.process_images(p)\n",
        "                    # image = processed.images[0] if len(processed.images) > 0 else None\n",
        "\n",
        "                    # def assign_current_image(self, image):\n",
        "                    #     self.current_image = image\n",
        "                    #     self.id_live_preview += 1\n",
        "                    # if unload:\n",
        "                    #     model.first_stage_model.to(\"cuda\")\n",
        "\n",
        "                    # if image is not None:\n",
        "                    #     assign_current_image(image)\n",
        "\n",
        "                    #     last_saved_image, last_text_info = images.save_image(image, images_dir, \"\", seed, prompt, samples_format=\"png\", processed.infotexts[0], p=p, forced_filename=forced_filename, save_to_dirs=False)\n",
        "                    #     last_saved_image += f\", prompt: {preview_text}\"\n",
        "\n",
        "                    #     if training_enable_tensorboard and training_tensorboard_save_images:\n",
        "                    #         tensorboard_add_image(tensorboard_writer, f\"Validation at epoch {epoch_num}\", image, embedding.step)\n",
        "\n",
        "                    # if save_image_with_stored_embedding and os.path.exists(last_saved_file) and embedding_yet_to_be_embedded:\n",
        "\n",
        "                    #     last_saved_image_chunks = os.path.join(images_embeds_dir, f'{embedding_name}-{steps_done}.png')\n",
        "\n",
        "                    #     info = PngImagePlugin.PngInfo()\n",
        "                    #     data = torch.load(last_saved_file)\n",
        "                    #     info.add_text(\"sd-ti-embedding\", embedding_to_b64(data))\n",
        "\n",
        "                    #     title = \"<{}>\".format(data.get('name', '???'))\n",
        "\n",
        "                    #     try:\n",
        "                    #         vectorSize = list(data['string_to_param'].values())[0].shape[0]\n",
        "                    #     except Exception as e:\n",
        "                    #         vectorSize = '?'\n",
        "\n",
        "                    #     checkpoint = root.model_checkpoint\n",
        "                    #     footer_left = checkpoint.model_name\n",
        "                    #     footer_mid = '[{}]'.format(checkpoint.shorthash)\n",
        "                    #     footer_right = '{}v {}s'.format(vectorSize, steps_done)\n",
        "\n",
        "                    #     captioned_image = caption_image_overlay(image, title, footer_left, footer_mid, footer_right)\n",
        "                    #     captioned_image = insert_image_data_embed(captioned_image, data)\n",
        "\n",
        "                    #     captioned_image.save(last_saved_image_chunks, \"PNG\", pnginfo=info)\n",
        "                    #     embedding_yet_to_be_embedded = False\n",
        "\n",
        "                    # last_saved_image, last_text_info = images.save_image(image, images_dir, \"\", p.seed, p.prompt, shared.opts.samples_format, processed.infotexts[0], p=p, forced_filename=forced_filename, save_to_dirs=False)\n",
        "                    # last_saved_image += f\", prompt: {preview_text}\"\n",
        "\n",
        "                job_no = embedding.step\n",
        "\n",
        "                textinfo = f\"\"\"\n",
        "# <p>\n",
        "# Loss: {loss_step:.7f}<br/>\n",
        "# Step: {steps_done}<br/>\n",
        "# Last prompt: {html.escape(batch.cond_text[0])}<br/>\n",
        "# Last saved embedding: {html.escape(last_saved_file)}<br/>\n",
        "# Last saved image: {html.escape(last_saved_image)}<br/>\n",
        "# </p>\n",
        "# \"\"\"\n",
        "        filename = os.path.join(root.embeddings_dir, f'{embedding_name}.pt')\n",
        "        save_embedding(embedding, optimizer, checkpoint, embedding_name, filename, remove_cached_checksum=True)\n",
        "    except Exception:\n",
        "        print(traceback.format_exc(), file=sys.stderr)\n",
        "        pass\n",
        "    finally:\n",
        "        pbar.leave = False\n",
        "        pbar.close()\n",
        "        model.first_stage_model.to(root.device)\n",
        "        parallel_processing_allowed = old_parallel_processing_allowed\n",
        "        hijack_checkpoint_remove()\n",
        "\n",
        "    return embedding, filename\n",
        "\n",
        "\n",
        "def save_embedding(embedding, optimizer, checkpoint, embedding_name, filename, remove_cached_checksum=True):\n",
        "    old_embedding_name = embedding.name\n",
        "    old_sd_checkpoint = embedding.sd_checkpoint if hasattr(embedding, \"sd_checkpoint\") else None\n",
        "    old_sd_checkpoint_name = embedding.sd_checkpoint_name if hasattr(embedding, \"sd_checkpoint_name\") else None\n",
        "    old_cached_checksum = embedding.cached_checksum if hasattr(embedding, \"cached_checksum\") else None\n",
        "    try:\n",
        "        embedding.sd_checkpoint = checkpoint.shorthash\n",
        "        embedding.sd_checkpoint_name = checkpoint.model_name\n",
        "        if remove_cached_checksum:\n",
        "            embedding.cached_checksum = None\n",
        "        embedding.name = embedding_name\n",
        "        embedding.optimizer_state_dict = optimizer.state_dict()\n",
        "        embedding.save(filename)\n",
        "    except:\n",
        "        embedding.sd_checkpoint = old_sd_checkpoint\n",
        "        embedding.sd_checkpoint_name = old_sd_checkpoint_name\n",
        "        embedding.name = old_embedding_name\n",
        "        embedding.cached_checksum = old_cached_checksum\n",
        "        raise\n",
        "\n",
        "#cond_func\n",
        "import importlib\n",
        "\n",
        "class CondFunc:\n",
        "    def __new__(cls, orig_func, sub_func, cond_func):\n",
        "        self = super(CondFunc, cls).__new__(cls)\n",
        "        if isinstance(orig_func, str):\n",
        "            func_path = orig_func.split('.')\n",
        "            for i in range(len(func_path)-1, -1, -1):\n",
        "                try:\n",
        "                    resolved_obj = importlib.import_module('.'.join(func_path[:i]))\n",
        "                    break\n",
        "                except ImportError:\n",
        "                    pass\n",
        "            for attr_name in func_path[i:-1]:\n",
        "                resolved_obj = getattr(resolved_obj, attr_name)\n",
        "            orig_func = getattr(resolved_obj, func_path[-1])\n",
        "            setattr(resolved_obj, func_path[-1], lambda *args, **kwargs: self(*args, **kwargs))\n",
        "        self.__init__(orig_func, sub_func, cond_func)\n",
        "        return lambda *args, **kwargs: self(*args, **kwargs)\n",
        "    def __init__(self, orig_func, sub_func, cond_func):\n",
        "        self.__orig_func = orig_func\n",
        "        self.__sub_func = sub_func\n",
        "        self.__cond_func = cond_func\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        if not self.__cond_func or self.__cond_func(self.__orig_func, *args, **kwargs):\n",
        "            return self.__sub_func(self.__orig_func, *args, **kwargs)\n",
        "        else:\n",
        "            return self.__orig_func(*args, **kwargs)\n",
        "\n",
        "#errors\n",
        "import sys\n",
        "import traceback\n",
        "\n",
        "\n",
        "def print_error_explanation(message):\n",
        "    lines = message.strip().split(\"\\n\")\n",
        "    max_len = max([len(x) for x in lines])\n",
        "\n",
        "    print('=' * max_len, file=sys.stderr)\n",
        "    for line in lines:\n",
        "        print(line, file=sys.stderr)\n",
        "    print('=' * max_len, file=sys.stderr)\n",
        "\n",
        "\n",
        "def display(e: Exception, task):\n",
        "    print(f\"{task or 'error'}: {type(e).__name__}\", file=sys.stderr)\n",
        "    print(traceback.format_exc(), file=sys.stderr)\n",
        "\n",
        "    message = str(e)\n",
        "    if \"copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape in current model is torch.Size([640, 768])\" in message:\n",
        "        print_error_explanation(\"\"\"\n",
        "The most likely cause of this is you are trying to load Stable Diffusion 2.0 model without specifying its config file.\n",
        "See https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#stable-diffusion-20 for how to solve this.\n",
        "        \"\"\")\n",
        "\n",
        "\n",
        "already_displayed = {}\n",
        "\n",
        "\n",
        "def display_once(e: Exception, task):\n",
        "    if task in already_displayed:\n",
        "        return\n",
        "\n",
        "    display(e, task)\n",
        "\n",
        "    already_displayed[task] = 1\n",
        "\n",
        "\n",
        "def run(code, task):\n",
        "    try:\n",
        "        code()\n",
        "    except Exception as e:\n",
        "        display(task, e)\n",
        "\n",
        "#sub_quad_attn\n",
        "from functools import partial\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "import math\n",
        "from typing import Optional, NamedTuple, List\n",
        "\n",
        "def without_autocast(disable=False):\n",
        "    return torch.autocast(\"cuda\", enabled=False) if torch.is_autocast_enabled() and not disable else contextlib.nullcontext()\n",
        "\n",
        "def narrow_trunc(\n",
        "    input: Tensor,\n",
        "    dim: int,\n",
        "    start: int,\n",
        "    length: int\n",
        ") -> Tensor:\n",
        "    return torch.narrow(input, dim, start, length if input.shape[dim] >= start + length else input.shape[dim] - start)\n",
        "\n",
        "\n",
        "class AttnChunk(NamedTuple):\n",
        "    exp_values: Tensor\n",
        "    exp_weights_sum: Tensor\n",
        "    max_score: Tensor\n",
        "\n",
        "\n",
        "class SummarizeChunk:\n",
        "    @staticmethod\n",
        "    def __call__(\n",
        "        query: Tensor,\n",
        "        key: Tensor,\n",
        "        value: Tensor,\n",
        "    ) -> AttnChunk: ...\n",
        "\n",
        "\n",
        "class ComputeQueryChunkAttn:\n",
        "    @staticmethod\n",
        "    def __call__(\n",
        "        query: Tensor,\n",
        "        key: Tensor,\n",
        "        value: Tensor,\n",
        "    ) -> Tensor: ...\n",
        "\n",
        "\n",
        "def _summarize_chunk(\n",
        "    query: Tensor,\n",
        "    key: Tensor,\n",
        "    value: Tensor,\n",
        "    scale: float,\n",
        ") -> AttnChunk:\n",
        "    attn_weights = torch.baddbmm(\n",
        "        torch.empty(1, 1, 1, device=query.device, dtype=query.dtype),\n",
        "        query,\n",
        "        key.transpose(1,2),\n",
        "        alpha=scale,\n",
        "        beta=0,\n",
        "    )\n",
        "    max_score, _ = torch.max(attn_weights, -1, keepdim=True)\n",
        "    max_score = max_score.detach()\n",
        "    exp_weights = torch.exp(attn_weights - max_score)\n",
        "    exp_values = torch.bmm(exp_weights, value) if query.device.type == 'mps' else torch.bmm(exp_weights, value.to(exp_weights.dtype)).to(value.dtype)\n",
        "    max_score = max_score.squeeze(-1)\n",
        "    return AttnChunk(exp_values, exp_weights.sum(dim=-1), max_score)\n",
        "\n",
        "\n",
        "def _query_chunk_attention(\n",
        "    query: Tensor,\n",
        "    key: Tensor,\n",
        "    value: Tensor,\n",
        "    summarize_chunk: SummarizeChunk,\n",
        "    kv_chunk_size: int,\n",
        ") -> Tensor:\n",
        "    batch_x_heads, k_tokens, k_channels_per_head = key.shape\n",
        "    _, _, v_channels_per_head = value.shape\n",
        "\n",
        "    def chunk_scanner(chunk_idx: int) -> AttnChunk:\n",
        "        key_chunk = narrow_trunc(\n",
        "            key,\n",
        "            1,\n",
        "            chunk_idx,\n",
        "            kv_chunk_size\n",
        "        )\n",
        "        value_chunk = narrow_trunc(\n",
        "            value,\n",
        "            1,\n",
        "            chunk_idx,\n",
        "            kv_chunk_size\n",
        "        )\n",
        "        return summarize_chunk(query, key_chunk, value_chunk)\n",
        "\n",
        "    chunks: List[AttnChunk] = [\n",
        "        chunk_scanner(chunk) for chunk in torch.arange(0, k_tokens, kv_chunk_size)\n",
        "    ]\n",
        "    acc_chunk = AttnChunk(*map(torch.stack, zip(*chunks)))\n",
        "    chunk_values, chunk_weights, chunk_max = acc_chunk\n",
        "\n",
        "    global_max, _ = torch.max(chunk_max, 0, keepdim=True)\n",
        "    max_diffs = torch.exp(chunk_max - global_max)\n",
        "    chunk_values *= torch.unsqueeze(max_diffs, -1)\n",
        "    chunk_weights *= max_diffs\n",
        "\n",
        "    all_values = chunk_values.sum(dim=0)\n",
        "    all_weights = torch.unsqueeze(chunk_weights, -1).sum(dim=0)\n",
        "    return all_values / all_weights\n",
        "\n",
        "\n",
        "# TODO: refactor CrossAttention#get_attention_scores to share code with this\n",
        "def _get_attention_scores_no_kv_chunking(\n",
        "    query: Tensor,\n",
        "    key: Tensor,\n",
        "    value: Tensor,\n",
        "    scale: float,\n",
        ") -> Tensor:\n",
        "    attn_scores = torch.baddbmm(\n",
        "        torch.empty(1, 1, 1, device=query.device, dtype=query.dtype),\n",
        "        query,\n",
        "        key.transpose(1,2),\n",
        "        alpha=scale,\n",
        "        beta=0,\n",
        "    )\n",
        "    attn_probs = attn_scores.softmax(dim=-1)\n",
        "    del attn_scores\n",
        "    hidden_states_slice = torch.bmm(attn_probs, value) if query.device.type == 'mps' else torch.bmm(attn_probs, value.to(attn_probs.dtype)).to(value.dtype)\n",
        "    return hidden_states_slice\n",
        "\n",
        "#sub_quadratic_attention\n",
        "class ScannedChunk(NamedTuple):\n",
        "    chunk_idx: int\n",
        "    attn_chunk: AttnChunk\n",
        "\n",
        "def efficient_dot_product_attention(\n",
        "    query: Tensor,\n",
        "    key: Tensor,\n",
        "    value: Tensor,\n",
        "    query_chunk_size=1024,\n",
        "    kv_chunk_size: Optional[int] = None,\n",
        "    kv_chunk_size_min: Optional[int] = None,\n",
        "    use_checkpoint=True,\n",
        "):\n",
        "    \"\"\"Computes efficient dot-product attention given query, key, and value.\n",
        "      This is efficient version of attention presented in\n",
        "      https://arxiv.org/abs/2112.05682v2 which comes with O(sqrt(n)) memory requirements.\n",
        "      Args:\n",
        "        query: queries for calculating attention with shape of\n",
        "          `[batch * num_heads, tokens, channels_per_head]`.\n",
        "        key: keys for calculating attention with shape of\n",
        "          `[batch * num_heads, tokens, channels_per_head]`.\n",
        "        value: values to be used in attention with shape of\n",
        "          `[batch * num_heads, tokens, channels_per_head]`.\n",
        "        query_chunk_size: int: query chunks size\n",
        "        kv_chunk_size: Optional[int]: key/value chunks size. if None: defaults to sqrt(key_tokens)\n",
        "        kv_chunk_size_min: Optional[int]: key/value minimum chunk size. only considered when kv_chunk_size is None. changes `sqrt(key_tokens)` into `max(sqrt(key_tokens), kv_chunk_size_min)`, to ensure our chunk sizes don't get too small (smaller chunks = more chunks = less concurrent work done).\n",
        "        use_checkpoint: bool: whether to use checkpointing (recommended True for training, False for inference)\n",
        "      Returns:\n",
        "        Output of shape `[batch * num_heads, query_tokens, channels_per_head]`.\n",
        "      \"\"\"\n",
        "    batch_x_heads, q_tokens, q_channels_per_head = query.shape\n",
        "    _, k_tokens, _ = key.shape\n",
        "    scale = q_channels_per_head ** -0.5\n",
        "\n",
        "    kv_chunk_size = min(kv_chunk_size or int(math.sqrt(k_tokens)), k_tokens)\n",
        "    if kv_chunk_size_min is not None:\n",
        "        kv_chunk_size = max(kv_chunk_size, kv_chunk_size_min)\n",
        "\n",
        "    def get_query_chunk(chunk_idx: int) -> Tensor:\n",
        "        return narrow_trunc(\n",
        "            query,\n",
        "            1,\n",
        "            chunk_idx,\n",
        "            min(query_chunk_size, q_tokens)\n",
        "        )\n",
        "    \n",
        "    summarize_chunk: SummarizeChunk = partial(_summarize_chunk, scale=scale)\n",
        "    summarize_chunk: SummarizeChunk = partial(checkpoint, summarize_chunk) if use_checkpoint else summarize_chunk\n",
        "    compute_query_chunk_attn: ComputeQueryChunkAttn = partial(\n",
        "        _get_attention_scores_no_kv_chunking,\n",
        "        scale=scale\n",
        "    ) if k_tokens <= kv_chunk_size else (\n",
        "        # fast-path for when there's just 1 key-value chunk per query chunk (this is just sliced attention btw)\n",
        "        partial(\n",
        "            _query_chunk_attention,\n",
        "            kv_chunk_size=kv_chunk_size,\n",
        "            summarize_chunk=summarize_chunk,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if q_tokens <= query_chunk_size:\n",
        "        # fast-path for when there's just 1 query chunk\n",
        "        return compute_query_chunk_attn(\n",
        "            query=query,\n",
        "            key=key,\n",
        "            value=value,\n",
        "        )\n",
        "    \n",
        "    # TODO: maybe we should use torch.empty_like(query) to allocate storage in-advance,\n",
        "    # and pass slices to be mutated, instead of torch.cat()ing the returned slices\n",
        "    res = torch.cat([\n",
        "        compute_query_chunk_attn(\n",
        "            query=get_query_chunk(i * query_chunk_size),\n",
        "            key=key,\n",
        "            value=value,\n",
        "        ) for i in range(math.ceil(q_tokens / query_chunk_size))\n",
        "    ], dim=1)\n",
        "    return res\n",
        "\n",
        "#cache\n",
        "import hashlib\n",
        "import json\n",
        "import os.path\n",
        "\n",
        "import filelock\n",
        "\n",
        "cache_filename = os.path.join(root.data_dir, \"cache.json\")\n",
        "cache_data = None\n",
        "\n",
        "\n",
        "def dump_cache():\n",
        "    with filelock.FileLock(cache_filename+\".lock\"):\n",
        "        with open(cache_filename, \"w\", encoding=\"utf8\") as file:\n",
        "            json.dump(cache_data, file, indent=4)\n",
        "\n",
        "\n",
        "def cache(subsection):\n",
        "    global cache_data\n",
        "\n",
        "    if cache_data is None:\n",
        "        with filelock.FileLock(cache_filename+\".lock\"):\n",
        "            if not os.path.isfile(cache_filename):\n",
        "                cache_data = {}\n",
        "            else:\n",
        "                with open(cache_filename, \"r\", encoding=\"utf8\") as file:\n",
        "                    cache_data = json.load(file)\n",
        "\n",
        "    s = cache_data.get(subsection, {})\n",
        "    cache_data[subsection] = s\n",
        "\n",
        "    return s\n",
        "\n",
        "\n",
        "def calculate_sha256(filename):\n",
        "    hash_sha256 = hashlib.sha256()\n",
        "    blksize = 1024 * 1024\n",
        "\n",
        "    with open(filename, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(blksize), b\"\"):\n",
        "            hash_sha256.update(chunk)\n",
        "\n",
        "    return hash_sha256.hexdigest()\n",
        "\n",
        "\n",
        "def sha256_from_cache(filename, title):\n",
        "    hashes = cache(\"hashes\")\n",
        "    ondisk_mtime = os.path.getmtime(filename)\n",
        "\n",
        "    if title not in hashes:\n",
        "        return None\n",
        "\n",
        "    cached_sha256 = hashes[title].get(\"sha256\", None)\n",
        "    cached_mtime = hashes[title].get(\"mtime\", 0)\n",
        "\n",
        "    if ondisk_mtime > cached_mtime or cached_sha256 is None:\n",
        "        return None\n",
        "\n",
        "    return cached_sha256\n",
        "\n",
        "\n",
        "def sha256(filename, title):\n",
        "    hashes = cache(\"hashes\")\n",
        "\n",
        "    sha256_value = sha256_from_cache(filename, title)\n",
        "    if sha256_value is not None:\n",
        "        return sha256_value\n",
        "\n",
        "    print(f\"Calculating sha256 for {filename}: \", end='')\n",
        "    sha256_value = calculate_sha256(filename)\n",
        "    print(f\"{sha256_value}\")\n",
        "\n",
        "    hashes[title] = {\n",
        "        \"mtime\": os.path.getmtime(filename),\n",
        "        \"sha256\": sha256_value,\n",
        "    }\n",
        "\n",
        "    dump_cache()\n",
        "\n",
        "    return sha256_value\n",
        "\n",
        "#hypernetwork\n",
        "loaded_hypernetworks=[]\n",
        "import csv\n",
        "import datetime\n",
        "import glob\n",
        "import html\n",
        "import os\n",
        "import sys\n",
        "import traceback\n",
        "import inspect\n",
        "\n",
        "import torch\n",
        "import tqdm\n",
        "from einops import rearrange, repeat\n",
        "from ldm.util import default\n",
        "from torch import einsum\n",
        "from torch.nn.init import normal_, xavier_normal_, xavier_uniform_, kaiming_normal_, kaiming_uniform_, zeros_\n",
        "\n",
        "from collections import defaultdict, deque\n",
        "from statistics import stdev, mean\n",
        "\n",
        "\n",
        "optimizer_dict = {optim_name : cls_obj for optim_name, cls_obj in inspect.getmembers(torch.optim, inspect.isclass) if optim_name != \"Optimizer\"}\n",
        "\n",
        "class HypernetworkModule(torch.nn.Module):\n",
        "    activation_dict = {\n",
        "        \"linear\": torch.nn.Identity,\n",
        "        \"relu\": torch.nn.ReLU,\n",
        "        \"leakyrelu\": torch.nn.LeakyReLU,\n",
        "        \"elu\": torch.nn.ELU,\n",
        "        \"swish\": torch.nn.Hardswish,\n",
        "        \"tanh\": torch.nn.Tanh,\n",
        "        \"sigmoid\": torch.nn.Sigmoid,\n",
        "    }\n",
        "    activation_dict.update({cls_name.lower(): cls_obj for cls_name, cls_obj in inspect.getmembers(torch.nn.modules.activation) if inspect.isclass(cls_obj) and cls_obj.__module__ == 'torch.nn.modules.activation'})\n",
        "\n",
        "    def __init__(self, dim, state_dict=None, layer_structure=None, activation_func=None, weight_init='Normal',\n",
        "                 add_layer_norm=False, activate_output=False, dropout_structure=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.multiplier = 1.0\n",
        "\n",
        "        assert layer_structure is not None, \"layer_structure must not be None\"\n",
        "        assert layer_structure[0] == 1, \"Multiplier Sequence should start with size 1!\"\n",
        "        assert layer_structure[-1] == 1, \"Multiplier Sequence should end with size 1!\"\n",
        "\n",
        "        linears = []\n",
        "        for i in range(len(layer_structure) - 1):\n",
        "\n",
        "            # Add a fully-connected layer\n",
        "            linears.append(torch.nn.Linear(int(dim * layer_structure[i]), int(dim * layer_structure[i+1])))\n",
        "\n",
        "            # Add an activation func except last layer\n",
        "            if activation_func == \"linear\" or activation_func is None or (i >= len(layer_structure) - 2 and not activate_output):\n",
        "                pass\n",
        "            elif activation_func in self.activation_dict:\n",
        "                linears.append(self.activation_dict[activation_func]())\n",
        "            else:\n",
        "                raise RuntimeError(f'hypernetwork uses an unsupported activation function: {activation_func}')\n",
        "\n",
        "            # Add layer normalization\n",
        "            if add_layer_norm:\n",
        "                linears.append(torch.nn.LayerNorm(int(dim * layer_structure[i+1])))\n",
        "\n",
        "            # Everything should be now parsed into dropout structure, and applied here.\n",
        "            # Since we only have dropouts after layers, dropout structure should start with 0 and end with 0.\n",
        "            if dropout_structure is not None and dropout_structure[i+1] > 0:\n",
        "                assert 0 < dropout_structure[i+1] < 1, \"Dropout probability should be 0 or float between 0 and 1!\"\n",
        "                linears.append(torch.nn.Dropout(p=dropout_structure[i+1]))\n",
        "            # Code explanation : [1, 2, 1] -> dropout is missing when last_layer_dropout is false. [1, 2, 2, 1] -> [0, 0.3, 0, 0], when its True, [0, 0.3, 0.3, 0].\n",
        "\n",
        "        self.linear = torch.nn.Sequential(*linears)\n",
        "\n",
        "        if state_dict is not None:\n",
        "            self.fix_old_state_dict(state_dict)\n",
        "            self.load_state_dict(state_dict)\n",
        "        else:\n",
        "            for layer in self.linear:\n",
        "                if type(layer) == torch.nn.Linear or type(layer) == torch.nn.LayerNorm:\n",
        "                    w, b = layer.weight.data, layer.bias.data\n",
        "                    if weight_init == \"Normal\" or type(layer) == torch.nn.LayerNorm:\n",
        "                        normal_(w, mean=0.0, std=0.01)\n",
        "                        normal_(b, mean=0.0, std=0)\n",
        "                    elif weight_init == 'XavierUniform':\n",
        "                        xavier_uniform_(w)\n",
        "                        zeros_(b)\n",
        "                    elif weight_init == 'XavierNormal':\n",
        "                        xavier_normal_(w)\n",
        "                        zeros_(b)\n",
        "                    elif weight_init == 'KaimingUniform':\n",
        "                        kaiming_uniform_(w, nonlinearity='leaky_relu' if 'leakyrelu' == activation_func else 'relu')\n",
        "                        zeros_(b)\n",
        "                    elif weight_init == 'KaimingNormal':\n",
        "                        kaiming_normal_(w, nonlinearity='leaky_relu' if 'leakyrelu' == activation_func else 'relu')\n",
        "                        zeros_(b)\n",
        "                    else:\n",
        "                        raise KeyError(f\"Key {weight_init} is not defined as initialization!\")\n",
        "        self.to(root.device)\n",
        "\n",
        "    def fix_old_state_dict(self, state_dict):\n",
        "        changes = {\n",
        "            'linear1.bias': 'linear.0.bias',\n",
        "            'linear1.weight': 'linear.0.weight',\n",
        "            'linear2.bias': 'linear.1.bias',\n",
        "            'linear2.weight': 'linear.1.weight',\n",
        "        }\n",
        "\n",
        "        for fr, to in changes.items():\n",
        "            x = state_dict.get(fr, None)\n",
        "            if x is None:\n",
        "                continue\n",
        "\n",
        "            del state_dict[fr]\n",
        "            state_dict[to] = x\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.linear(x) * (self.multiplier if not self.training else 1)\n",
        "\n",
        "    def trainables(self):\n",
        "        layer_structure = []\n",
        "        for layer in self.linear:\n",
        "            if type(layer) == torch.nn.Linear or type(layer) == torch.nn.LayerNorm:\n",
        "                layer_structure += [layer.weight, layer.bias]\n",
        "        return layer_structure\n",
        "\n",
        "\n",
        "#param layer_structure : sequence used for length, use_dropout : controlling boolean, last_layer_dropout : for compatibility check.\n",
        "def parse_dropout_structure(layer_structure, use_dropout, last_layer_dropout):\n",
        "    if layer_structure is None:\n",
        "        layer_structure = [1, 2, 1]\n",
        "    if not use_dropout:\n",
        "        return [0] * len(layer_structure)\n",
        "    dropout_values = [0]\n",
        "    dropout_values.extend([0.3] * (len(layer_structure) - 3))\n",
        "    if last_layer_dropout:\n",
        "        dropout_values.append(0.3)\n",
        "    else:\n",
        "        dropout_values.append(0)\n",
        "    dropout_values.append(0)\n",
        "    return dropout_values\n",
        "\n",
        "def shorthash(self):\n",
        "        sha256 = sha256(self.filename, f'hypernet/{self.name}')\n",
        "\n",
        "        return sha256[0:10]\n",
        "\n",
        "class Hypernetwork:\n",
        "    filename = None\n",
        "    name = None\n",
        "\n",
        "    def __init__(self, name=None, enable_sizes=None, layer_structure=None, activation_func=None, weight_init=None, add_layer_norm=False, use_dropout=False, activate_output=False, **kwargs):\n",
        "        self.filename = None\n",
        "        self.name = name\n",
        "        self.layers = {}\n",
        "        self.step = 0\n",
        "        self.sd_checkpoint = None\n",
        "        self.sd_checkpoint_name = None\n",
        "        self.layer_structure = layer_structure\n",
        "        self.activation_func = activation_func\n",
        "        self.weight_init = weight_init\n",
        "        self.add_layer_norm = add_layer_norm\n",
        "        self.use_dropout = use_dropout\n",
        "        self.activate_output = activate_output\n",
        "        self.last_layer_dropout = kwargs.get('last_layer_dropout', True)\n",
        "        self.dropout_structure = kwargs.get('dropout_structure', None)\n",
        "        if self.dropout_structure is None:\n",
        "            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n",
        "        self.optimizer_name = None\n",
        "        self.optimizer_state_dict = None\n",
        "        self.optional_info = None\n",
        "\n",
        "        for size in enable_sizes or []:\n",
        "            self.layers[size] = (\n",
        "                HypernetworkModule(size, None, self.layer_structure, self.activation_func, self.weight_init,\n",
        "                                   self.add_layer_norm, self.activate_output, dropout_structure=self.dropout_structure),\n",
        "                HypernetworkModule(size, None, self.layer_structure, self.activation_func, self.weight_init,\n",
        "                                   self.add_layer_norm, self.activate_output, dropout_structure=self.dropout_structure),\n",
        "            )\n",
        "        self.eval()\n",
        "\n",
        "    def weights(self):\n",
        "        res = []\n",
        "        for k, layers in self.layers.items():\n",
        "            for layer in layers:\n",
        "                res += layer.parameters()\n",
        "        return res\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        for k, layers in self.layers.items():\n",
        "            for layer in layers:\n",
        "                layer.train(mode=mode)\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = mode\n",
        "\n",
        "    def to(self, device):\n",
        "        for k, layers in self.layers.items():\n",
        "            for layer in layers:\n",
        "                layer.to(device)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def set_multiplier(self, multiplier):\n",
        "        for k, layers in self.layers.items():\n",
        "            for layer in layers:\n",
        "                layer.multiplier = multiplier\n",
        "\n",
        "        return self\n",
        "\n",
        "    def eval(self):\n",
        "        for k, layers in self.layers.items():\n",
        "            for layer in layers:\n",
        "                layer.eval()\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "    def save(self, filename):\n",
        "        state_dict = {}\n",
        "        optimizer_saved_dict = {}\n",
        "\n",
        "        for k, v in self.layers.items():\n",
        "            state_dict[k] = (v[0].state_dict(), v[1].state_dict())\n",
        "\n",
        "        state_dict['step'] = self.step\n",
        "        state_dict['name'] = self.name\n",
        "        state_dict['layer_structure'] = self.layer_structure\n",
        "        state_dict['activation_func'] = self.activation_func\n",
        "        state_dict['is_layer_norm'] = self.add_layer_norm\n",
        "        state_dict['weight_initialization'] = self.weight_init\n",
        "        state_dict['sd_checkpoint'] = self.sd_checkpoint\n",
        "        state_dict['sd_checkpoint_name'] = self.sd_checkpoint_name\n",
        "        state_dict['activate_output'] = self.activate_output\n",
        "        state_dict['use_dropout'] = self.use_dropout\n",
        "        state_dict['dropout_structure'] = self.dropout_structure\n",
        "        state_dict['last_layer_dropout'] = (self.dropout_structure[-2] != 0) if self.dropout_structure is not None else self.last_layer_dropout\n",
        "        state_dict['optional_info'] = self.optional_info if self.optional_info else None\n",
        "\n",
        "        if self.optimizer_name is not None:\n",
        "            optimizer_saved_dict['optimizer_name'] = self.optimizer_name\n",
        "\n",
        "        torch.save(state_dict, filename)\n",
        "        save_optimizer_state=True\n",
        "        if save_optimizer_state and self.optimizer_state_dict:\n",
        "            optimizer_saved_dict['hash'] = self.shorthash()\n",
        "            optimizer_saved_dict['optimizer_state_dict'] = self.optimizer_state_dict\n",
        "            torch.save(optimizer_saved_dict, filename + '.optim')\n",
        "    \n",
        "    def sha256(filename, title):\n",
        "        hashes = cache(\"hashes\")\n",
        "\n",
        "        sha256_value = sha256_from_cache(filename, title)\n",
        "        if sha256_value is not None:\n",
        "            return sha256_value\n",
        "\n",
        "        print(f\"Calculating sha256 for {filename}: \", end='')\n",
        "        sha256_value = calculate_sha256(filename)\n",
        "        print(f\"{sha256_value}\")\n",
        "\n",
        "        hashes[title] = {\n",
        "            \"mtime\": os.path.getmtime(filename),\n",
        "            \"sha256\": sha256_value,\n",
        "        }\n",
        "\n",
        "        dump_cache()\n",
        "\n",
        "        return sha256_value\n",
        "    def load(self, filename):\n",
        "        self.filename = filename\n",
        "        if self.name is None:\n",
        "            self.name = os.path.splitext(os.path.basename(filename))[0]\n",
        "\n",
        "        state_dict = torch.load(filename, map_location='cpu')\n",
        "\n",
        "        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n",
        "        self.optional_info = state_dict.get('optional_info', None)\n",
        "        self.activation_func = state_dict.get('activation_func', None)\n",
        "        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n",
        "        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n",
        "        self.dropout_structure = state_dict.get('dropout_structure', None)\n",
        "        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n",
        "        self.activate_output = state_dict.get('activate_output', True)\n",
        "        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n",
        "        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n",
        "        if self.dropout_structure is None:\n",
        "            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n",
        "\n",
        "        print_hypernet_extra=True\n",
        "        if print_hypernet_extra:\n",
        "            if self.optional_info is not None:\n",
        "                print(f\"  INFO:\\n {self.optional_info}\\n\")\n",
        "\n",
        "            print(f\"  Layer structure: {self.layer_structure}\")\n",
        "            print(f\"  Activation function: {self.activation_func}\")\n",
        "            print(f\"  Weight initialization: {self.weight_init}\")\n",
        "            print(f\"  Layer norm: {self.add_layer_norm}\")\n",
        "            print(f\"  Dropout usage: {self.use_dropout}\" )\n",
        "            print(f\"  Activate last layer: {self.activate_output}\")\n",
        "            print(f\"  Dropout structure: {self.dropout_structure}\")\n",
        "\n",
        "        optimizer_saved_dict = torch.load(self.filename + '.optim', map_location='cpu') if os.path.exists(self.filename + '.optim') else {}\n",
        "\n",
        "        if self.shorthash() == optimizer_saved_dict.get('hash', None):\n",
        "            self.optimizer_state_dict = optimizer_saved_dict.get('optimizer_state_dict', None)\n",
        "        else:\n",
        "            self.optimizer_state_dict = None\n",
        "        if self.optimizer_state_dict:\n",
        "            self.optimizer_name = optimizer_saved_dict.get('optimizer_name', 'AdamW')\n",
        "            if print_hypernet_extra:\n",
        "                print(\"Loaded existing optimizer from checkpoint\")\n",
        "                print(f\"Optimizer name is {self.optimizer_name}\")\n",
        "        else:\n",
        "            self.optimizer_name = \"AdamW\"\n",
        "            if print_hypernet_extra:\n",
        "                print(\"No saved optimizer exists in checkpoint\")\n",
        "\n",
        "        for size, sd in state_dict.items():\n",
        "            if type(size) == int:\n",
        "                self.layers[size] = (\n",
        "                    HypernetworkModule(size, sd[0], self.layer_structure, self.activation_func, self.weight_init,\n",
        "                                       self.add_layer_norm, self.activate_output, self.dropout_structure),\n",
        "                    HypernetworkModule(size, sd[1], self.layer_structure, self.activation_func, self.weight_init,\n",
        "                                       self.add_layer_norm, self.activate_output, self.dropout_structure),\n",
        "                )\n",
        "\n",
        "        self.name = state_dict.get('name', self.name)\n",
        "        self.step = state_dict.get('step', 0)\n",
        "        self.sd_checkpoint = state_dict.get('sd_checkpoint', None)\n",
        "        self.sd_checkpoint_name = state_dict.get('sd_checkpoint_name', None)\n",
        "        self.eval()\n",
        "    \n",
        "    import hashlib\n",
        "\n",
        "    def shorthash(self):\n",
        "        sha256 = hashlib.sha256()\n",
        "        sha256.update(f'{self.filename}hypernet/{self.name}'.encode())\n",
        "        print(f\"sha256= \", sha256.hexdigest()[0:10])\n",
        "        return sha256.hexdigest()[0:10]\n",
        "\n",
        "\n",
        "def list_hypernetworks(path):\n",
        "    res = {}\n",
        "    for filename in sorted(glob.iglob(os.path.join(path, '**/*.pt'), recursive=True)):\n",
        "        name = os.path.splitext(os.path.basename(filename))[0]\n",
        "        # Prevent a hypothetical \"None.pt\" from being listed.\n",
        "        if name != \"None\":\n",
        "            res[name] = filename\n",
        "    return res\n",
        "\n",
        "\n",
        "def load_hypernetwork(name):\n",
        "    path = hypernetworks.get(name, None)\n",
        "\n",
        "    if path is None:\n",
        "        return None\n",
        "\n",
        "    hypernetwork = Hypernetwork()\n",
        "\n",
        "    try:\n",
        "        hypernetwork.load(path)\n",
        "    except Exception:\n",
        "        print(f\"Error loading hypernetwork {path}\", file=sys.stderr)\n",
        "        print(traceback.format_exc(), file=sys.stderr)\n",
        "        return None\n",
        "\n",
        "    return hypernetwork\n",
        "\n",
        "\n",
        "def load_hypernetworks(names, multipliers=None):\n",
        "    already_loaded = {}\n",
        "\n",
        "    for hypernetwork in loaded_hypernetworks:\n",
        "        if hypernetwork.name in names:\n",
        "            already_loaded[hypernetwork.name] = hypernetwork\n",
        "\n",
        "    loaded_hypernetworks.clear()\n",
        "\n",
        "    for i, name in enumerate(names):\n",
        "        hypernetwork = already_loaded.get(name, None)\n",
        "        if hypernetwork is None:\n",
        "            hypernetwork = load_hypernetwork(name)\n",
        "\n",
        "        if hypernetwork is None:\n",
        "            continue\n",
        "\n",
        "        hypernetwork.set_multiplier(multipliers[i] if multipliers else 1.0)\n",
        "        loaded_hypernetworks.append(hypernetwork)\n",
        "    print(f\"Hypernetworks Loaded\", names)\n",
        "\n",
        "def find_closest_hypernetwork_name(search: str):\n",
        "    if not search:\n",
        "        return None\n",
        "    search = search.lower()\n",
        "    applicable = [name for name in hypernetworks if search in name.lower()]\n",
        "    if not applicable:\n",
        "        return None\n",
        "    applicable = sorted(applicable, key=lambda name: len(name))\n",
        "    return applicable[0]\n",
        "\n",
        "\n",
        "def apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=None):\n",
        "    hypernetwork_layers = (hypernetwork.layers if hypernetwork is not None else {}).get(context_k.shape[2], None)\n",
        "\n",
        "    if hypernetwork_layers is None:\n",
        "        return context_k, context_v\n",
        "\n",
        "    if layer is not None:\n",
        "        layer.hyper_k = hypernetwork_layers[0]\n",
        "        layer.hyper_v = hypernetwork_layers[1]\n",
        "\n",
        "    context_k = hypernetwork_layers[0](context_k)\n",
        "    context_v = hypernetwork_layers[1](context_v)\n",
        "    return context_k, context_v\n",
        "\n",
        "\n",
        "def apply_hypernetworks(hypernetworks, context, layer=None):\n",
        "    context_k = context\n",
        "    context_v = context\n",
        "    for hypernetwork in hypernetworks:\n",
        "        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n",
        "\n",
        "    return context_k, context_v\n",
        "\n",
        "\n",
        "def attention_CrossAttention_forward(self, x, context=None, mask=None):\n",
        "    h = self.heads\n",
        "\n",
        "    q = self.to_q(x)\n",
        "    context = default(context, x)\n",
        "\n",
        "    context_k, context_v = apply_hypernetworks(loaded_hypernetworks, context, self)\n",
        "    k = self.to_k(context_k)\n",
        "    v = self.to_v(context_v)\n",
        "\n",
        "    q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n",
        "\n",
        "    sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
        "\n",
        "    if mask is not None:\n",
        "        mask = rearrange(mask, 'b ... -> b (...)')\n",
        "        max_neg_value = -torch.finfo(sim.dtype).max\n",
        "        mask = repeat(mask, 'b j -> (b h) () j', h=h)\n",
        "        sim.masked_fill_(~mask, max_neg_value)\n",
        "\n",
        "    # attention, what we cannot get enough of\n",
        "    attn = sim.softmax(dim=-1)\n",
        "\n",
        "    out = einsum('b i j, b j d -> b i d', attn, v)\n",
        "    out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
        "    return self.to_out(out)\n",
        "\n",
        "\n",
        "def stack_conds(conds):\n",
        "    if len(conds) == 1:\n",
        "        return torch.stack(conds)\n",
        "\n",
        "    # same as in reconstruct_multicond_batch\n",
        "    token_count = max([x.shape[0] for x in conds])\n",
        "    for i in range(len(conds)):\n",
        "        if conds[i].shape[0] != token_count:\n",
        "            last_vector = conds[i][-1:]\n",
        "            last_vector_repeated = last_vector.repeat([token_count - conds[i].shape[0], 1])\n",
        "            conds[i] = torch.vstack([conds[i], last_vector_repeated])\n",
        "\n",
        "    return torch.stack(conds)\n",
        "\n",
        "\n",
        "def statistics(data):\n",
        "    if len(data) < 2:\n",
        "        std = 0\n",
        "    else:\n",
        "        std = stdev(data)\n",
        "    total_information = f\"loss:{mean(data):.3f}\" + u\"\\u00B1\" + f\"({std/ (len(data) ** 0.5):.3f})\"\n",
        "    recent_data = data[-32:]\n",
        "    if len(recent_data) < 2:\n",
        "        std = 0\n",
        "    else:\n",
        "        std = stdev(recent_data)\n",
        "    recent_information = f\"recent 32 loss:{mean(recent_data):.3f}\" + u\"\\u00B1\" + f\"({std / (len(recent_data) ** 0.5):.3f})\"\n",
        "    return total_information, recent_information\n",
        "\n",
        "\n",
        "def report_statistics(loss_info:dict):\n",
        "    keys = sorted(loss_info.keys(), key=lambda x: sum(loss_info[x]) / len(loss_info[x]))\n",
        "    for key in keys:\n",
        "        try:\n",
        "            print(\"Loss statistics for file \" + key)\n",
        "            info, recent = statistics(list(loss_info[key]))\n",
        "            print(info)\n",
        "            print(recent)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "def create_hypernetwork(name, enable_sizes, overwrite_old, layer_structure=None, activation_func=None, weight_init=None, add_layer_norm=False, use_dropout=False, dropout_structure=None):\n",
        "    # Remove illegal characters from name.\n",
        "    name = \"\".join( x for x in name if (x.isalnum() or x in \"._- \"))\n",
        "    assert name, \"Name cannot be empty!\"\n",
        "\n",
        "    fn = os.path.join(root.hypernetwork_dir, f\"{name}.pt\")\n",
        "    if not overwrite_old:\n",
        "        assert not os.path.exists(fn), f\"file {fn} already exists\"\n",
        "\n",
        "    if type(layer_structure) == str:\n",
        "        layer_structure = [float(x.strip()) for x in layer_structure.split(\",\")]\n",
        "\n",
        "    if use_dropout and dropout_structure and type(dropout_structure) == str:\n",
        "        dropout_structure = [float(x.strip()) for x in dropout_structure.split(\",\")]\n",
        "    else:\n",
        "        dropout_structure = [0] * len(layer_structure)\n",
        "\n",
        "    hypernet = Hypernetwork(\n",
        "        name=name,\n",
        "        enable_sizes=[int(x) for x in enable_sizes],\n",
        "        layer_structure=layer_structure,\n",
        "        activation_func=activation_func,\n",
        "        weight_init=weight_init,\n",
        "        add_layer_norm=add_layer_norm,\n",
        "        use_dropout=use_dropout,\n",
        "        dropout_structure=dropout_structure\n",
        "    )\n",
        "    hypernet.save(fn)\n",
        "\n",
        "    reload_hypernetworks()\n",
        "\n",
        "#apply_optimizations\n",
        "hypernetwork=Hypernetwork()\n",
        "import math\n",
        "import sys\n",
        "import traceback\n",
        "import psutil\n",
        "import contextlib\n",
        "import torch\n",
        "from torch import einsum\n",
        "\n",
        "from ldm.util import default\n",
        "from einops import rearrange\n",
        "\n",
        "if root.use_xformers:\n",
        "    try:\n",
        "        import xformers.ops\n",
        "        xformers_available = True\n",
        "    except Exception:\n",
        "        print(\"Cannot import xformers\", file=sys.stderr)\n",
        "        print(traceback.format_exc(), file=sys.stderr)\n",
        "\n",
        "\n",
        "def get_available_vram():\n",
        "    if root.device == 'cuda':\n",
        "        stats = torch.cuda.memory_stats(root.device)\n",
        "        mem_active = stats['active_bytes.all.current']\n",
        "        mem_reserved = stats['reserved_bytes.all.current']\n",
        "        mem_free_cuda, _ = torch.cuda.mem_get_info(torch.cuda.current_device())\n",
        "        mem_free_torch = mem_reserved - mem_active\n",
        "        mem_free_total = mem_free_cuda + mem_free_torch\n",
        "        return mem_free_total\n",
        "    else:\n",
        "        return psutil.virtual_memory().available\n",
        "\n",
        "# see https://github.com/basujindal/stable-diffusion/pull/117 for discussion\n",
        "def split_cross_attention_forward_v1(self, x, context=None, mask=None):\n",
        "    h = self.heads\n",
        "\n",
        "    q_in = self.to_q(x)\n",
        "    context = default(context, x)\n",
        "\n",
        "    context_k, context_v = hypernetwork.apply_hypernetworks(loaded_hypernetworks, context)\n",
        "    k_in = self.to_k(context_k)\n",
        "    v_in = self.to_v(context_v)\n",
        "    del context, context_k, context_v, x\n",
        "\n",
        "    q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q_in, k_in, v_in))\n",
        "    del q_in, k_in, v_in\n",
        "\n",
        "    dtype = q.dtype\n",
        "    upcast_attn=True\n",
        "    if upcast_attn:\n",
        "        q, k, v = q.float(), k.float(), v.float()\n",
        "    \n",
        "    upcast_attn=True\n",
        "    with without_autocast(disable=not upcast_attn):\n",
        "        r1 = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device, dtype=q.dtype)\n",
        "        for i in range(0, q.shape[0], 2):\n",
        "            end = i + 2\n",
        "            s1 = einsum('b i d, b j d -> b i j', q[i:end], k[i:end])\n",
        "            s1 *= self.scale\n",
        "    \n",
        "            s2 = s1.softmax(dim=-1)\n",
        "            del s1\n",
        "    \n",
        "            r1[i:end] = einsum('b i j, b j d -> b i d', s2, v[i:end])\n",
        "            del s2\n",
        "        del q, k, v\n",
        "\n",
        "    r1 = r1.to(dtype)\n",
        "\n",
        "    r2 = rearrange(r1, '(b h) n d -> b n (h d)', h=h)\n",
        "    del r1\n",
        "\n",
        "    return self.to_out(r2)\n",
        "\n",
        "\n",
        "# taken from https://github.com/Doggettx/stable-diffusion and modified\n",
        "def split_cross_attention_forward(self, x, context=None, mask=None):\n",
        "    h = self.heads\n",
        "\n",
        "    q_in = self.to_q(x)\n",
        "    context = default(context, x)\n",
        "\n",
        "    context_k, context_v = hypernetwork.apply_hypernetworks(loaded_hypernetworks, context)\n",
        "    k_in = self.to_k(context_k)\n",
        "    v_in = self.to_v(context_v)\n",
        "\n",
        "    dtype = q_in.dtype\n",
        "    upcast_attn=True\n",
        "    if upcast_attn:\n",
        "        q_in, k_in, v_in = q_in.float(), k_in.float(), v_in if v_in.device.type == 'mps' else v_in.float()\n",
        "    upcast_attn=True\n",
        "    with without_autocast(disable=not upcast_attn):\n",
        "        k_in = k_in * self.scale\n",
        "    \n",
        "        del context, x\n",
        "    \n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q_in, k_in, v_in))\n",
        "        del q_in, k_in, v_in\n",
        "    \n",
        "        r1 = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device, dtype=q.dtype)\n",
        "    \n",
        "        mem_free_total = get_available_vram()\n",
        "    \n",
        "        gb = 1024 ** 3\n",
        "        tensor_size = q.shape[0] * q.shape[1] * k.shape[1] * q.element_size()\n",
        "        modifier = 3 if q.element_size() == 2 else 2.5\n",
        "        mem_required = tensor_size * modifier\n",
        "        steps = 1\n",
        "    \n",
        "        if mem_required > mem_free_total:\n",
        "            steps = 2 ** (math.ceil(math.log(mem_required / mem_free_total, 2)))\n",
        "            # print(f\"Expected tensor size:{tensor_size/gb:0.1f}GB, cuda free:{mem_free_cuda/gb:0.1f}GB \"\n",
        "            #       f\"torch free:{mem_free_torch/gb:0.1f} total:{mem_free_total/gb:0.1f} steps:{steps}\")\n",
        "    \n",
        "        if steps > 64:\n",
        "            max_res = math.floor(math.sqrt(math.sqrt(mem_free_total / 2.5)) / 8) * 64\n",
        "            raise RuntimeError(f'Not enough memory, use lower resolution (max approx. {max_res}x{max_res}). '\n",
        "                               f'Need: {mem_required / 64 / gb:0.1f}GB free, Have:{mem_free_total / gb:0.1f}GB free')\n",
        "    \n",
        "        slice_size = q.shape[1] // steps if (q.shape[1] % steps) == 0 else q.shape[1]\n",
        "        for i in range(0, q.shape[1], slice_size):\n",
        "            end = i + slice_size\n",
        "            s1 = einsum('b i d, b j d -> b i j', q[:, i:end], k)\n",
        "    \n",
        "            s2 = s1.softmax(dim=-1, dtype=q.dtype)\n",
        "            del s1\n",
        "    \n",
        "            r1[:, i:end] = einsum('b i j, b j d -> b i d', s2, v)\n",
        "            del s2\n",
        "    \n",
        "        del q, k, v\n",
        "\n",
        "    r1 = r1.to(dtype)\n",
        "\n",
        "    r2 = rearrange(r1, '(b h) n d -> b n (h d)', h=h)\n",
        "    del r1\n",
        "\n",
        "    return self.to_out(r2)\n",
        "\n",
        "\n",
        "# -- Taken from https://github.com/invoke-ai/InvokeAI and modified --\n",
        "mem_total_gb = psutil.virtual_memory().total // (1 << 30)\n",
        "\n",
        "def einsum_op_compvis(q, k, v):\n",
        "    s = einsum('b i d, b j d -> b i j', q, k)\n",
        "    s = s.softmax(dim=-1, dtype=s.dtype)\n",
        "    return einsum('b i j, b j d -> b i d', s, v)\n",
        "\n",
        "def einsum_op_slice_0(q, k, v, slice_size):\n",
        "    r = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device, dtype=q.dtype)\n",
        "    for i in range(0, q.shape[0], slice_size):\n",
        "        end = i + slice_size\n",
        "        r[i:end] = einsum_op_compvis(q[i:end], k[i:end], v[i:end])\n",
        "    return r\n",
        "\n",
        "def einsum_op_slice_1(q, k, v, slice_size):\n",
        "    r = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device, dtype=q.dtype)\n",
        "    for i in range(0, q.shape[1], slice_size):\n",
        "        end = i + slice_size\n",
        "        r[:, i:end] = einsum_op_compvis(q[:, i:end], k, v)\n",
        "    return r\n",
        "\n",
        "def einsum_op_mps_v1(q, k, v):\n",
        "    if q.shape[0] * q.shape[1] <= 2**16: # (512x512) max q.shape[1]: 4096\n",
        "        return einsum_op_compvis(q, k, v)\n",
        "    else:\n",
        "        slice_size = math.floor(2**30 / (q.shape[0] * q.shape[1]))\n",
        "        if slice_size % 4096 == 0:\n",
        "            slice_size -= 1\n",
        "        return einsum_op_slice_1(q, k, v, slice_size)\n",
        "\n",
        "def einsum_op_mps_v2(q, k, v):\n",
        "    if mem_total_gb > 8 and q.shape[0] * q.shape[1] <= 2**16:\n",
        "        return einsum_op_compvis(q, k, v)\n",
        "    else:\n",
        "        return einsum_op_slice_0(q, k, v, 1)\n",
        "\n",
        "def einsum_op_tensor_mem(q, k, v, max_tensor_mb):\n",
        "    size_mb = q.shape[0] * q.shape[1] * k.shape[1] * q.element_size() // (1 << 20)\n",
        "    if size_mb <= max_tensor_mb:\n",
        "        return einsum_op_compvis(q, k, v)\n",
        "    div = 1 << int((size_mb - 1) / max_tensor_mb).bit_length()\n",
        "    if div <= q.shape[0]:\n",
        "        return einsum_op_slice_0(q, k, v, q.shape[0] // div)\n",
        "    return einsum_op_slice_1(q, k, v, max(q.shape[1] // div, 1))\n",
        "\n",
        "def einsum_op_cuda(q, k, v):\n",
        "    stats = torch.cuda.memory_stats(q.device)\n",
        "    mem_active = stats['active_bytes.all.current']\n",
        "    mem_reserved = stats['reserved_bytes.all.current']\n",
        "    mem_free_cuda, _ = torch.cuda.mem_get_info(q.device)\n",
        "    mem_free_torch = mem_reserved - mem_active\n",
        "    mem_free_total = mem_free_cuda + mem_free_torch\n",
        "    # Divide factor of safety as there's copying and fragmentation\n",
        "    return einsum_op_tensor_mem(q, k, v, mem_free_total / 3.3 / (1 << 20))\n",
        "\n",
        "def einsum_op(q, k, v):\n",
        "    if q.device.type == 'cuda':\n",
        "        return einsum_op_cuda(q, k, v)\n",
        "\n",
        "    if q.device.type == 'mps':\n",
        "        if mem_total_gb >= 32 and q.shape[0] % 32 != 0 and q.shape[0] * q.shape[1] < 2**18:\n",
        "            return einsum_op_mps_v1(q, k, v)\n",
        "        return einsum_op_mps_v2(q, k, v)\n",
        "\n",
        "    # Smaller slices are faster due to L2/L3/SLC caches.\n",
        "    # Tested on i7 with 8MB L3 cache.\n",
        "    return einsum_op_tensor_mem(q, k, v, 32)\n",
        "\n",
        "def split_cross_attention_forward_invokeAI(self, x, context=None, mask=None):\n",
        "    h = self.heads\n",
        "\n",
        "    q = self.to_q(x)\n",
        "    context = default(context, x)\n",
        "\n",
        "    context_k, context_v = hypernetwork.apply_hypernetworks(loaded_hypernetworks, context)\n",
        "    k = self.to_k(context_k)\n",
        "    v = self.to_v(context_v)\n",
        "    del context, context_k, context_v, x\n",
        "\n",
        "    dtype = q.dtype\n",
        "    upcast_attn=True\n",
        "    if upcast_attn:\n",
        "        q, k, v = q.float(), k.float(), v if v.device.type == 'mps' else v.float()\n",
        "\n",
        "    with without_autocast(disable=not upcast_attn):\n",
        "        k = k * self.scale\n",
        "    \n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n",
        "        r = einsum_op(q, k, v)\n",
        "    r = r.to(dtype)\n",
        "    return self.to_out(rearrange(r, '(b h) n d -> b n (h d)', h=h))\n",
        "\n",
        "# -- End of code from https://github.com/invoke-ai/InvokeAI --\n",
        "\n",
        "\n",
        "# Based on Birch-san's modified implementation of sub-quadratic attention from https://github.com/Birch-san/diffusers/pull/1\n",
        "# The sub_quad_attention_forward function is under the MIT License listed under Memory Efficient Attention in the Licenses section of the web UI interface\n",
        "def sub_quad_attention_forward(self, x, context=None, mask=None):\n",
        "    assert mask is None, \"attention-mask not currently implemented for SubQuadraticCrossAttnProcessor.\"\n",
        "\n",
        "    h = self.heads\n",
        "\n",
        "    q = self.to_q(x)\n",
        "    context = default(context, x)\n",
        "\n",
        "    context_k, context_v = hypernetwork.apply_hypernetworks(loaded_hypernetworks, context)\n",
        "    k = self.to_k(context_k)\n",
        "    v = self.to_v(context_v)\n",
        "    del context, context_k, context_v, x\n",
        "\n",
        "    q = q.unflatten(-1, (h, -1)).transpose(1,2).flatten(end_dim=1)\n",
        "    k = k.unflatten(-1, (h, -1)).transpose(1,2).flatten(end_dim=1)\n",
        "    v = v.unflatten(-1, (h, -1)).transpose(1,2).flatten(end_dim=1)\n",
        "\n",
        "    dtype = q.dtype\n",
        "    upcast_attn=True\n",
        "    if upcast_attn:\n",
        "        q, k = q.float(), k.float()\n",
        "    sub_quad_q_chunk_size=1024\n",
        "    x = sub_quad_attention(q, k, v, q_chunk_size=sub_quad_q_chunk_size, kv_chunk_size=sub_quad_kv_chunk_size, chunk_threshold=shared.cmd_opts.sub_quad_chunk_threshold, use_checkpoint=self.training)\n",
        "\n",
        "    x = x.to(dtype)\n",
        "\n",
        "    x = x.unflatten(0, (-1, h)).transpose(1,2).flatten(start_dim=2)\n",
        "\n",
        "    out_proj, dropout = self.to_out\n",
        "    x = out_proj(x)\n",
        "    x = dropout(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def sub_quad_attention(q, k, v, q_chunk_size=1024, kv_chunk_size=None, kv_chunk_size_min=None, chunk_threshold=None, use_checkpoint=True):\n",
        "    bytes_per_token = torch.finfo(q.dtype).bits//8\n",
        "    batch_x_heads, q_tokens, _ = q.shape\n",
        "    _, k_tokens, _ = k.shape\n",
        "    qk_matmul_size_bytes = batch_x_heads * bytes_per_token * q_tokens * k_tokens\n",
        "\n",
        "    if chunk_threshold is None:\n",
        "        chunk_threshold_bytes = int(get_available_vram() * 0.9) if q.device.type == 'mps' else int(get_available_vram() * 0.7)\n",
        "    elif chunk_threshold == 0:\n",
        "        chunk_threshold_bytes = None\n",
        "    else:\n",
        "        chunk_threshold_bytes = int(0.01 * chunk_threshold * get_available_vram())\n",
        "\n",
        "    if kv_chunk_size_min is None and chunk_threshold_bytes is not None:\n",
        "        kv_chunk_size_min = chunk_threshold_bytes // (batch_x_heads * bytes_per_token * (k.shape[2] + v.shape[2]))\n",
        "    elif kv_chunk_size_min == 0:\n",
        "        kv_chunk_size_min = None\n",
        "\n",
        "    if chunk_threshold_bytes is not None and qk_matmul_size_bytes <= chunk_threshold_bytes:\n",
        "        # the big matmul fits into our memory limit; do everything in 1 chunk,\n",
        "        # i.e. send it down the unchunked fast-path\n",
        "        query_chunk_size = q_tokens\n",
        "        kv_chunk_size = k_tokens\n",
        "\n",
        "    with without_autocast(disable=q.dtype == v.dtype):\n",
        "        return efficient_dot_product_attention(\n",
        "            q,\n",
        "            k,\n",
        "            v,\n",
        "            query_chunk_size=q_chunk_size,\n",
        "            kv_chunk_size=kv_chunk_size,\n",
        "            kv_chunk_size_min = kv_chunk_size_min,\n",
        "            use_checkpoint=use_checkpoint,\n",
        "        )\n",
        "\n",
        "xformers_flash_attention=True\n",
        "def get_xformers_flash_attention_op(q, k, v):\n",
        "    if not xformers_flash_attention:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        flash_attention_op = xformers.ops.MemoryEfficientAttentionFlashAttentionOp\n",
        "        fw, bw = flash_attention_op\n",
        "        if fw.supports(xformers.ops.fmha.Inputs(query=q, key=k, value=v, attn_bias=None)):\n",
        "            return flash_attention_op\n",
        "    except Exception as e:\n",
        "        display_once(e, \"enabling flash attention\")\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def xformers_attention_forward(self, x, context=None, mask=None):\n",
        "    h = self.heads\n",
        "    q_in = self.to_q(x)\n",
        "    context = default(context, x)\n",
        "\n",
        "    context_k, context_v = hypernetwork.apply_hypernetworks(loaded_hypernetworks, context)\n",
        "    k_in = self.to_k(context_k)\n",
        "    v_in = self.to_v(context_v)\n",
        "\n",
        "    q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b n h d', h=h), (q_in, k_in, v_in))\n",
        "    del q_in, k_in, v_in\n",
        "\n",
        "    dtype = q.dtype\n",
        "    upcast_attn=True\n",
        "    if upcast_attn:\n",
        "        q, k = q.float(), k.float()\n",
        "\n",
        "    out = xformers.ops.memory_efficient_attention(q, k, v, attn_bias=None, op=get_xformers_flash_attention_op(q, k, v))\n",
        "\n",
        "    out = out.to(dtype)\n",
        "\n",
        "    out = rearrange(out, 'b n h d -> b n (h d)', h=h)\n",
        "    return self.to_out(out)\n",
        "\n",
        "def cross_attention_attnblock_forward(self, x):\n",
        "        h_ = x\n",
        "        h_ = self.norm(h_)\n",
        "        q1 = self.q(h_)\n",
        "        k1 = self.k(h_)\n",
        "        v = self.v(h_)\n",
        "\n",
        "        # compute attention\n",
        "        b, c, h, w = q1.shape\n",
        "\n",
        "        q2 = q1.reshape(b, c, h*w)\n",
        "        del q1\n",
        "\n",
        "        q = q2.permute(0, 2, 1)   # b,hw,c\n",
        "        del q2\n",
        "\n",
        "        k = k1.reshape(b, c, h*w) # b,c,hw\n",
        "        del k1\n",
        "\n",
        "        h_ = torch.zeros_like(k, device=q.device)\n",
        "\n",
        "        mem_free_total = get_available_vram()\n",
        "\n",
        "        tensor_size = q.shape[0] * q.shape[1] * k.shape[2] * q.element_size()\n",
        "        mem_required = tensor_size * 2.5\n",
        "        steps = 1\n",
        "\n",
        "        if mem_required > mem_free_total:\n",
        "            steps = 2**(math.ceil(math.log(mem_required / mem_free_total, 2)))\n",
        "\n",
        "        slice_size = q.shape[1] // steps if (q.shape[1] % steps) == 0 else q.shape[1]\n",
        "        for i in range(0, q.shape[1], slice_size):\n",
        "            end = i + slice_size\n",
        "\n",
        "            w1 = torch.bmm(q[:, i:end], k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
        "            w2 = w1 * (int(c)**(-0.5))\n",
        "            del w1\n",
        "            w3 = torch.nn.functional.softmax(w2, dim=2, dtype=q.dtype)\n",
        "            del w2\n",
        "\n",
        "            # attend to values\n",
        "            v1 = v.reshape(b, c, h*w)\n",
        "            w4 = w3.permute(0, 2, 1)   # b,hw,hw (first hw of k, second of q)\n",
        "            del w3\n",
        "\n",
        "            h_[:, :, i:end] = torch.bmm(v1, w4)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
        "            del v1, w4\n",
        "\n",
        "        h2 = h_.reshape(b, c, h, w)\n",
        "        del h_\n",
        "\n",
        "        h3 = self.proj_out(h2)\n",
        "        del h2\n",
        "\n",
        "        h3 += x\n",
        "\n",
        "        return h3\n",
        "    \n",
        "def xformers_attnblock_forward(self, x):\n",
        "    try:\n",
        "        h_ = x\n",
        "        h_ = self.norm(h_)\n",
        "        q = self.q(h_)\n",
        "        k = self.k(h_)\n",
        "        v = self.v(h_)\n",
        "        b, c, h, w = q.shape\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b c h w -> b (h w) c'), (q, k, v))\n",
        "        dtype = q.dtype\n",
        "        upcast_attn=True\n",
        "        if upcast_attn:\n",
        "            q, k = q.float(), k.float()\n",
        "        q = q.contiguous()\n",
        "        k = k.contiguous()\n",
        "        v = v.contiguous()\n",
        "        out = xformers.ops.memory_efficient_attention(q, k, v, op=get_xformers_flash_attention_op(q, k, v))\n",
        "        out = out.to(dtype)\n",
        "        out = rearrange(out, 'b (h w) c -> b c h w', h=h)\n",
        "        out = self.proj_out(out)\n",
        "        return x + out\n",
        "    except NotImplementedError:\n",
        "        return cross_attention_attnblock_forward(self, x)\n",
        "\n",
        "def sub_quad_attnblock_forward(self, x):\n",
        "    h_ = x\n",
        "    h_ = self.norm(h_)\n",
        "    q = self.q(h_)\n",
        "    k = self.k(h_)\n",
        "    v = self.v(h_)\n",
        "    b, c, h, w = q.shape\n",
        "    q, k, v = map(lambda t: rearrange(t, 'b c h w -> b (h w) c'), (q, k, v))\n",
        "    q = q.contiguous()\n",
        "    k = k.contiguous()\n",
        "    v = v.contiguous()\n",
        "    sub_quad_q_chunk_size=1024\n",
        "    sub_quad_kv_chunk_size=None\n",
        "    sub_quad_chunk_threshold=None\n",
        "    out = sub_quad_attention(q, k, v, q_chunk_size=sub_quad_q_chunk_size, kv_chunk_size=sub_quad_kv_chunk_size, chunk_threshold=sub_quad_chunk_threshold, use_checkpoint=self.training)\n",
        "    out = rearrange(out, 'b (h w) c -> b c h w', h=h)\n",
        "    out = self.proj_out(out)\n",
        "    return x + out\n",
        "\n",
        "#sd_hijack_unet\n",
        "import torch\n",
        "from packaging import version\n",
        "cpu = torch.device(\"cpu\")\n",
        "device = device_interrogate = device_gfpgan = device_esrgan = device_codeformer = None\n",
        "dtype = torch.float32\n",
        "dtype_vae = torch.float16\n",
        "dtype_unet = torch.float16\n",
        "unet_needs_upcast = False\n",
        "precision = \"full\"\n",
        "\n",
        "def autocast(disable=False):\n",
        "    \n",
        "    if disable:\n",
        "        return contextlib.nullcontext()\n",
        "\n",
        "    if dtype == torch.float32 or precision == \"full\":\n",
        "        return contextlib.nullcontext()\n",
        "\n",
        "    return torch.autocast(\"cuda\")\n",
        "\n",
        "class TorchHijackForUnet:\n",
        "    \"\"\"\n",
        "    This is torch, but with cat that resizes tensors to appropriate dimensions if they do not match;\n",
        "    this makes it possible to create pictures with dimensions that are multiples of 8 rather than 64\n",
        "    \"\"\"\n",
        "\n",
        "    def __getattr__(self, item):\n",
        "        if item == 'cat':\n",
        "            return self.cat\n",
        "\n",
        "        if hasattr(torch, item):\n",
        "            return getattr(torch, item)\n",
        "\n",
        "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(type(self).__name__, item))\n",
        "\n",
        "    def cat(self, tensors, *args, **kwargs):\n",
        "        if len(tensors) == 2:\n",
        "            a, b = tensors\n",
        "            if a.shape[-2:] != b.shape[-2:]:\n",
        "                a = torch.nn.functional.interpolate(a, b.shape[-2:], mode=\"nearest\")\n",
        "\n",
        "            tensors = (a, b)\n",
        "\n",
        "        return torch.cat(tensors, *args, **kwargs)\n",
        "\n",
        "\n",
        "#th = TorchHijackForUnet()\n",
        "\n",
        "\n",
        "# Below are monkey patches to enable upcasting a float16 UNet for float32 sampling\n",
        "def apply_model(orig_func, self, x_noisy, t, cond, **kwargs):\n",
        "\n",
        "    if isinstance(cond, dict):\n",
        "        for y in cond.keys():\n",
        "            cond[y] = [x.to(dtype_unet) if isinstance(x, torch.Tensor) else x for x in cond[y]]\n",
        "\n",
        "    with autocast():\n",
        "        return orig_func(self, x_noisy.to(dtype_unet), t.to(dtype_unet), cond, **kwargs).float()\n",
        "unet_needs_upcast=True\n",
        "class GELUHijack(torch.nn.GELU, torch.nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        torch.nn.GELU.__init__(self, *args, **kwargs)\n",
        "    def forward(self, x):\n",
        "        if unet_needs_upcast:\n",
        "            return torch.nn.GELU.forward(self.float(), x.float()).to(dtype_unet)\n",
        "        else:\n",
        "            return torch.nn.GELU.forward(self, x)\n",
        "\n",
        "unet_needs_upcast = lambda *args, **kwargs: unet_needs_upcast\n",
        "CondFunc('ldm.models.diffusion.ddpm.LatentDiffusion.apply_model', apply_model, unet_needs_upcast)\n",
        "CondFunc('ldm.modules.diffusionmodules.openaimodel.timestep_embedding', lambda orig_func, timesteps, *args, **kwargs: orig_func(timesteps, *args, **kwargs).to(torch.float32 if timesteps.dtype == torch.int64 else dtype_unet), unet_needs_upcast)\n",
        "if version.parse(torch.__version__) <= version.parse(\"1.13.1\"):\n",
        "    CondFunc('ldm.modules.diffusionmodules.util.GroupNorm32.forward', lambda orig_func, self, *args, **kwargs: orig_func(self.float(), *args, **kwargs), unet_needs_upcast)\n",
        "    CondFunc('ldm.modules.attention.GEGLU.forward', lambda orig_func, self, x: orig_func(self.float(), x.float()).to(dtype_unet), unet_needs_upcast)\n",
        "    CondFunc('open_clip.transformer.ResidualAttentionBlock.__init__', lambda orig_func, *args, **kwargs: kwargs.update({'act_layer': GELUHijack}) and False or orig_func(*args, **kwargs), lambda _, *args, **kwargs: kwargs.get('act_layer') is None or kwargs['act_layer'] == torch.nn.GELU)\n",
        "\n",
        "first_stage_cond = lambda _, self, *args, **kwargs: unet_needs_upcast and self.model.diffusion_model.dtype == torch.float16\n",
        "first_stage_sub = lambda orig_func, self, x, **kwargs: orig_func(self, x.to(dtype_vae), **kwargs)\n",
        "CondFunc('ldm.models.diffusion.ddpm.LatentDiffusion.decode_first_stage', first_stage_sub, first_stage_cond)\n",
        "CondFunc('ldm.models.diffusion.ddpm.LatentDiffusion.encode_first_stage', first_stage_sub, first_stage_cond)\n",
        "CondFunc('ldm.models.diffusion.ddpm.LatentDiffusion.get_first_stage_encoding', lambda orig_func, *args, **kwargs: orig_func(*args, **kwargs).float(), first_stage_cond)\n",
        "\n",
        "#@title sd_hijack\n",
        "import torch\n",
        "from torch.nn.functional import silu\n",
        "import ldm.modules.attention\n",
        "import ldm.modules.diffusionmodules.model\n",
        "import ldm.modules.diffusionmodules.openaimodel\n",
        "import ldm.models.diffusion.ddim\n",
        "import ldm.models.diffusion.plms\n",
        "import ldm.modules.encoders.modules\n",
        "\n",
        "attention_CrossAttention_forward = ldm.modules.attention.CrossAttention.forward\n",
        "diffusionmodules_model_nonlinearity = ldm.modules.diffusionmodules.model.nonlinearity\n",
        "diffusionmodules_model_AttnBlock_forward = ldm.modules.diffusionmodules.model.AttnBlock.forward\n",
        "\n",
        "# new memory efficient cross attention blocks do not support hypernets and we already\n",
        "# have memory efficient cross attention anyway, so this disables SD2.0's memory efficient cross attention\n",
        "ldm.modules.attention.MemoryEfficientCrossAttention = ldm.modules.attention.CrossAttention\n",
        "ldm.modules.attention.BasicTransformerBlock.ATTENTION_MODES[\"softmax-xformers\"] = ldm.modules.attention.CrossAttention\n",
        "\n",
        "# silence new console spam from SD2\n",
        "ldm.modules.attention.print = lambda *args: None\n",
        "ldm.modules.diffusionmodules.model.print = lambda *args: None\n",
        "th = TorchHijackForUnet()\n",
        "def apply_optimizations():\n",
        "    undo_optimizations()\n",
        "\n",
        "    ldm.modules.diffusionmodules.model.nonlinearity = silu\n",
        "    ldm.modules.diffusionmodules.openaimodel.th = th\n",
        "    \n",
        "    optimization_method = None\n",
        "\n",
        "    if root.use_xformers and torch.version.cuda and (6, 0) <= torch.cuda.get_device_capability(root.device) <= (9, 0):\n",
        "        print(\"Applying xformers cross attention optimization.\")\n",
        "        ldm.modules.attention.CrossAttention.forward = xformers_attention_forward\n",
        "        ldm.modules.diffusionmodules.model.AttnBlock.forward = xformers_attnblock_forward\n",
        "        optimization_method = 'xformers'\n",
        "    elif root.use_sub_quad_attention and not root.use_xformers:\n",
        "        print(\"Applying sub-quadratic cross attention optimization.\")\n",
        "        ldm.modules.attention.CrossAttention.forward = sub_quad_attention_forward\n",
        "        ldm.modules.diffusionmodules.model.AttnBlock.forward = sub_quad_attnblock_forward\n",
        "        optimization_method = 'sub-quadratic'\n",
        "    elif root.use_split_attention_v1 and not root.use_sub_quad_attention and not root.use_xformers:\n",
        "        print(\"Applying v1 cross attention optimization.\")\n",
        "        ldm.modules.attention.CrossAttention.forward = split_cross_attention_forward_v1\n",
        "        optimization_method = 'V1'\n",
        "    elif not disable_opt_split_attention and (root.use_split_cross_attention_forward_invokeAI or not opt_split_attention and not torch.cuda.is_available()):\n",
        "        print(\"Applying cross attention optimization (InvokeAI).\")\n",
        "        ldm.modules.attention.CrossAttention.forward = split_cross_attention_forward_invokeAI\n",
        "        optimization_method = 'InvokeAI'\n",
        "    elif not disable_opt_split_attention and (opt_split_attention or torch.cuda.is_available()):\n",
        "        print(\"Applying cross attention optimization (Doggettx).\")\n",
        "        ldm.modules.attention.CrossAttention.forward = split_cross_attention_forward\n",
        "        ldm.modules.diffusionmodules.model.AttnBlock.forward = cross_attention_attnblock_forward\n",
        "        optimization_method = 'Doggettx'\n",
        "\n",
        "    return optimization_method\n",
        "\n",
        "\n",
        "def undo_optimizations():\n",
        "    ldm.modules.attention.CrossAttention.forward = attention_CrossAttention_forward\n",
        "    ldm.modules.diffusionmodules.model.nonlinearity = diffusionmodules_model_nonlinearity\n",
        "    ldm.modules.diffusionmodules.model.AttnBlock.forward = diffusionmodules_model_AttnBlock_forward\n",
        "\n",
        "\n",
        "def fix_checkpoint():\n",
        "    \"\"\"checkpoints are now added and removed in embedding/hypernet code, since torch doesn't want\n",
        "    checkpoints to be added when not training (there's a warning)\"\"\"\n",
        "\n",
        "    pass\n",
        "\n",
        "from transformers import BertPreTrainedModel,BertModel,BertConfig\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from transformers.models.xlm_roberta.configuration_xlm_roberta import XLMRobertaConfig\n",
        "from transformers import XLMRobertaModel,XLMRobertaTokenizer\n",
        "from typing import Optional\n",
        "\n",
        "class BertSeriesConfig(BertConfig):\n",
        "    def __init__(self, vocab_size=30522, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072, hidden_act=\"gelu\", hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1, max_position_embeddings=512, type_vocab_size=2, initializer_range=0.02, layer_norm_eps=1e-12, pad_token_id=0, position_embedding_type=\"absolute\", use_cache=True, classifier_dropout=None,project_dim=512, pooler_fn=\"average\",learn_encoder=False,model_type='bert',**kwargs):\n",
        "\n",
        "        super().__init__(vocab_size, hidden_size, num_hidden_layers, num_attention_heads, intermediate_size, hidden_act, hidden_dropout_prob, attention_probs_dropout_prob, max_position_embeddings, type_vocab_size, initializer_range, layer_norm_eps, pad_token_id, position_embedding_type, use_cache, classifier_dropout, **kwargs)\n",
        "        self.project_dim = project_dim\n",
        "        self.pooler_fn = pooler_fn\n",
        "        self.learn_encoder = learn_encoder\n",
        "\n",
        "class RobertaSeriesConfig(XLMRobertaConfig):\n",
        "    def __init__(self, pad_token_id=1, bos_token_id=0, eos_token_id=2,project_dim=512,pooler_fn='cls',learn_encoder=False, **kwargs):\n",
        "        super().__init__(pad_token_id=pad_token_id, bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n",
        "        self.project_dim = project_dim\n",
        "        self.pooler_fn = pooler_fn\n",
        "        self.learn_encoder = learn_encoder\n",
        "\n",
        "\n",
        "class BertSeriesModelWithTransformation(BertPreTrainedModel):\n",
        "\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n",
        "    config_class = BertSeriesConfig\n",
        "\n",
        "    def __init__(self, config=None, **kargs):\n",
        "        # modify initialization for autoloading \n",
        "        if config is None:\n",
        "            config = XLMRobertaConfig()\n",
        "            config.attention_probs_dropout_prob= 0.1\n",
        "            config.bos_token_id=0\n",
        "            config.eos_token_id=2\n",
        "            config.hidden_act='gelu'\n",
        "            config.hidden_dropout_prob=0.1\n",
        "            config.hidden_size=1024\n",
        "            config.initializer_range=0.02\n",
        "            config.intermediate_size=4096\n",
        "            config.layer_norm_eps=1e-05\n",
        "            config.max_position_embeddings=514\n",
        "\n",
        "            config.num_attention_heads=16\n",
        "            config.num_hidden_layers=24\n",
        "            config.output_past=True\n",
        "            config.pad_token_id=1\n",
        "            config.position_embedding_type= \"absolute\"\n",
        "\n",
        "            config.type_vocab_size= 1\n",
        "            config.use_cache=True\n",
        "            config.vocab_size= 250002\n",
        "            config.project_dim = 768\n",
        "            config.learn_encoder = False\n",
        "        super().__init__(config)\n",
        "        self.roberta = XLMRobertaModel(config)\n",
        "        self.transformation = nn.Linear(config.hidden_size,config.project_dim)\n",
        "        self.pre_LN=nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')\n",
        "        self.pooler = lambda x: x[:,0]\n",
        "        self.post_init()\n",
        "\n",
        "    def encode(self,c):\n",
        "        device = next(self.parameters()).device\n",
        "        text = self.tokenizer(c,\n",
        "                        truncation=True,\n",
        "                        max_length=77,\n",
        "                        return_length=False,\n",
        "                        return_overflowing_tokens=False,\n",
        "                        padding=\"max_length\",\n",
        "                        return_tensors=\"pt\")\n",
        "        text[\"input_ids\"] = torch.tensor(text[\"input_ids\"]).to(device)\n",
        "        text[\"attention_mask\"] = torch.tensor(\n",
        "            text['attention_mask']).to(device)\n",
        "        features = self(**text)\n",
        "        return features['projection_state'] \n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "    ) :\n",
        "        r\"\"\"\n",
        "        \"\"\"\n",
        "\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        # last module outputs\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "\n",
        "        # project every module\n",
        "        sequence_output_ln = self.pre_LN(sequence_output)\n",
        "\n",
        "        # pooler\n",
        "        pooler_output = self.pooler(sequence_output_ln)\n",
        "        pooler_output = self.transformation(pooler_output)\n",
        "        projection_state = self.transformation(outputs.last_hidden_state)\n",
        "\n",
        "        return {\n",
        "            'pooler_output':pooler_output,\n",
        "            'last_hidden_state':outputs.last_hidden_state,\n",
        "            'hidden_states':outputs.hidden_states,\n",
        "            'attentions':outputs.attentions,\n",
        "            'projection_state':projection_state,\n",
        "            'sequence_out': sequence_output\n",
        "        }\n",
        "\n",
        "\n",
        "class RobertaSeriesModelWithTransformation(BertSeriesModelWithTransformation):\n",
        "    base_model_prefix = 'roberta'\n",
        "    config_class= RobertaSeriesConfig\n",
        "\n",
        "class StableDiffusionModelHijack:\n",
        "    fixes = None\n",
        "    comments = []\n",
        "    layers = None\n",
        "    circular_enabled = False\n",
        "    clip = None\n",
        "    optimization_method = None\n",
        "\n",
        "    embedding_db = EmbeddingDatabase()\n",
        "\n",
        "    def __init__(self):\n",
        "        self.embedding_db.add_embedding_dir(root.embeddings_dir)\n",
        "\n",
        "    def hijack(self, m):\n",
        "\n",
        "        # if type(m.cond_stage_model) == BertSeriesModelWithTransformation:\n",
        "        #     model_embeddings = m.cond_stage_model.roberta.embeddings\n",
        "        #     model_embeddings.token_embedding = EmbeddingsWithFixes(model_embeddings.word_embeddings, self)\n",
        "        #     m.cond_stage_model = FrozenXLMREmbedderWithCustomWords(m.cond_stage_model, self)\n",
        "\n",
        "        if type(m.cond_stage_model) == ldm.modules.encoders.modules.FrozenCLIPEmbedder:\n",
        "            model_embeddings = m.cond_stage_model.transformer.text_model.embeddings\n",
        "            model_embeddings.token_embedding = EmbeddingsWithFixes(model_embeddings.token_embedding, self)\n",
        "            m.cond_stage_model = FrozenCLIPEmbedderWithCustomWords(m.cond_stage_model, self)\n",
        "\n",
        "        elif type(m.cond_stage_model) == ldm.modules.encoders.modules.FrozenOpenCLIPEmbedder:\n",
        "            m.cond_stage_model.model.token_embedding = EmbeddingsWithFixes(m.cond_stage_model.model.token_embedding, self)\n",
        "            m.cond_stage_model = FrozenOpenCLIPEmbedderWithCustomWords(m.cond_stage_model, self)\n",
        "\n",
        "        optimization_method = apply_optimizations()\n",
        "\n",
        "        self.clip = m.cond_stage_model\n",
        "\n",
        "        def flatten(el):\n",
        "            flattened = [flatten(children) for children in el.children()]\n",
        "            res = [el]\n",
        "            for c in flattened:\n",
        "                res += c\n",
        "            return res\n",
        "\n",
        "        self.layers = flatten(m)\n",
        "\n",
        "    def undo_hijack(self, m):\n",
        "        \n",
        "        if type(m.cond_stage_model) == FrozenCLIPEmbedderWithCustomWords:\n",
        "            m.cond_stage_model = m.cond_stage_model.wrapped\n",
        "\n",
        "            model_embeddings = m.cond_stage_model.transformer.text_model.embeddings\n",
        "            if type(model_embeddings.token_embedding) == EmbeddingsWithFixes:\n",
        "                model_embeddings.token_embedding = model_embeddings.token_embedding.wrapped\n",
        "        elif type(m.cond_stage_model) == FrozenOpenCLIPEmbedderWithCustomWords:\n",
        "            m.cond_stage_model.wrapped.model.token_embedding = m.cond_stage_model.wrapped.model.token_embedding.wrapped\n",
        "            m.cond_stage_model = m.cond_stage_model.wrapped\n",
        "\n",
        "        self.apply_circular(False)\n",
        "        self.layers = None\n",
        "        self.clip = None\n",
        "\n",
        "    def apply_circular(self, enable):\n",
        "        if self.circular_enabled == enable:\n",
        "            return\n",
        "\n",
        "        self.circular_enabled = enable\n",
        "\n",
        "        for layer in [layer for layer in self.layers if type(layer) == torch.nn.Conv2d]:\n",
        "            layer.padding_mode = 'circular' if enable else 'zeros'\n",
        "\n",
        "    def clear_comments(self):\n",
        "        self.comments = []\n",
        "\n",
        "    def get_prompt_lengths(self, text):\n",
        "        _, token_count = self.clip.process_texts([text])\n",
        "\n",
        "        return token_count, self.clip.get_target_prompt_token_count(token_count)\n",
        "\n",
        "\n",
        "class EmbeddingsWithFixes(torch.nn.Module):\n",
        "    def __init__(self, wrapped, embeddings):\n",
        "        super().__init__()\n",
        "        self.wrapped = wrapped\n",
        "        self.embeddings = embeddings\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        batch_fixes = self.embeddings.fixes\n",
        "        self.embeddings.fixes = None\n",
        "\n",
        "        inputs_embeds = self.wrapped(input_ids)\n",
        "\n",
        "        if batch_fixes is None or len(batch_fixes) == 0 or max([len(x) for x in batch_fixes]) == 0:\n",
        "            return inputs_embeds\n",
        "\n",
        "        vecs = []\n",
        "        for fixes, tensor in zip(batch_fixes, inputs_embeds):\n",
        "            for offset, embedding in fixes:\n",
        "                emb = embedding.vec\n",
        "                emb_len = min(tensor.shape[0] - offset - 1, emb.shape[0])\n",
        "                tensor = torch.cat([tensor[0:offset + 1], emb[0:emb_len], tensor[offset + 1 + emb_len:]])\n",
        "\n",
        "            vecs.append(tensor)\n",
        "\n",
        "        return torch.stack(vecs)\n",
        "\n",
        "\n",
        "def add_circular_option_to_conv_2d():\n",
        "    conv2d_constructor = torch.nn.Conv2d.__init__\n",
        "\n",
        "    def conv2d_constructor_circular(self, *args, **kwargs):\n",
        "        return conv2d_constructor(self, *args, padding_mode='circular', **kwargs)\n",
        "\n",
        "    torch.nn.Conv2d.__init__ = conv2d_constructor_circular\n",
        "\n",
        "\n",
        "model_hijack = StableDiffusionModelHijack()\n",
        "\n",
        "\n",
        "def register_buffer(self, name, attr):\n",
        "    \"\"\"\n",
        "    Fix register buffer bug for Mac OS.\n",
        "    \"\"\"\n",
        "\n",
        "    if type(attr) == torch.Tensor:\n",
        "        if attr.device != root.device:\n",
        "            attr = attr.to(device=root.device, dtype=(torch.float32 if root.device.type == 'mps' else None))\n",
        "\n",
        "    setattr(self, name, attr)\n",
        "\n",
        "\n",
        "ldm.models.diffusion.ddim.DDIMSampler.register_buffer = register_buffer\n",
        "ldm.models.diffusion.plms.PLMSSampler.register_buffer = register_buffer\n",
        "\n",
        "#@title hijack_checkpoint\n",
        "\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "\n",
        "import ldm.modules.attention\n",
        "import ldm.modules.diffusionmodules.openaimodel\n",
        "\n",
        "\n",
        "def BasicTransformerBlock_forward(self, x, context=None):\n",
        "    return checkpoint(self._forward, x, context)\n",
        "\n",
        "\n",
        "def AttentionBlock_forward(self, x):\n",
        "    return checkpoint(self._forward, x)\n",
        "\n",
        "\n",
        "def ResBlock_forward(self, x, emb):\n",
        "    return checkpoint(self._forward, x, emb)\n",
        "\n",
        "\n",
        "stored = []\n",
        "\n",
        "\n",
        "def hijack_checkpoint_add():\n",
        "    if len(stored) != 0:\n",
        "        return\n",
        "\n",
        "    stored.extend([\n",
        "        ldm.modules.attention.BasicTransformerBlock.forward,\n",
        "        ldm.modules.diffusionmodules.openaimodel.ResBlock.forward,\n",
        "        ldm.modules.diffusionmodules.openaimodel.AttentionBlock.forward\n",
        "    ])\n",
        "\n",
        "    ldm.modules.attention.BasicTransformerBlock.forward = BasicTransformerBlock_forward\n",
        "    ldm.modules.diffusionmodules.openaimodel.ResBlock.forward = ResBlock_forward\n",
        "    ldm.modules.diffusionmodules.openaimodel.AttentionBlock.forward = AttentionBlock_forward\n",
        "\n",
        "\n",
        "def hijack_checkpoint_remove():\n",
        "    if len(stored) == 0:\n",
        "        return\n",
        "\n",
        "    ldm.modules.attention.BasicTransformerBlock.forward = stored[0]\n",
        "    ldm.modules.diffusionmodules.openaimodel.ResBlock.forward = stored[1]\n",
        "    ldm.modules.diffusionmodules.openaimodel.AttentionBlock.forward = stored[2]\n",
        "\n",
        "    stored.clear()\n",
        "\n",
        "\n",
        "#@title hijack_clip\n",
        "\n",
        "import math\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "\n",
        "class PromptChunk:\n",
        "    \"\"\"\n",
        "    This object contains token ids, weight (multipliers:1.4) and textual inversion embedding info for a chunk of prompt.\n",
        "    If a prompt is short, it is represented by one PromptChunk, otherwise, multiple are necessary.\n",
        "    Each PromptChunk contains an exact amount of tokens - 77, which includes one for start and end token,\n",
        "    so just 75 tokens from prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.tokens = []\n",
        "        self.multipliers = []\n",
        "        self.fixes = []\n",
        "\n",
        "\n",
        "PromptChunkFix = namedtuple('PromptChunkFix', ['offset', 'embedding'])\n",
        "\"\"\"An object of this type is a marker showing that textual inversion embedding's vectors have to placed at offset in the prompt\n",
        "chunk. Thos objects are found in PromptChunk.fixes and, are placed into FrozenCLIPEmbedderWithCustomWordsBase.hijack.fixes, and finally\n",
        "are applied by sd_hijack.EmbeddingsWithFixes's forward function.\"\"\"\n",
        "\n",
        "\n",
        "class FrozenCLIPEmbedderWithCustomWordsBase(torch.nn.Module):\n",
        "    \"\"\"A pytorch module that is a wrapper for FrozenCLIPEmbedder module. it enhances FrozenCLIPEmbedder, making it possible to\n",
        "    have unlimited prompt length and assign weights to tokens in prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, wrapped, hijack):\n",
        "        super().__init__()\n",
        "\n",
        "        self.wrapped = wrapped\n",
        "        \"\"\"Original FrozenCLIPEmbedder module; can also be FrozenOpenCLIPEmbedder or xlmr.BertSeriesModelWithTransformation,\n",
        "        depending on model.\"\"\"\n",
        "\n",
        "        self.hijack: StableDiffusionModelHijack = hijack\n",
        "        self.chunk_length = 75\n",
        "\n",
        "    def empty_chunk(self):\n",
        "        \"\"\"creates an empty PromptChunk and returns it\"\"\"\n",
        "\n",
        "        chunk = PromptChunk()\n",
        "        chunk.tokens = [self.id_start] + [self.id_end] * (self.chunk_length + 1)\n",
        "        chunk.multipliers = [1.0] * (self.chunk_length + 2)\n",
        "        return chunk\n",
        "\n",
        "    def get_target_prompt_token_count(self, token_count):\n",
        "        \"\"\"returns the maximum number of tokens a prompt of a known length can have before it requires one more PromptChunk to be represented\"\"\"\n",
        "\n",
        "        return math.ceil(max(token_count, 1) / self.chunk_length) * self.chunk_length\n",
        "\n",
        "    def tokenize(self, texts):\n",
        "        \"\"\"Converts a batch of texts into a batch of token ids\"\"\"\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def encode_with_transformers(self, tokens):\n",
        "        \"\"\"\n",
        "        converts a batch of token ids (in python lists) into a single tensor with numeric respresentation of those tokens;\n",
        "        All python lists with tokens are assumed to have same length, usually 77.\n",
        "        if input is a list with B elements and each element has T tokens, expected output shape is (B, T, C), where C depends on\n",
        "        model - can be 768 and 1024.\n",
        "        Among other things, this call will read self.hijack.fixes, apply it to its inputs, and clear it (setting it to None).\n",
        "        \"\"\"\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def encode_embedding_init_text(self, init_text, nvpt):\n",
        "        \"\"\"Converts text into a tensor with this text's tokens' embeddings. Note that those are embeddings before they are passed through\n",
        "        transformers. nvpt is used as a maximum length in tokens. If text produces less teokens than nvpt, only this many is returned.\"\"\"\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def tokenize_line(self, line):\n",
        "        \"\"\"\n",
        "        this transforms a single prompt into a list of PromptChunk objects - as many as needed to\n",
        "        represent the prompt.\n",
        "        Returns the list and the total number of tokens in the prompt.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        parsed = [[line, 1.0]]\n",
        "\n",
        "        tokenized = self.tokenize([text for text, _ in parsed])\n",
        "\n",
        "        chunks = []\n",
        "        chunk = PromptChunk()\n",
        "        token_count = 0\n",
        "        last_comma = -1\n",
        "\n",
        "        def next_chunk(is_last=False):\n",
        "            \"\"\"puts current chunk into the list of results and produces the next one - empty;\n",
        "            if is_last is true, tokens <end-of-text> tokens at the end won't add to token_count\"\"\"\n",
        "            nonlocal token_count\n",
        "            nonlocal last_comma\n",
        "            nonlocal chunk\n",
        "\n",
        "            if is_last:\n",
        "                token_count += len(chunk.tokens)\n",
        "            else:\n",
        "                token_count += self.chunk_length\n",
        "\n",
        "            to_add = self.chunk_length - len(chunk.tokens)\n",
        "            if to_add > 0:\n",
        "                chunk.tokens += [self.id_end] * to_add\n",
        "                chunk.multipliers += [1.0] * to_add\n",
        "\n",
        "            chunk.tokens = [self.id_start] + chunk.tokens + [self.id_end]\n",
        "            chunk.multipliers = [1.0] + chunk.multipliers + [1.0]\n",
        "\n",
        "            last_comma = -1\n",
        "            chunks.append(chunk)\n",
        "            chunk = PromptChunk()\n",
        "\n",
        "        for tokens, (text, weight) in zip(tokenized, parsed):\n",
        "            if text == 'BREAK' and weight == -1:\n",
        "                next_chunk()\n",
        "                continue\n",
        "\n",
        "            position = 0\n",
        "            while position < len(tokens):\n",
        "                token = tokens[position]\n",
        "\n",
        "                if token == self.comma_token:\n",
        "                    last_comma = len(chunk.tokens)\n",
        "                # this is when we are at the end of alloted 75 tokens for the current chunk, and the current token is not a comma. opts.comma_padding_backtrack\n",
        "                # is a setting that specifies that if there is a comma nearby, the text after the comma should be moved out of this chunk and into the next.\n",
        "                # comma_padding_backtrack=74    \n",
        "                # elif comma_padding_backtrack != 0 and len(chunk.tokens) == self.chunk_length and last_comma != -1 and len(chunk.tokens) - last_comma <= comma_padding_backtrack:\n",
        "                #     break_location = last_comma + 1\n",
        "\n",
        "                #     reloc_tokens = chunk.tokens[break_location:]\n",
        "                #     reloc_mults = chunk.multipliers[break_location:]\n",
        "\n",
        "                #     chunk.tokens = chunk.tokens[:break_location]\n",
        "                #     chunk.multipliers = chunk.multipliers[:break_location]\n",
        "\n",
        "                #     next_chunk()\n",
        "                #     chunk.tokens = reloc_tokens\n",
        "                #     chunk.multipliers = reloc_mults\n",
        "\n",
        "                if len(chunk.tokens) == self.chunk_length:\n",
        "                    next_chunk()\n",
        "\n",
        "                embedding, embedding_length_in_tokens = self.hijack.embedding_db.find_embedding_at_position(tokens, position)\n",
        "                if embedding is None:\n",
        "                    chunk.tokens.append(token)\n",
        "                    chunk.multipliers.append(weight)\n",
        "                    position += 1\n",
        "                    continue\n",
        "\n",
        "                emb_len = int(embedding.vec.shape[0])\n",
        "                if len(chunk.tokens) + emb_len > self.chunk_length:\n",
        "                    next_chunk()\n",
        "\n",
        "                chunk.fixes.append(PromptChunkFix(len(chunk.tokens), embedding))\n",
        "\n",
        "                chunk.tokens += [0] * emb_len\n",
        "                chunk.multipliers += [weight] * emb_len\n",
        "                position += embedding_length_in_tokens\n",
        "\n",
        "        if len(chunk.tokens) > 0 or len(chunks) == 0:\n",
        "            next_chunk(is_last=True)\n",
        "\n",
        "        return chunks, token_count\n",
        "\n",
        "    def process_texts(self, texts):\n",
        "        \"\"\"\n",
        "        Accepts a list of texts and calls tokenize_line() on each, with cache. Returns the list of results and maximum\n",
        "        length, in tokens, of all texts.\n",
        "        \"\"\"\n",
        "\n",
        "        token_count = 0\n",
        "\n",
        "        cache = {}\n",
        "        batch_chunks = []\n",
        "        for line in texts:\n",
        "            if line in cache:\n",
        "                chunks = cache[line]\n",
        "            else:\n",
        "                chunks, current_token_count = self.tokenize_line(line)\n",
        "                token_count = max(current_token_count, token_count)\n",
        "\n",
        "                cache[line] = chunks\n",
        "\n",
        "            batch_chunks.append(chunks)\n",
        "\n",
        "        return batch_chunks, token_count\n",
        "\n",
        "    def forward(self, texts):\n",
        "        \"\"\"\n",
        "        Accepts an array of texts; Passes texts through transformers network to create a tensor with numerical representation of those texts.\n",
        "        Returns a tensor with shape of (B, T, C), where B is length of the array; T is length, in tokens, of texts (including padding) - T will\n",
        "        be a multiple of 77; and C is dimensionality of each token - for SD1 it's 768, and for SD2 it's 1024.\n",
        "        An example shape returned by this function can be: (2, 77, 768).\n",
        "        Webui usually sends just one text at a time through this function - the only time when texts is an array with more than one elemenet\n",
        "        is when you do prompt editing: \"a picture of a [cat:dog:0.4] eating ice cream\"\n",
        "        \"\"\"\n",
        "\n",
        "        # if opts.use_old_emphasis_implementation:\n",
        "        #     import modules.sd_hijack_clip_old\n",
        "        #     return modules.sd_hijack_clip_old.forward_old(self, texts)\n",
        "\n",
        "        batch_chunks, token_count = self.process_texts(texts)\n",
        "\n",
        "        used_embeddings = {}\n",
        "        chunk_count = max([len(x) for x in batch_chunks])\n",
        "\n",
        "        zs = []\n",
        "        for i in range(chunk_count):\n",
        "            batch_chunk = [chunks[i] if i < len(chunks) else self.empty_chunk() for chunks in batch_chunks]\n",
        "\n",
        "            tokens = [x.tokens for x in batch_chunk]\n",
        "            multipliers = [x.multipliers for x in batch_chunk]\n",
        "            self.hijack.fixes = [x.fixes for x in batch_chunk]\n",
        "\n",
        "            for fixes in self.hijack.fixes:\n",
        "                for position, embedding in fixes:\n",
        "                    used_embeddings[embedding.name] = embedding\n",
        "\n",
        "            z = self.process_tokens(tokens, multipliers)\n",
        "            zs.append(z)\n",
        "\n",
        "        if len(used_embeddings) > 0:\n",
        "            embeddings_list = \", \".join([f'{name} [{embedding.checksum()}]' for name, embedding in used_embeddings.items()])\n",
        "            self.hijack.comments.append(f\"Used embeddings: {embeddings_list}\")\n",
        "\n",
        "        return torch.hstack(zs)\n",
        "\n",
        "    def process_tokens(self, remade_batch_tokens, batch_multipliers):\n",
        "        \"\"\"\n",
        "        sends one single prompt chunk to be encoded by transformers neural network.\n",
        "        remade_batch_tokens is a batch of tokens - a list, where every element is a list of tokens; usually\n",
        "        there are exactly 77 tokens in the list. batch_multipliers is the same but for multipliers instead of tokens.\n",
        "        Multipliers are used to give more or less weight to the outputs of transformers network. Each multiplier\n",
        "        corresponds to one token.\n",
        "        \"\"\"\n",
        "        tokens = torch.asarray(remade_batch_tokens).to(root.device)\n",
        "\n",
        "        # this is for SD2: SD1 uses the same token for padding and end of text, while SD2 uses different ones.\n",
        "        if self.id_end != self.id_pad:\n",
        "            for batch_pos in range(len(remade_batch_tokens)):\n",
        "                index = remade_batch_tokens[batch_pos].index(self.id_end)\n",
        "                tokens[batch_pos, index+1:tokens.shape[1]] = self.id_pad\n",
        "\n",
        "        z = self.encode_with_transformers(tokens)\n",
        "\n",
        "        # restoring original mean is likely not correct, but it seems to work well to prevent artifacts that happen otherwise\n",
        "        batch_multipliers = torch.asarray(batch_multipliers).to(root.device)\n",
        "        original_mean = z.mean()\n",
        "        z = z * batch_multipliers.reshape(batch_multipliers.shape + (1,)).expand(z.shape)\n",
        "        new_mean = z.mean()\n",
        "        z = z * (original_mean / new_mean)\n",
        "\n",
        "        return z\n",
        "\n",
        "\n",
        "class FrozenCLIPEmbedderWithCustomWords(FrozenCLIPEmbedderWithCustomWordsBase):\n",
        "    def __init__(self, wrapped, hijack):\n",
        "        super().__init__(wrapped, hijack)\n",
        "        self.tokenizer = wrapped.tokenizer\n",
        "\n",
        "        vocab = self.tokenizer.get_vocab()\n",
        "\n",
        "        self.comma_token = vocab.get(',</w>', None)\n",
        "\n",
        "        self.token_mults = {}\n",
        "        tokens_with_parens = [(k, v) for k, v in vocab.items() if '(' in k or ')' in k or '[' in k or ']' in k]\n",
        "        for text, ident in tokens_with_parens:\n",
        "            mult = 1.0\n",
        "            for c in text:\n",
        "                if c == '[':\n",
        "                    mult /= 1.1\n",
        "                if c == ']':\n",
        "                    mult *= 1.1\n",
        "                if c == '(':\n",
        "                    mult *= 1.1\n",
        "                if c == ')':\n",
        "                    mult /= 1.1\n",
        "\n",
        "            if mult != 1.0:\n",
        "                self.token_mults[ident] = mult\n",
        "\n",
        "        self.id_start = self.wrapped.tokenizer.bos_token_id\n",
        "        self.id_end = self.wrapped.tokenizer.eos_token_id\n",
        "        self.id_pad = self.id_end\n",
        "\n",
        "    def tokenize(self, texts):\n",
        "        tokenized = self.wrapped.tokenizer(texts, truncation=False, add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "        return tokenized\n",
        "\n",
        "    def encode_with_transformers(self, tokens):\n",
        "        CLIP_stop_at_last_layers=1\n",
        "        outputs = self.wrapped.transformer(input_ids=tokens, output_hidden_states=CLIP_stop_at_last_layers)\n",
        "\n",
        "        z = outputs.last_hidden_state\n",
        "\n",
        "        return z\n",
        "\n",
        "    def encode_embedding_init_text(self, init_text, nvpt):\n",
        "        embedding_layer = self.wrapped.transformer.text_model.embeddings\n",
        "        ids = self.wrapped.tokenizer(init_text, max_length=nvpt, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"]\n",
        "        embedded = embedding_layer.token_embedding.wrapped(ids.to(embedding_layer.token_embedding.wrapped.weight.device)).squeeze(0)\n",
        "\n",
        "        return embedded\n",
        "\n",
        "#@title hijack_clip_old\n",
        "# from drive.MyDrive import sd_hijack_clip\n",
        "def process_text_old(self: FrozenCLIPEmbedderWithCustomWordsBase, texts):\n",
        "    id_start = self.id_start\n",
        "    id_end = self.id_end\n",
        "    maxlen = self.wrapped.max_length  # you get to stay at 77\n",
        "    used_custom_terms = []\n",
        "    remade_batch_tokens = []\n",
        "    hijack_comments = []\n",
        "    hijack_fixes = []\n",
        "    token_count = 0\n",
        "\n",
        "    cache = {}\n",
        "    batch_tokens = self.tokenize(texts)\n",
        "    batch_multipliers = []\n",
        "    for tokens in batch_tokens:\n",
        "        tuple_tokens = tuple(tokens)\n",
        "\n",
        "        if tuple_tokens in cache:\n",
        "            remade_tokens, fixes, multipliers = cache[tuple_tokens]\n",
        "        else:\n",
        "            fixes = []\n",
        "            remade_tokens = []\n",
        "            multipliers = []\n",
        "            mult = 1.0\n",
        "\n",
        "            i = 0\n",
        "            while i < len(tokens):\n",
        "                token = tokens[i]\n",
        "\n",
        "                embedding, embedding_length_in_tokens = self.hijack.embedding_db.find_embedding_at_position(tokens, i)\n",
        "\n",
        "                mult_change = self.token_mults.get(token)\n",
        "                if mult_change is not None:\n",
        "                    mult *= mult_change\n",
        "                    i += 1\n",
        "                elif embedding is None:\n",
        "                    remade_tokens.append(token)\n",
        "                    multipliers.append(mult)\n",
        "                    i += 1\n",
        "                else:\n",
        "                    emb_len = int(embedding.vec.shape[0])\n",
        "                    fixes.append((len(remade_tokens), embedding))\n",
        "                    remade_tokens += [0] * emb_len\n",
        "                    multipliers += [mult] * emb_len\n",
        "                    used_custom_terms.append((embedding.name, embedding.checksum()))\n",
        "                    i += embedding_length_in_tokens\n",
        "\n",
        "            if len(remade_tokens) > maxlen - 2:\n",
        "                vocab = {v: k for k, v in self.wrapped.tokenizer.get_vocab().items()}\n",
        "                ovf = remade_tokens[maxlen - 2:]\n",
        "                overflowing_words = [vocab.get(int(x), \"\") for x in ovf]\n",
        "                overflowing_text = self.wrapped.tokenizer.convert_tokens_to_string(''.join(overflowing_words))\n",
        "                hijack_comments.append(f\"Warning: too many input tokens; some ({len(overflowing_words)}) have been truncated:\\n{overflowing_text}\\n\")\n",
        "\n",
        "            token_count = len(remade_tokens)\n",
        "            remade_tokens = remade_tokens + [id_end] * (maxlen - 2 - len(remade_tokens))\n",
        "            remade_tokens = [id_start] + remade_tokens[0:maxlen - 2] + [id_end]\n",
        "            cache[tuple_tokens] = (remade_tokens, fixes, multipliers)\n",
        "\n",
        "        multipliers = multipliers + [1.0] * (maxlen - 2 - len(multipliers))\n",
        "        multipliers = [1.0] + multipliers[0:maxlen - 2] + [1.0]\n",
        "\n",
        "        remade_batch_tokens.append(remade_tokens)\n",
        "        hijack_fixes.append(fixes)\n",
        "        batch_multipliers.append(multipliers)\n",
        "    return batch_multipliers, remade_batch_tokens, used_custom_terms, hijack_comments, hijack_fixes, token_count\n",
        "\n",
        "\n",
        "def forward_old(self: FrozenCLIPEmbedderWithCustomWordsBase, texts):\n",
        "    batch_multipliers, remade_batch_tokens, used_custom_terms, hijack_comments, hijack_fixes, token_count = process_text_old(self, texts)\n",
        "\n",
        "    self.hijack.comments += hijack_comments\n",
        "\n",
        "    if len(used_custom_terms) > 0:\n",
        "        self.hijack.comments.append(\"Used embeddings: \" + \", \".join([f'{word} [{checksum}]' for word, checksum in used_custom_terms]))\n",
        "\n",
        "    self.hijack.fixes = hijack_fixes\n",
        "    return self.process_tokens(remade_batch_tokens, batch_multipliers)\n",
        "\n",
        "#@title hijack_open_clip\n",
        "\n",
        "import open_clip.tokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = open_clip.tokenizer._tokenizer\n",
        "\n",
        "\n",
        "class FrozenOpenCLIPEmbedderWithCustomWords(FrozenCLIPEmbedderWithCustomWordsBase):\n",
        "    def __init__(self, wrapped, hijack):\n",
        "        super().__init__(wrapped, hijack)\n",
        "\n",
        "        self.comma_token = [v for k, v in tokenizer.encoder.items() if k == ',</w>'][0]\n",
        "        self.id_start = tokenizer.encoder[\"<start_of_text>\"]\n",
        "        self.id_end = tokenizer.encoder[\"<end_of_text>\"]\n",
        "        self.id_pad = 0\n",
        "\n",
        "    def tokenize(self, texts):\n",
        "\n",
        "        tokenized = [tokenizer.encode(text) for text in texts]\n",
        "\n",
        "        return tokenized\n",
        "\n",
        "    def encode_with_transformers(self, tokens):\n",
        "        # set self.wrapped.layer_idx here according to opts.CLIP_stop_at_last_layers\n",
        "        z = self.wrapped.encode_with_transformer(tokens)\n",
        "\n",
        "        return z\n",
        "\n",
        "    def encode_embedding_init_text(self, init_text, nvpt):\n",
        "        ids = tokenizer.encode(init_text)\n",
        "        ids = torch.asarray([ids], device=\"cuda\", dtype=torch.int)\n",
        "        embedded = self.wrapped.model.token_embedding.wrapped(ids).squeeze(0)\n",
        "\n",
        "        return embedded\n",
        "\n",
        "import open_clip.tokenizer\n",
        "import torch\n",
        "\n",
        "class FrozenXLMREmbedderWithCustomWords(FrozenCLIPEmbedderWithCustomWords):\n",
        "    def __init__(self, wrapped, hijack):\n",
        "        super().__init__(wrapped, hijack)\n",
        "\n",
        "        self.id_start = wrapped.config.bos_token_id\n",
        "        self.id_end = wrapped.config.eos_token_id\n",
        "        self.id_pad = wrapped.config.pad_token_id\n",
        "\n",
        "        self.comma_token = self.tokenizer.get_vocab().get(',', None)  # alt diffusion doesn't have </w> bits for comma\n",
        "\n",
        "    def encode_with_transformers(self, tokens):\n",
        "        # there's no CLIP Skip here because all hidden layers have size of 1024 and the last one uses a\n",
        "        # trained layer to transform those 1024 into 768 for unet; so you can't choose which transformer\n",
        "        # layer to work with - you have to use the last\n",
        "\n",
        "        attention_mask = (tokens != self.id_pad).to(device=tokens.device, dtype=torch.int64)\n",
        "        features = self.wrapped(input_ids=tokens, attention_mask=attention_mask)\n",
        "        z = features['projection_state']\n",
        "\n",
        "        return z\n",
        "\n",
        "    def encode_embedding_init_text(self, init_text, nvpt):\n",
        "        embedding_layer = self.wrapped.roberta.embeddings\n",
        "        ids = self.wrapped.tokenizer(init_text, max_length=nvpt, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"]\n",
        "        embedded = embedding_layer.token_embedding.wrapped(ids.to(root.device)).squeeze(0)\n",
        "\n",
        "        return embedded\n",
        "\n",
        "def reload_hypernetworks():\n",
        "    \n",
        "    global hypernetworks\n",
        "\n",
        "    hypernetworks = list_hypernetworks(root.hypernetwork_dir)\n",
        "\n",
        "model_hijack = StableDiffusionModelHijack()\n",
        "embedding_db = EmbeddingDatabase()\n",
        "hypernetwork = Hypernetwork()\n",
        "model_hijack.hijack(model)\n",
        "model_hijack.embedding_db.load_textual_inversion_embeddings(force_reload=True)\n",
        "reload_hypernetworks()\n",
        "names = []\n",
        "names_list1 = list_hypernetworks(root.hypernetwork_dir)\n",
        "names_list = tuple(list_hypernetworks(root.hypernetwork_dir).keys())\n",
        "names.append(names_list)\n",
        "multipliers=3.0\n",
        "load_hypernetworks(names, multipliers)\n",
        "for name in names_list:\n",
        "    load_hypernetwork(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "E0tJVYA4WM_u"
      },
      "outputs": [],
      "source": [
        "#@title $\\color{blue} {\\large \\textsf{Animation Settings}}$\n",
        "def DeforumAnimArgs():\n",
        "\n",
        "    #@markdown ####**Animation:**\n",
        "    animation_mode = '3D' #@param ['None', '2D', '3D', 'Video Input', 'Interpolation'] {type:'string'}\n",
        "    max_frames = 6000 #@param {type:\"number\"}\n",
        "    border = 'wrap' #@param ['wrap', 'replicate'] {type:'string'}\n",
        "\n",
        "    #@markdown ####**Motion Parameters:**\n",
        "    angle = \"0:(0)\"#@param {type:\"string\"}\n",
        "    zoom = \"0:(1.02+0.02*sin(2*3.14*t/300))\"#@param {type:\"string\"}\n",
        "    translation_x = \"0:(3*sin(2*3.14*t/270))\"#@param {type:\"string\"}\n",
        "    translation_y = \"0:((3*sin(2*3.14*t/270)**20)-3)\"#@param {type:\"string\"}\n",
        "    translation_z = \"0: (2), 30: ((10*(sin(3.141*t/300)**900)+2) + (+0.1 *(sin(3.141*t/900)**50)+2))\"#@param {type:\"string\"}\n",
        "    rotation_3d_x = \"0:(0)\"#@param {type:\"string\"}\n",
        "    rotation_3d_y = \"0:(-0.2*sin(2*3.14*t/270))\"#@param {type:\"string\"}\n",
        "    rotation_3d_z = \"0: (0), 30: ((10*(sin(3.141*t/600)**5000)+0.5) + (+0.1 *(sin(3.141*t/600)**50)-0.5))\"#@param {type:\"string\"}\n",
        "    rotation_3d_w = \"0: (0.4)\"\n",
        "    flip_2d_perspective = False #@param {type:\"boolean\"}\n",
        "    perspective_flip_theta = \"0:(0)\"#@param {type:\"string\"}\n",
        "    perspective_flip_phi = \"0:(t%15)\"#@param {type:\"string\"}\n",
        "    perspective_flip_gamma = \"0:(0)\"#@param {type:\"string\"}\n",
        "    perspective_flip_fv = \"0:(53)\"#@param {type:\"string\"}\n",
        "    noise_schedule = \"0:(.02)\"#@param {type:\"string\"}\n",
        "    strength_schedule = \"0:(.60), 30: (-4*(cos(3.141*t/90)**1000)+0.60)\"#@param {type:\"string\"}\n",
        "    contrast_schedule = \"0: (1.0)\"#@param {type:\"string\"}\n",
        "    hybrid_comp_alpha_schedule = \"0:(1)\" #@param {type:\"string\"}\n",
        "    hybrid_comp_mask_blend_alpha_schedule = \"0:(0.5)\" #@param {type:\"string\"}\n",
        "    hybrid_comp_mask_contrast_schedule = \"0:(1)\" #@param {type:\"string\"}\n",
        "    hybrid_comp_mask_auto_contrast_cutoff_high_schedule =  \"0:(100)\" #@param {type:\"string\"}\n",
        "    hybrid_comp_mask_auto_contrast_cutoff_low_schedule =  \"0:(0)\" #@param {type:\"string\"}\n",
        "\n",
        "    #@markdown ####**Cadence Scheduling:**\n",
        "    enable_cadence_schedule = True #@param {type:\"boolean\"}\n",
        "    cadence_schedule = \"0:(4),30:(6), 60:(8), 90:(4)\" #@param{type:'string'}\n",
        "\n",
        "    #@markdown ####**Sampler Scheduling:**\n",
        "    enable_schedule_samplers = False #@param {type:\"boolean\"}\n",
        "    sampler_schedule = \"0:('euler'),10:('dpm2'),20:('dpm2_ancestral'),30:('heun'),40:('euler'),50:('euler_ancestral'),60:('dpm_fast'),70:('dpm_adaptive'),80:('dpmpp_2s_a'),90:('dpmpp_2m')\" #@param {type:\"string\"}\n",
        "\n",
        "    #@markdown ####**Unsharp mask (anti-blur) Parameters:**\n",
        "    kernel_schedule = \"0: (5)\"#@param {type:\"string\"}\n",
        "    sigma_schedule = \"0: (1.0)\"#@param {type:\"string\"}\n",
        "    amount_schedule = \"0: (0.2)\"#@param {type:\"string\"}\n",
        "    threshold_schedule = \"0: (0.0)\"#@param {type:\"string\"}\n",
        "\n",
        "    #@markdown ####**Coherence:**\n",
        "    color_coherence = 'Match Frame 0 RGB' #@param ['None', 'Match Frame 0 HSV', 'Match Frame 0 LAB', 'Match Frame 0 RGB', 'Video Input'] {type:'string'}\n",
        "    color_coherence_video_every_N_frames = 1 #@param {type:\"integer\"}\n",
        "    color_force_grayscale = False #@param {type:\"boolean\"}\n",
        "    diffusion_cadence = '4' #@param ['1','2','3','4','5','6','7','8'] {type:'string'}\n",
        "\n",
        "    #@markdown ####**3D Depth Warping:**\n",
        "    use_depth_warping = True #@param {type:\"boolean\"}\n",
        "    midas_weight = 0.3#@param {type:\"number\"}\n",
        "    near_plane = 200\n",
        "    far_plane = 10000\n",
        "    fov = 40#@param {type:\"number\"}\n",
        "    padding_mode = 'border'#@param ['border', 'reflection', 'zeros'] {type:'string'}\n",
        "    sampling_mode = 'bicubic'#@param ['bicubic', 'bilinear', 'nearest'] {type:'string'}\n",
        "    save_depth_maps = False #@param {type:\"boolean\"}\n",
        "\n",
        "    #@markdown ####**Video Input:**\n",
        "    video_init_path ='/content/video_in.mp4'#@param {type:\"string\"}\n",
        "    extract_nth_frame = 1#@param {type:\"number\"}\n",
        "    overwrite_extracted_frames = True #@param {type:\"boolean\"}\n",
        "    use_mask_video = False #@param {type:\"boolean\"}\n",
        "    video_mask_path ='/content/video_in.mp4'#@param {type:\"string\"}\n",
        "\n",
        "    #@markdown ####**Hybrid Video for 2D/3D Animation Mode:**\n",
        "    hybrid_generate_inputframes = False #@param {type:\"boolean\"}\n",
        "    hybrid_use_first_frame_as_init_image = True #@param {type:\"boolean\"}\n",
        "    hybrid_motion = \"None\" #@param ['None','Optical Flow','Perspective','Affine']\n",
        "    hybrid_motion_use_prev_img = False #@param {type:\"boolean\"}\n",
        "    hybrid_flow_method = \"DIS Medium\" #@param ['DenseRLOF','DIS Medium','Farneback','SF']\n",
        "    hybrid_composite = False #@param {type:\"boolean\"}\n",
        "    hybrid_comp_mask_type = \"None\" #@param ['None', 'Depth', 'Video Depth', 'Blend', 'Difference']\n",
        "    hybrid_comp_mask_inverse = False #@param {type:\"boolean\"}\n",
        "    hybrid_comp_mask_equalize = \"None\" #@param  ['None','Before','After','Both']\n",
        "    hybrid_comp_mask_auto_contrast = False #@param {type:\"boolean\"}\n",
        "    hybrid_comp_save_extra_frames = False #@param {type:\"boolean\"}\n",
        "    hybrid_use_video_as_mse_image = False #@param {type:\"boolean\"}\n",
        "\n",
        "    #@markdown ####**Interpolation:**\n",
        "    interpolate_key_frames = False #@param {type:\"boolean\"}\n",
        "    interpolate_x_frames = 4 #@param {type:\"number\"}\n",
        "    \n",
        "    #@markdown ####**Resume Animation:**\n",
        "    resume_from_timestring = False #@param {type:\"boolean\"}\n",
        "    resume_timestring = \"20230203050018\" #@param {type:\"string\"}\n",
        "\n",
        "    return locals()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IAxui2GrDYjA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "f05b573372de4927ab46fb894e6da8f5",
            "71f551f5685649c28cd3787229d45d6f",
            "738d26c3b1fd4cc6ada03d7b6da603ad",
            "81a8bb9a77c24c6b93d0023ad12b01af",
            "6767e12be257488092078b1e0354464a",
            "8ccdb764501843c795af6c878d257178",
            "52cc01bdae32400b9f56c737f4d4f2c9",
            "98ce06aeed3241c7b83745c09667b305",
            "21fd17da1e5a4f79962c87fb6d645443",
            "beb3bfcfdd904e01b8692714b4ddbdea",
            "10a529398263461e8f805245d2874e15",
            "67f00a986ca241fea9fc133d0e08e9a5",
            "96e3727d666f45739e93e4b34b6b4eba",
            "eb59dfdd5f3e45bb987c42e24dd2977e",
            "3c53751d212a4b958a5967dd93cbab89",
            "74a486bfc017495dbcc60171a74ac121",
            "cb68144c40164881a84b0d674ac533b6",
            "07668ff6ec5e45559c004d5cd5f99a1b"
          ]
        },
        "outputId": "fbdb1cc2-e60f-483d-eb40-257aa8b6ee6f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='danger', description='Psychedelic Artists', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f05b573372de4927ab46fb894e6da8f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='success', description='Surrealist Artists', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81a8bb9a77c24c6b93d0023ad12b01af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='info', description='Anime Artists', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52cc01bdae32400b9f56c737f4d4f2c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='success', description='High Definition Words', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "beb3bfcfdd904e01b8692714b4ddbdea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='info', description='Art Styles', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96e3727d666f45739e93e4b34b6b4eba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='warning', description='Remix', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74a486bfc017495dbcc60171a74ac121"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title $\\color{BLUE} {\\large \\textsf{Simple Prompt Modifier Generator}}$\n",
        "\n",
        "import random\n",
        "from ipywidgets import widgets, Label\n",
        "from IPython import display\n",
        "#artists\n",
        "psychedelic_artists = [\"Pablo Amaringo\", \"Chuck Arnett\", \"Chris Dyer\", \"Doug Binder\",  \"Brummbaer\", \"Mark Boyle\", \"Joan Hills\",]\n",
        "surrealist_artists = [\"Genevieve Leavold\", \"Sam Wilde\", \"Adam Lawrence\", \"Violet Polsangi\",  \"Kazuhiro Higashi\", \"Kim Marra\", \"Ricardo Harris-Fuentes\",]\n",
        "anime_artists = [\"George Morikaw\", \"Keisuke Itagaki\", \"Yoichi Takahash\", \"Hirohiko Araki\", \"Masashi Kishimoto\", \"Yoshihiro Togashi\", \"Hajime Isayama\", \"Gosho Aoyama\", \"Akira Toriyama\", \"Eiichiro Oda\",]\n",
        "\n",
        "#high definition\n",
        "high_definition = [\"4K\",\"High-resolution\",\"High-quality\",\"Sharp\",\"Detailed\",\"Vibrant\",\"Clear\",\"Realistic\",\"Immersive\",\"Stunning\",\"Crisp\",\"Bright\",\"Colorful\",\"Smooth\",\"Life-like\",\"Rich\",\"Lifelike\",\"Vivid\",\"Gorgeous\",\"Beautiful\",\"Striking\",\"Sharpness\",\"Depth\",\"Intense\",\"Brilliant\",\"Extraordinary\",\"Dynamic\",\"Elegant\",\"Expansive\",\"Fantastic\",\"Fine\",\"Outstanding\",\"Magnificent\",\"Mesmerizing\",\"Incredible\",\"Impressive\",\"Sensational\",\"Spectacular\",\"Superb\",\"Stupendous\",\"Tremendous\",\"Unbelievable\",\"Unreal\"]\n",
        "\n",
        "#art style\n",
        "art_style = [\"Renaissance\",\"Baroque\",\"Rococo\",\"Gothic\",\"Impressionism\",\"Post-Impressionism\",\"Expressionism\",\"Art Nouveau\",\"Futurism\",\"Cubism\",\"Surrealism\",\"Abstract Expressionism\",\"Pop Art\",\"Minimalism\",\"Conceptual Art\",\"Realism\",\"Romanticism\",\"Neoclassicism\",\"Pre-Raphaelitism\",\"Fauvism\",\"Orphism\",\"De Stijl\",\"Suprematism\",\"Constructivism\",\"Dadaism\",\"Expressionist Architecture\",\"Bauhaus\",\"Art Deco\",\"Futurist Architecture\",\"Concrete Art\",\"Arte Povera\",\"New Objectivity\",\"Op Art\",\"Hyperrealism\",\"Photorealism\",\"Magic Realism\",\"Social Realism\",\"Early Netherlandish Painting\",\"High Renaissance\",\"Mannerism\",\"Northern Renaissance\",\"Baroque Classicism\",\"Caravaggism\",\"Dutch Golden Age Painting\",\"Romanticism in Art\",\"American Regionalism\",\"Abstract Illusionism\",\"Abstract Impressionism\",\"Neo-Expressionism\",\"Hyperrealistic Sculpture\",\"Environmental Art\",\"New Media Art\",\"Contemporary Realism\",\"Neo-Pop Art\"]\n",
        "\n",
        "button1 = widgets.Button(description=\"Psychedelic Artists\", button_style=\"danger\")\n",
        "\n",
        "def on_button1_clicked(b):\n",
        "\n",
        "  selected_artists = random.sample(psychedelic_artists, 3)\n",
        "  print(\"Psychedelic Artists: \" + \", \".join(selected_artists))\n",
        "  # label = Label(value=\"Selected artists: \" + \", \".join(selected_artists))\n",
        "  # display(label)\n",
        "button1.on_click(on_button1_clicked)\n",
        "\n",
        "button2 = widgets.Button(description=\"Surrealist Artists\", button_style=\"success\")\n",
        "\n",
        "def on_button2_clicked(b):\n",
        "\n",
        "  selected_artists = random.sample(surrealist_artists, 3)\n",
        "  print(\"Surrealist Artists: \" + \", \".join(selected_artists))\n",
        "  # label = Label(value=\"Selected artists: \" + \", \".join(selected_artists))\n",
        "  # display(label)\n",
        "button2.on_click(on_button2_clicked)\n",
        "\n",
        "button3 = widgets.Button(description=\"Anime Artists\", button_style=\"info\")\n",
        "\n",
        "def on_button3_clicked(b):\n",
        "\n",
        "  selected_artists = random.sample(anime_artists, 3)\n",
        "  print(\"Anime Artists: \" + \", \".join(selected_artists))\n",
        "  # label = Label(value=\"Selected artists: \" + \", \".join(selected_artists))\n",
        "  # display(label)\n",
        "button3.on_click(on_button3_clicked)\n",
        "\n",
        "button_all = widgets.Button(description=\"Remix\", button_style=\"Warning\")\n",
        "\n",
        "def on_button_all_clicked(b):\n",
        "\n",
        "  first_artist = random.choice(psychedelic_artists)\n",
        "  second_artist = random.choice(surrealist_artists)\n",
        "  third_artist = random.choice(anime_artists)\n",
        "  art_styles = random.choice(art_style)\n",
        "  hd_word = random.choice(high_definition)\n",
        "  print(\"Remix of Prompt Words: {}, {}, {}, {}, {}\".format(first_artist, second_artist, third_artist, art_styles, hd_word))\n",
        "  # label = Label(value=\"Selected artists: \" + \", \".join(selected_artists))\n",
        "  # display(label)\n",
        "button_all.on_click(on_button_all_clicked)\n",
        "\n",
        "button4 = widgets.Button(description=\"High Definition Words\", button_style=\"success\")\n",
        "\n",
        "def on_button4_clicked(b):\n",
        "\n",
        "  selected_hd = random.sample(high_definition, 4)\n",
        "  print(\"High Definition Words: \" + \", \".join(selected_hd))\n",
        "  # label = Label(value=\"Selected artists: \" + \", \".join(selected_artists))\n",
        "  # display(label)\n",
        "button4.on_click(on_button4_clicked)\n",
        "\n",
        "button5 = widgets.Button(description=\"Art Styles\", button_style=\"info\")\n",
        "\n",
        "def on_button5_clicked(b):\n",
        "\n",
        "  selected_style = random.sample(art_style, 4)\n",
        "  print(\"Art Styles: \" + \", \".join(selected_style))\n",
        "  # label = Label(value=\"Selected artists: \" + \", \".join(selected_artists))\n",
        "  # display(label)\n",
        "button5.on_click(on_button5_clicked)\n",
        "\n",
        "display.display(button1, button2, button3, button4, button5, button_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grr4ZlvWS7p8"
      },
      "outputs": [],
      "source": [
        "# conditional (postitive) prompts\n",
        "cond_prompts = {\n",
        "    0: \"dreamlikeart portrait of (LuisapDarkage_pinguinstyle5Darkage:3) hulking herculean extra terrestrial aquatic grey alien horns, dark, gore, creepy, luisap\",\n",
        "    200: \"dreamlikeart portrait of (mjv4Hypernetwork_v1:3) melting psychedelic android robot, portrait, octane render, highly detailed\",\n",
        "    400: \"dreamlikeart portrait of (LuisapDarkage_pinguinstyle5Darkage:3) hulking herculean extra terrestrial aquatic grey alien horns muted colors, dark, gore, creepy, luisap\",\n",
        "    600: \"dreamlikeart portrait of (mjv4Hypernetwork_v1:3) melting psychedelic zombie, portrait, octane render, highly detailed\",\n",
        "    \n",
        "}\n",
        "\n",
        "# unconditional (negative) prompts\n",
        "uncond_prompts = {\n",
        "    0: \"realistic, bad drawing, incoherent, messy, low resolution, pixelated\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XVzhbmizWM_u"
      },
      "outputs": [],
      "source": [
        "#@title $\\color{BLUE} {\\large \\textsf{Load Settings}}$\n",
        "override_settings_with_file = False #@param {type:\"boolean\"}\n",
        "settings_file = \"custom\" #@param [\"custom\", \"512x512_aesthetic_0.json\",\"512x512_aesthetic_1.json\",\"512x512_colormatch_0.json\",\"512x512_colormatch_1.json\",\"512x512_colormatch_2.json\",\"512x512_colormatch_3.json\"]\n",
        "custom_settings_file = \"/content/drive/MyDrive/Settings.txt\"#@param {type:\"string\"}\n",
        "\n",
        "def DeforumArgs():\n",
        "    #@markdown **Image Settings**\n",
        "    W = 512 #@param\n",
        "    H = 512 #@param\n",
        "    W, H = map(lambda x: x - x % 64, (W, H))  # resize to integer multiple of 64\n",
        "    bit_depth_output = 8 #@param [8, 16, 32] {type:\"raw\"}\n",
        "\n",
        "    #@markdown **Sampling Settings**\n",
        "    seed = 724456508 #@param\n",
        "    sampler = 'dpm2_ancestral' #@param [\"klms\",\"dpm2\",\"dpm2_ancestral\",\"heun\",\"euler\",\"euler_ancestral\",\"plms\", \"ddim\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_a\", \"dpmpp_2m\"]\n",
        "    steps = 50 #@param\n",
        "    scale = 7 #@param\n",
        "    ddim_eta = 0.0 #@param\n",
        "    dynamic_threshold = None\n",
        "    static_threshold = None   \n",
        "\n",
        "    #@markdown **Save & Display Settings**\n",
        "    save_samples = True #@param {type:\"boolean\"}\n",
        "    save_settings = True #@param {type:\"boolean\"}\n",
        "    display_samples = True #@param {type:\"boolean\"}\n",
        "    save_sample_per_step = False #@param {type:\"boolean\"}\n",
        "    show_sample_per_step = False #@param {type:\"boolean\"}\n",
        "\n",
        "    #@markdown **Batch Settings**\n",
        "    n_batch = 1 #@param\n",
        "    n_samples = 1 #@param\n",
        "    batch_name = \"upscale\" #@param {type:\"string\"}\n",
        "    filename_format = \"{timestring}_{index}_{prompt}.png\" #@param [\"{timestring}_{index}_{seed}.png\",\"{timestring}_{index}_{prompt}.png\"]\n",
        "    seed_behavior = \"iter\" #@param [\"iter\",\"fixed\",\"random\",\"ladder\",\"alternate\"]\n",
        "    seed_iter_N = 1 #@param {type:'integer'}\n",
        "    make_grid = False #@param {type:\"boolean\"}\n",
        "    grid_rows = 2 #@param \n",
        "    outdir = get_output_folder(root.output_path, batch_name)\n",
        "\n",
        "    #@markdown **Init Settings**\n",
        "    use_init = False #@param {type:\"boolean\"}\n",
        "    strength = 0.65 #@param {type:\"number\"}\n",
        "    strength_0_no_init = True # Set the strength to 0 automatically when no init image is used\n",
        "    init_image = \"https://cdn.pixabay.com/photo/2022/07/30/13/10/green-longhorn-beetle-7353749_1280.jpg\" #@param {type:\"string\"}\n",
        "    add_init_noise = True #@param {type:\"boolean\"}\n",
        "    init_noise = 0.01 #@param\n",
        "    # Whiter areas of the mask are areas that change more\n",
        "    use_mask = False #@param {type:\"boolean\"}\n",
        "    use_alpha_as_mask = False # use the alpha channel of the init image as the mask\n",
        "    mask_file = \"https://www.filterforge.com/wiki/images/archive/b/b7/20080927223728%21Polygonal_gradient_thumb.jpg\" #@param {type:\"string\"}\n",
        "    invert_mask = False #@param {type:\"boolean\"}\n",
        "    # Adjust mask image, 1.0 is no adjustment. Should be positive numbers.\n",
        "    mask_brightness_adjust = 1.0  #@param {type:\"number\"}\n",
        "    mask_contrast_adjust = 1.0  #@param {type:\"number\"}\n",
        "    # Overlay the masked image at the end of the generation so it does not get degraded by encoding and decoding\n",
        "    overlay_mask = True  # {type:\"boolean\"}\n",
        "    # Blur edges of final overlay mask, if used. Minimum = 0 (no blur)\n",
        "    mask_overlay_blur = 5 # {type:\"number\"}\n",
        "\n",
        "    #@markdown **Exposure/Contrast Conditional Settings**\n",
        "    mean_scale = 0 #@param {type:\"number\"}\n",
        "    var_scale = 0 #@param {type:\"number\"}\n",
        "    exposure_scale = 0 #@param {type:\"number\"}\n",
        "    exposure_target = 0.5 #@param {type:\"number\"}\n",
        "\n",
        "    #@markdown **Color Match Conditional Settings**\n",
        "    colormatch_scale = 0 #@param {type:\"number\"}\n",
        "    colormatch_image = \"https://www.saasdesign.io/wp-content/uploads/2021/02/palette-3-min-980x588.png\" #@param {type:\"string\"}\n",
        "    colormatch_n_colors = 4 #@param {type:\"number\"}\n",
        "    ignore_sat_weight = 0 #@param {type:\"number\"}\n",
        "\n",
        "    #@markdown **CLIP\\Aesthetics Conditional Settings**\n",
        "    clip_name = 'ViT-L/14' #@param ['ViT-L/14', 'ViT-L/14@336px', 'ViT-B/16', 'ViT-B/32']\n",
        "    clip_scale = 0 #@param {type:\"number\"}\n",
        "    aesthetics_scale = 0 #@param {type:\"number\"}\n",
        "    cutn = 1 #@param {type:\"number\"}\n",
        "    cut_pow = 0.0001 #@param {type:\"number\"}\n",
        "\n",
        "    #@markdown **Other Conditional Settings**\n",
        "    init_mse_scale = 0 #@param {type:\"number\"}\n",
        "    init_mse_image = \"https://cdn.pixabay.com/photo/2022/07/30/13/10/green-longhorn-beetle-7353749_1280.jpg\" #@param {type:\"string\"}\n",
        "    blue_scale = 0 #@param {type:\"number\"}\n",
        "    \n",
        "    #@markdown **Conditional Gradient Settings**\n",
        "    gradient_wrt = 'x0_pred' #@param [\"x\", \"x0_pred\"]\n",
        "    gradient_add_to = 'both' #@param [\"cond\", \"uncond\", \"both\"]\n",
        "    decode_method = 'linear' #@param [\"autoencoder\",\"linear\"]\n",
        "    grad_threshold_type = 'dynamic' #@param [\"dynamic\", \"static\", \"mean\", \"schedule\"]\n",
        "    clamp_grad_threshold = 0.2 #@param {type:\"number\"}\n",
        "    clamp_start = 0.2 #@param\n",
        "    clamp_stop = 0.01 #@param\n",
        "    grad_inject_timing = list(range(1,10)) #@param\n",
        "\n",
        "    #@markdown **Speed vs VRAM Settings**\n",
        "    cond_uncond_sync = True #@param {type:\"boolean\"}\n",
        "    precision = 'autocast' \n",
        "    C = 4\n",
        "    f = 8\n",
        "\n",
        "    cond_prompt = \"\"\n",
        "    cond_prompts = \"\"\n",
        "    uncond_prompt = \"\"\n",
        "    uncond_prompts = \"\"\n",
        "    timestring = \"\"\n",
        "    init_latent = None\n",
        "    init_sample = None\n",
        "    init_sample_raw = None\n",
        "    mask_sample = None\n",
        "    init_c = None\n",
        "    seed_internal = 0\n",
        "\n",
        "    return locals()\n",
        "\n",
        "args_dict = DeforumArgs()\n",
        "anim_args_dict = DeforumAnimArgs()\n",
        "\n",
        "if override_settings_with_file:\n",
        "    load_args(args_dict, anim_args_dict, settings_file, custom_settings_file, verbose=False)\n",
        "\n",
        "args = SimpleNamespace(**args_dict)\n",
        "anim_args = SimpleNamespace(**anim_args_dict)\n",
        "\n",
        "args.timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "args.strength = max(0.0, min(1.0, args.strength))\n",
        "\n",
        "# Load clip model if using clip guidance\n",
        "if (args.clip_scale > 0) or (args.aesthetics_scale > 0):\n",
        "    root.clip_model = clip.load(args.clip_name, jit=False)[0].eval().requires_grad_(False).to(root.device)\n",
        "    if (args.aesthetics_scale > 0):\n",
        "        root.aesthetics_model = load_aesthetics_model(args, root)\n",
        "\n",
        "if args.seed == -1:\n",
        "    args.seed = random.randint(0, 2**32 - 1)\n",
        "if not args.use_init:\n",
        "    args.init_image = None\n",
        "if args.sampler == 'plms' and (args.use_init or anim_args.animation_mode != 'None'):\n",
        "    print(f\"Init images aren't supported with PLMS yet, switching to KLMS\")\n",
        "    args.sampler = 'klms'\n",
        "if args.sampler != 'ddim':\n",
        "    args.ddim_eta = 0\n",
        "\n",
        "if anim_args.animation_mode == 'None':\n",
        "    anim_args.max_frames = 1\n",
        "elif anim_args.animation_mode == 'Video Input':\n",
        "    args.use_init = True\n",
        "\n",
        "# clean up unused memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# dispatch to appropriate renderer\n",
        "if anim_args.animation_mode == '2D' or anim_args.animation_mode == '3D':\n",
        "    render_animation(root, anim_args, args, cond_prompts, uncond_prompts)\n",
        "elif anim_args.animation_mode == 'Video Input':\n",
        "    render_input_video(root, anim_args, args, cond_prompts, uncond_prompts)\n",
        "elif anim_args.animation_mode == 'Interpolation':\n",
        "    render_interpolation(root, anim_args, args, cond_prompts, uncond_prompts)\n",
        "else:\n",
        "    render_image_batch(root, args, cond_prompts, uncond_prompts)\n",
        "print(\"\\033[92m..Your Animation Has Completed, Congratulations..\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWNMwmr-JWuE"
      },
      "source": [
        " $\\color{blue} {\\large \\textsf{Create Video From Frames}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "P0j4xHmQJWuE"
      },
      "outputs": [],
      "source": [
        "skip_video_for_run_all = False #@param {type: 'boolean'}\n",
        "fps = 30 #@param {type:\"number\"}\n",
        "#@markdown **Manual Settings**\n",
        "use_manual_settings = False #@param {type:\"boolean\"}\n",
        "image_path = \"/content/drive/MyDrive/AI/StableDiffusion/2023-02/hypernet/20230101212135_%05d.png\" #@param {type:\"string\"}\n",
        "mp4_path = \"/content/drive/MyDrive/AI/StableDiffusion/2023-02/hypernet/20230101212135.mp4\" #@param {type:\"string\"}\n",
        "render_steps = False  #@param {type: 'boolean'}\n",
        "path_name_modifier = \"x0_pred\" #@param [\"x0_pred\",\"x\"]\n",
        "make_gif = False #@param{type:'boolean'}\n",
        "bitdepth_extension = \"exr\" if args.bit_depth_output == 32 else \"png\"\n",
        "\n",
        "if skip_video_for_run_all == True:\n",
        "    print('Skipping video creation, uncheck skip_video_for_run_all if you want to run it')\n",
        "else:\n",
        "    import os\n",
        "    import subprocess\n",
        "    from base64 import b64encode\n",
        "\n",
        "    print(f\"{image_path} -> {mp4_path}\")\n",
        "\n",
        "    if use_manual_settings:\n",
        "        max_frames = \"200\" #@param {type:\"string\"}\n",
        "    else:\n",
        "        if render_steps: # render steps from a single image\n",
        "            fname = f\"{path_name_modifier}_%05d.png\"\n",
        "            all_step_dirs = [os.path.join(args.outdir, d) for d in os.listdir(args.outdir) if os.path.isdir(os.path.join(args.outdir,d))]\n",
        "            newest_dir = max(all_step_dirs, key=os.path.getmtime)\n",
        "            image_path = os.path.join(newest_dir, fname)\n",
        "            print(f\"Reading images from {image_path}\")\n",
        "            mp4_path = os.path.join(newest_dir, f\"{args.timestring}_{path_name_modifier}.mp4\")\n",
        "            max_frames = str(args.steps)\n",
        "        else: # render images for a video\n",
        "            image_path = os.path.join(args.outdir, f\"{args.timestring}_%05d.{bitdepth_extension}\")\n",
        "            mp4_path = os.path.join(args.outdir, f\"{args.timestring}.mp4\")\n",
        "            max_frames = str(anim_args.max_frames)\n",
        "\n",
        "    # make video\n",
        "    cmd = [\n",
        "        'ffmpeg',\n",
        "        '-y',\n",
        "        '-vcodec', bitdepth_extension,\n",
        "        '-r', str(fps),\n",
        "        '-start_number', str(0),\n",
        "        '-i', image_path,\n",
        "        '-frames:v', max_frames,\n",
        "        '-c:v', 'libx264',\n",
        "        '-vf',\n",
        "        f'fps={fps}',\n",
        "        '-pix_fmt', 'yuv420p',\n",
        "        '-crf', '17',\n",
        "        '-preset', 'veryfast',\n",
        "        '-pattern_type', 'sequence',\n",
        "        mp4_path\n",
        "    ]\n",
        "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    stdout, stderr = process.communicate()\n",
        "    if process.returncode != 0:\n",
        "        print(stderr)\n",
        "        raise RuntimeError(stderr)\n",
        "\n",
        "    mp4 = open(mp4_path,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    display.display(display.HTML(f'<video controls loop><source src=\"{data_url}\" type=\"video/mp4\"></video>') )\n",
        "    \n",
        "    if make_gif:\n",
        "         gif_path = os.path.splitext(mp4_path)[0]+'.gif'\n",
        "         cmd_gif = [\n",
        "             'ffmpeg',\n",
        "             '-y',\n",
        "             '-i', mp4_path,\n",
        "             '-r', str(fps),\n",
        "             gif_path\n",
        "         ]\n",
        "         process_gif = subprocess.Popen(cmd_gif, stdout=subprocess.PIPE, stderr=subprocess.PIPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgN1AHkkT_R_"
      },
      "source": [
        " $\\color{blue} {\\large \\textsf{Create Video From Frames(Alternative Version)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "15mORbcZT8YI"
      },
      "outputs": [],
      "source": [
        "#@markdown **Alternative Version**\n",
        "from helpers.ffmpeg_helpers import make_mp4_ffmpeg\n",
        "skip_video_for_run_all = False #@param {type: 'boolean'}\n",
        "\n",
        "if skip_video_for_run_all == True:\n",
        "    print('Skipping video creation, uncheck skip_video_for_run_all if you want to run it')\n",
        "else:\n",
        "\n",
        "    from helpers.ffmpeg_helpers import get_extension_maxframes, get_auto_outdir_timestring, get_ffmpeg_path, make_mp4_ffmpeg, make_gif_ffmpeg, patrol_cycle\n",
        "\n",
        "    def ffmpegArgs():\n",
        "        ffmpeg_mode = \"auto\" #@param [\"auto\",\"manual\",\"timestring\"]\n",
        "        ffmpeg_outdir = \"/content/drive/MyDrive/\" #@param {type:\"string\"}\n",
        "        ffmpeg_timestring = \"20230203050018\" #@param {type:\"string\"}\n",
        "        ffmpeg_image_path = \"/content/drive/MyDrive/AI/StableDiffusion/2023-02/hypernet/20230203050018_%05d.png\" #@param {type:\"string\"}\n",
        "        ffmpeg_mp4_path = \"/content/drive/MyDrive/20230203050018.mp4\" #@param {type:\"string\"}\n",
        "        ffmpeg_gif_path = \"\" #@param {type:\"string\"}\n",
        "        ffmpeg_extension = \"png\" #@param {type:\"string\"}\n",
        "        ffmpeg_maxframes = 1120 #@param\n",
        "        ffmpeg_fps = 30 #@param\n",
        "\n",
        "        # determine auto paths\n",
        "        if ffmpeg_mode == 'auto':\n",
        "            ffmpeg_outdir, ffmpeg_timestring = get_auto_outdir_timestring(args,ffmpeg_mode)\n",
        "        if ffmpeg_mode in [\"auto\",\"timestring\"]:\n",
        "            ffmpeg_extension, ffmpeg_maxframes = get_extension_maxframes(args,ffmpeg_outdir,ffmpeg_timestring)\n",
        "            ffmpeg_image_path, ffmpeg_mp4_path, ffmpeg_gif_path = get_ffmpeg_path(ffmpeg_outdir, ffmpeg_timestring, ffmpeg_extension)\n",
        "        return locals()\n",
        "\n",
        "    ffmpeg_args_dict = ffmpegArgs()\n",
        "    ffmpeg_args = SimpleNamespace(**ffmpeg_args_dict)\n",
        "    make_mp4_ffmpeg(ffmpeg_args, display_ffmpeg=True, debug=True)\n",
        "    # make_gif_ffmpeg(ffmpeg_args, debug=True)\n",
        "    # patrol_cycle(args,ffmpeg_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Lnds1E7y_m61"
      },
      "outputs": [],
      "source": [
        "#@title $\\color{blue} {\\textsf{Display The Video Using kora(may error out, try uninstalling with \"pip uninstall kora\" if it doesn't work)!}}$\n",
        "\n",
        "from kora.drive import upload_public\n",
        "from IPython.display import HTML\n",
        "url = '/content/drive/MyDrive/20230202065924.mp4' #@param{type:'string'}\n",
        "url = upload_public(url)\n",
        "# mp4_path = \"/content/drive/MyDrive/AI/StableDiffusion/2023-01/kong/20230123004448.mp4\"\n",
        "# mp4 = open(mp4_path,'rb').read()\n",
        "# data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "# then display it\n",
        "HTML(f\"\"\"<video src={url} width=500 controls/>\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4Z-jtRAVJWuG"
      },
      "outputs": [],
      "source": [
        "skip_disconnect_for_run_all = True #@param {type: 'boolean'}\n",
        "\n",
        "if skip_disconnect_for_run_all == True:\n",
        "    print('Skipping disconnect, uncheck skip_disconnect_for_run_all if you want to run it')\n",
        "else:\n",
        "    from google.colab import runtime\n",
        "    runtime.unassign()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBqZ6ZUNL8xS"
      },
      "source": [
        "# $ \\color{cyan} {\\large \\textsf{Model Merger}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4nEedGxHeuXc"
      },
      "outputs": [],
      "source": [
        "#@title $ \\color{cyan} {\\large \\textsf{Merge Model!}}$\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "import tqdm\n",
        "from types import SimpleNamespace\n",
        "import safetensors.torch\n",
        "\n",
        "def Load_checkpoints_to_be_merged():\n",
        "  ckpt_dir = \"/content/drive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion\" #@param{type:'string'}\n",
        "  primary_model_name = \"/content/drive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/protogenNovaExperimental_protogenX80.ckpt\" #@param{type:'string'}\n",
        "  secondary_model_name = \"/content/drive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/DeliberateDream.ckpt\" #@param{type:'string'}\n",
        "  tertiary_model_name = \"/content/drive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/DreamShaper.ckpt\" #@param{type:'string'}\n",
        "  output_modelname = \"Deliberate_Nova_Dream\" #@param{type:'string'}\n",
        "  config_source = \"/content/drive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/Dreamlike-Gen1.0.yaml\" #@param{type:'string'}\n",
        "  map_location = \"cuda\" #@param['cuda', 'cpu']\n",
        "  interp_method = \"Weighted sum\" #@param[\"No interpolation\", \"Weighted sum\", \"Add difference\"]\n",
        "  multiplier = 0.6 #@param{type:'slider', min:0.1, max:1, step:0.1}\n",
        "  save_as_half = False #@param{type:'boolean'}\n",
        "  custom_name = \"cuda\" #@param['cuda', 'cpu']\n",
        "  checkpoint_format = \"ckpt\" #@param['ckpt', 'safetensor']\n",
        "  bake_in_vae = \"/content/drive/MyDrive/AI/models/vae-ft-mse-840000-ema-pruned.ckpt\" #@param{type:'string'}\n",
        "  discard_weights = True #@param{type:'boolean'}\n",
        "  return locals()\n",
        "\n",
        "load_checkpoints_to_be_merged = Load_checkpoints_to_be_merged()\n",
        "load_checkpoints_to_be_merged = SimpleNamespace(**load_checkpoints_to_be_merged)\n",
        "\n",
        "chckpoint_dict_replacements = {\n",
        "    'cond_stage_model.transformer.embeddings.': 'cond_stage_model.transformer.text_model.embeddings.',\n",
        "    'cond_stage_model.transformer.encoder.': 'cond_stage_model.transformer.text_model.encoder.',\n",
        "    'cond_stage_model.transformer.final_layer_norm.': 'cond_stage_model.transformer.text_model.final_layer_norm.',\n",
        "}\n",
        "\n",
        "def _load_vae_dict(model, vae_dict_1):\n",
        "    model.first_stage_model.load_state_dict(vae_dict_1)\n",
        "    model.first_stage_model.to(\"cuda\")\n",
        "\n",
        "def load_vae_dict(filename, map_location):\n",
        "    vae_ckpt = read_state_dict(filename, map_location=map_location)\n",
        "    vae_dict_1 = {k: v for k, v in vae_ckpt.items() if k[0:4] != \"loss\" and k not in vae_ignore_keys}\n",
        "    return vae_dict_1\n",
        "\n",
        "\n",
        "def load_vae(model, vae_file=None, vae_source=\"from unknown source\"):\n",
        "    global vae_dict, loaded_vae_file\n",
        "    # save_settings = False\n",
        "\n",
        "\n",
        "    if vae_file:\n",
        "      _load_vae_dict(model, vae_file)\n",
        "    else:\n",
        "      assert os.path.isfile(vae_file), f\"VAE {vae_source} doesn't exist: {vae_file}\"\n",
        "      print(f\"Loading VAE weights {vae_source}: {vae_file}\")\n",
        "            \n",
        "\n",
        "    vae_dict_1 = load_vae_dict(vae_file, load_checkpoints_to_be_merged.map_location)\n",
        "    _load_vae_dict(model, vae_dict_1)\n",
        "\n",
        "    vae_file = vae_dict_1.copy()\n",
        "\n",
        "    loaded_vae_file = vae_file\n",
        "\n",
        "\n",
        "\n",
        "def find_checkpoint_config(info):\n",
        "    config = os.path.splitext(load_checkpoints_to_be_merged.primary_model_name) + \".yaml\"\n",
        "    if os.path.exists(config):\n",
        "        return config\n",
        "\n",
        "    return config\n",
        "\n",
        "def transform_checkpoint_dict_key(k):\n",
        "    for text, replacement in chckpoint_dict_replacements.items():\n",
        "        if k.startswith(text):\n",
        "            k = replacement + k[len(text):]\n",
        "\n",
        "    return k\n",
        "\n",
        "def get_state_dict_from_checkpoint(pl_sd):\n",
        "    pl_sd = pl_sd.pop(\"state_dict\", pl_sd)\n",
        "    pl_sd.pop(\"state_dict\", None)\n",
        "\n",
        "    sd = {}\n",
        "    for k, v in pl_sd.items():\n",
        "        new_key = transform_checkpoint_dict_key(k)\n",
        "\n",
        "        if new_key is not None:\n",
        "            sd[new_key] = v\n",
        "\n",
        "    pl_sd.clear()\n",
        "    pl_sd.update(sd)\n",
        "\n",
        "    return pl_sd\n",
        "\n",
        "def read_state_dict(checkpoint_file, print_global_state=False, map_location=None):\n",
        "    _, extension = os.path.splitext(checkpoint_file)\n",
        "    if extension.lower() == \".safetensors\":\n",
        "        device = load_checkpoints_to_be_merged.map_location\n",
        "        if device is None:\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        pl_sd = safetensors.torch.load_file(checkpoint_file, device=device)\n",
        "    else:\n",
        "        pl_sd = torch.load(checkpoint_file, map_location=map_location)\n",
        "\n",
        "    if print_global_state and \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "\n",
        "    sd = get_state_dict_from_checkpoint(pl_sd)\n",
        "    return sd\n",
        "\n",
        "def create_config(ckpt_result, config_source, a, b, c):\n",
        "\n",
        "    cfg = load_checkpoints_to_be_merged.config_source\n",
        "\n",
        "    filename, _ = os.path.splitext(load_checkpoints_to_be_merged.output_modelname)\n",
        "    checkpoint_filename = os.path.dirname(load_checkpoints_to_be_merged.primary_model_name) + \"/\" + ckpt_result + \".yaml\"\n",
        "    print(\"Copying config:\")\n",
        "    print(\"   from:\", cfg)\n",
        "    print(\"     to:\", checkpoint_filename)\n",
        "    shutil.copyfile(cfg, checkpoint_filename)\n",
        "\n",
        "\n",
        "checkpoint_dict_skip_on_merge = [\"cond_stage_model.transformer.text_model.embeddings.position_ids\"]\n",
        "\n",
        "\n",
        "def to_half(tensor, enable):\n",
        "    if enable and tensor.dtype == torch.float:\n",
        "        return tensor.half()\n",
        "\n",
        "    return tensor\n",
        "\n",
        "vae_ignore_keys = {\"model_ema.decay\", \"model_ema.num_updates\"}\n",
        "vae_dict = {}\n",
        "def run_modelmerger(primary_model_name, secondary_model_name, tertiary_model_name, interp_method, multiplier, save_as_half, custom_name, checkpoint_format, config_source, bake_in_vae, discard_weights):\n",
        "\n",
        "    def fail(message):\n",
        "        textinfo = message\n",
        "        return message\n",
        "\n",
        "    def weighted_sum(theta0, theta1, alpha):\n",
        "        return ((1 - alpha) * theta0) + (alpha * theta1)\n",
        "\n",
        "    def get_difference(theta1, theta2):\n",
        "        return theta1 - theta2\n",
        "\n",
        "    def add_difference(theta0, theta1_2_diff, alpha):\n",
        "        return theta0 + (alpha * theta1_2_diff)\n",
        "\n",
        "    def filename_weighted_sum():\n",
        "        a = load_checkpoints_to_be_merged.primary_model_name\n",
        "        b = load_checkpoints_to_be_merged.secondary_model_name\n",
        "        Ma = round(1 - load_checkpoints_to_be_merged.multiplier, 2)\n",
        "        Mb = round(load_checkpoints_to_be_merged.multiplier, 2)\n",
        "\n",
        "        return f\"{Ma}({a}) + {Mb}({b})\"\n",
        "\n",
        "    def filename_add_difference():\n",
        "        a = load_checkpoints_to_be_merged.primary_model_name\n",
        "        b = load_checkpoints_to_be_merged.secondary_model_name\n",
        "        c = load_checkpoints_to_be_merged.tertiary_model_name\n",
        "        M = round(load_checkpoints_to_be_merged.multiplier, 2)\n",
        "\n",
        "        return f\"{a} + {M}({b} - {c})\"\n",
        "\n",
        "    def filename_nothing():\n",
        "        return load_checkpoints_to_be_merged.primary_model_name\n",
        "\n",
        "    theta_funcs = {\n",
        "        \"Weighted sum\": (filename_weighted_sum, None, weighted_sum),\n",
        "        \"Add difference\": (filename_add_difference, get_difference, add_difference),\n",
        "        \"No interpolation\": (filename_nothing, None, None),\n",
        "    }\n",
        "    filename_generator, theta_func1, theta_func2 = theta_funcs[load_checkpoints_to_be_merged.interp_method]\n",
        "    job_count = (1 if theta_func1 else 0) + (1 if theta_func2 else 0)\n",
        "\n",
        "    if not primary_model_name:\n",
        "        return fail(\"Failed: Merging requires a primary model.\")\n",
        "\n",
        "    primary_model_info = load_checkpoints_to_be_merged.primary_model_name\n",
        "\n",
        "    if theta_func2 and not secondary_model_name:\n",
        "        return fail(\"Failed: Merging requires a secondary model.\")\n",
        "\n",
        "    secondary_model_info = load_checkpoints_to_be_merged.secondary_model_name if theta_func2 else None\n",
        "\n",
        "    if theta_func1 and not tertiary_model_name:\n",
        "        return fail(f\"Failed: Interpolation method ({interp_method}) requires a tertiary model.\")\n",
        "\n",
        "    tertiary_model_info = load_checkpoints_to_be_merged.tertiary_model_name if theta_func1 else None\n",
        "\n",
        "    result_is_inpainting_model = False\n",
        "\n",
        "    if theta_func2:\n",
        "        textinfo = f\"Loading B\"\n",
        "        print(f\"Loading {secondary_model_info}...\")\n",
        "        theta_1 = read_state_dict(secondary_model_info, map_location='cpu')\n",
        "    else:\n",
        "        theta_1 = None\n",
        "\n",
        "    if theta_func1:\n",
        "        textinfo = f\"Loading C\"\n",
        "        print(f\"Loading {tertiary_model_info}...\")\n",
        "        theta_2 = read_state_dict(tertiary_model_info, map_location='cpu')\n",
        "\n",
        "        textinfo = 'Merging B and C'\n",
        "        sampling_steps = len(theta_1.keys())\n",
        "        for key in tqdm.tqdm(theta_1.keys()):\n",
        "            if key in checkpoint_dict_skip_on_merge:\n",
        "                continue\n",
        "\n",
        "            if 'model' in key:\n",
        "                if key in theta_2:\n",
        "                    t2 = theta_2.get(key, torch.zeros_like(theta_1[key]))\n",
        "                    theta_1[key] = theta_func1(theta_1[key], t2)\n",
        "                else:\n",
        "                    theta_1[key] = torch.zeros_like(theta_1[key])\n",
        "\n",
        "            sampling_steps += 1\n",
        "        del theta_2\n",
        "\n",
        "\n",
        "    textinfo = f\"Loading {primary_model_info}...\"\n",
        "    print(f\"Loading {primary_model_info}...\")\n",
        "    theta_0 = read_state_dict(primary_model_info, map_location='cpu')\n",
        "\n",
        "    print(\"Merging...\")\n",
        "    textinfo = 'Merging A and B'\n",
        "    sampling_steps = len(theta_0.keys())\n",
        "    for key in tqdm.tqdm(theta_0.keys()):\n",
        "        if theta_1 and 'model' in key and key in theta_1:\n",
        "\n",
        "            if key in checkpoint_dict_skip_on_merge:\n",
        "                continue\n",
        "\n",
        "            a = theta_0[key]\n",
        "            b = theta_1[key]\n",
        "\n",
        "            # this enables merging an inpainting model (A) with another one (B);\n",
        "            # where normal model would have 4 channels, for latenst space, inpainting model would\n",
        "            # have another 4 channels for unmasked picture's latent space, plus one channel for mask, for a total of 9\n",
        "            if a.shape != b.shape and a.shape[0:1] + a.shape[2:] == b.shape[0:1] + b.shape[2:]:\n",
        "                if a.shape[1] == 4 and b.shape[1] == 9:\n",
        "                    raise RuntimeError(\"When merging inpainting model with a normal one, A must be the inpainting model.\")\n",
        "\n",
        "                assert a.shape[1] == 9 and b.shape[1] == 4, f\"Bad dimensions for merged layer {key}: A={a.shape}, B={b.shape}\"\n",
        "\n",
        "                theta_0[key][:, 0:4, :, :] = theta_func2(a[:, 0:4, :, :], b, multiplier)\n",
        "                result_is_inpainting_model = True\n",
        "            else:\n",
        "                theta_0[key] = theta_func2(a, b, multiplier)\n",
        "\n",
        "            theta_0[key] = to_half(theta_0[key], save_as_half)\n",
        "\n",
        "        sampling_steps += 1\n",
        "\n",
        "    del theta_1\n",
        "\n",
        "    bake_in_vae_filename = load_checkpoints_to_be_merged.bake_in_vae\n",
        "    if bake_in_vae_filename is not None:\n",
        "        print(f\"Baking in VAE from {bake_in_vae_filename}\")\n",
        "        textinfo = 'Baking in VAE'\n",
        "        vae_dict = load_vae_dict(bake_in_vae_filename, map_location='cpu')\n",
        "\n",
        "        for key in vae_dict.keys():\n",
        "            theta_0_key = 'first_stage_model.' + key\n",
        "            if theta_0_key in theta_0:\n",
        "                theta_0[theta_0_key] = to_half(vae_dict[key], save_as_half)\n",
        "\n",
        "        del vae_dict\n",
        "\n",
        "    if save_as_half and not theta_func2:\n",
        "        for key in theta_0.keys():\n",
        "            theta_0[key] = to_half(theta_0[key], save_as_half)\n",
        "\n",
        "    # if discard_weights:\n",
        "    #     regex = re.compile(discard_weights)\n",
        "    #     for key in list(theta_0):\n",
        "    #         if re.search(regex, key):\n",
        "    #             theta_0.pop(key, None)\n",
        "\n",
        "    ckpt_dir = load_checkpoints_to_be_merged.ckpt_dir\n",
        "    filename = filename_generator() if custom_name == '' else custom_name\n",
        "    filename += \".inpainting\" if result_is_inpainting_model else \"\"\n",
        "    filename += \".\" + checkpoint_format\n",
        "\n",
        "    output_modelname = os.path.join(ckpt_dir, filename)\n",
        "\n",
        "    textinfo = \"Saving\"\n",
        "    print(f\"Saving to {output_modelname}...\")\n",
        "\n",
        "    _, extension = os.path.splitext(output_modelname)\n",
        "    if extension.lower() == \".safetensors\":\n",
        "        safetensors.torch.save_file(theta_0, output_modelname, metadata={\"format\": \"pt\"})\n",
        "    else:\n",
        "        torch.save(theta_0, output_modelname)\n",
        "\n",
        "\n",
        "    ckpt_cfg = create_config(load_checkpoints_to_be_merged.output_modelname,\n",
        "                  load_checkpoints_to_be_merged.config_source,\n",
        "                  primary_model_info,\n",
        "                  secondary_model_info,\n",
        "                  tertiary_model_info)\n",
        "\n",
        "    print(f\"Checkpoint saved to {output_modelname}.\")\n",
        "    textinfo = \"Checkpoint saved\"\n",
        "    \n",
        "\n",
        "    return load_checkpoints_to_be_merged.output_modelname, ckpt_cfg\n",
        "\n",
        "run_modelmerger(load_checkpoints_to_be_merged.primary_model_name,\n",
        "                load_checkpoints_to_be_merged.secondary_model_name, \n",
        "                load_checkpoints_to_be_merged.tertiary_model_name, \n",
        "                load_checkpoints_to_be_merged.interp_method, \n",
        "                load_checkpoints_to_be_merged.multiplier, \n",
        "                load_checkpoints_to_be_merged.save_as_half, \n",
        "                load_checkpoints_to_be_merged.output_modelname, \n",
        "                load_checkpoints_to_be_merged.checkpoint_format, \n",
        "                load_checkpoints_to_be_merged.config_source, \n",
        "                load_checkpoints_to_be_merged.bake_in_vae, \n",
        "                load_checkpoints_to_be_merged.discard_weights)\n",
        "\n",
        "print(\"\\033[92m..Model Merging Has Completed, Congratulations..\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGAjrH5REUBm"
      },
      "source": [
        "# $\\color{salmon} {\\large \\textsf{REAL-ESRGAN Video Upscaling!}}$\n",
        "\n",
        "Please Be Sure to Run The Enviornment Setup Cell!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BuNwcv9xuCDW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "#@title $\\color{salmon} {\\large \\textsf { ð¹ RUN REAL-ESRGAN VIDEO UPSCALE PHASE (from video) ð¹}}$ \n",
        "model_name = 'realesr-general-x4v3' #@param ['realesr-general-x4v3', 'realesr-general-wdn-x4v3', 'RealESRGAN_x4plus','RealESRGAN_x4plus_anime_6B','RealESRGAN_x2plus'] {type:\"string\"}\n",
        "lowres_vid = '/content/drive/MyDrive/LINUM/lowres/lowres_input_video/cell.mp4' #@param{type:'string'}\n",
        "#@markdown $\\color{salmon} {\\textsf {input your lowres video path here to be upscaled}}$\n",
        "highres_vid_dir = '/content/drive/MyDrive/upscaled_storage' #@param{type:'string'}\n",
        "#@markdown $\\color{salmon} {\\textsf {input your high res video output path here}}$\n",
        "if not os.path.exists(highres_vid_dir):\n",
        "  os.makedirs(highres_vid_dir)\n",
        "outscale = \"4\" #@param ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13']\n",
        "denoising_strength = 1 #@param{type:'slider', step:0.01, min:0.01, max:1}\n",
        "target_fps = 30 #@param{type:'number'}\n",
        "fp32 = 'fp32'\n",
        "gpu_id = 0 #@param ['0', '1', '2', '3', '4']\n",
        "\n",
        "%cd '/content/Real-ESRGAN/'\n",
        "!python3 /content/Real-ESRGAN/inference_realesrgan_video.py --model_name $model_name --outscale $outscale -dn $denoising_strength --fp32 --input $lowres_vid --fps $target_fps --output $highres_vid_dir\n",
        "\n",
        "%cd '/content/'\n",
        "print(\"\\033[92m...Upscaling Phase Has Completed, You May Proceed...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EGL93Ofs0GTT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "#@title $\\color{salmon} {\\large \\textsf { ð¹ !BATCH! RUN REAL-ESRGAN VIDEO UPSCALE PHASE (from video) ð¹}}$ \n",
        "model_name = 'realesr-general-x4v3' #@param ['realesr-general-x4v3', 'realesr-general-wdn-x4v3', 'RealESRGAN_x4plus','RealESRGAN_x4plus_anime_6B','RealESRGAN_x2plus'] {type:\"string\"}\n",
        "batch_viddir = \"/content/drive/MyDrive/LINUM/lowres/lowres_input_video/\" #@param{type:'string'}\n",
        "#@markdown $\\color{salmon} {\\textsf {input folder path with videos to upscale **WILL UPSCALE ALL VIDEOS IN THE FOLDER**}}$\n",
        "highres_vid_dir = '/content/drive/MyDrive/LINUM/4k' #@param{type:'string'}\n",
        "#@markdown $\\color{salmon} {\\textsf {input your high res video output path here}}$\n",
        "if not os.path.exists(highres_vid_dir):\n",
        "  os.makedirs(highres_vid_dir)\n",
        "outscale = \"4\" #@param ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13']\n",
        "denoising_strength = 1 #@param{type:'slider', step:0.01, min:0.01, max:1}\n",
        "target_fps = 30 #@param{type:'number'}\n",
        "fp32 = 'fp32'\n",
        "gpu_id = 0 #@param ['0', '1', '2', '3', '4']\n",
        "updir = os.path.split(highres_vid_dir)[0] + \"/\"\n",
        "\n",
        "for vid in os.listdir(batch_viddir):\n",
        "  new_vid = os.path.join(batch_viddir, vid)\n",
        "  print(new_vid)\n",
        "  %cd '/content/Real-ESRGAN'\n",
        "  !python3 /content/Real-ESRGAN/inference_realesrgan_video.py --model_name $model_name --outscale $outscale -dn $denoising_strength --fp32 --input $new_vid --fps $target_fps --output $highres_vid_dir\n",
        "  !mv $new_vid $updir --verbose\n",
        "  print(f\"\\033[93mMoving {new_vid} to {updir}.\")\n",
        "%cd '/content/'\n",
        "print(\"\\033[92m...Upscaling Phase Has Completed, You May Proceed...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjk7GkzMEfBU"
      },
      "source": [
        "# $\\color{orange} {\\large \\textsf{RIFE 4.6 Interpolation!}}$\n",
        "\n",
        "Please Be Sure to Run The Enviornment Setup Cell!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "K9cFuhyW91kH"
      },
      "outputs": [],
      "source": [
        "#@title $\\color{orange} {\\large \\textsf{Define Params and Run RIFE!}}$\n",
        "\n",
        "input_RIFE = '/content/drive/MyDrive/upscaled_storage/cell_out.mp4' # @param {type: 'string'}\n",
        "#@markdown $\\color{orange} {\\textsf{THIS IS THE FILE NAME FOR YOUR COMPRESSED VID}}$\n",
        "\n",
        "output_RIFE = '/content/drive/MyDrive/LINUM/4k/cell_out_RIFE.mp4' # @param {type: 'string'}\n",
        "#@markdown $\\color{orange} {\\textsf{INPUT YOUR OUTPUT NAME FOR THE INTERPOLATED VIDEO}}$\n",
        "fps_RIFE = 60 # @param {type: 'number'}\n",
        "target_length_RIFE = 35 # @param {type: 'number'}\n",
        "target_scale_RIFE = 0.5 #@param ['0.25', '0.5', '1.0', '2.0', '4.0']\n",
        "_skip_RIFEsmoothing = False\n",
        "#@markdown $\\color{orange} {\\textsf{Additional Params}}$\n",
        "print(\"\\n ------------------------------------\\n\")\n",
        "print(\"\\n Beginning RIFE motion smoothing phase... \\n\")\n",
        "\n",
        "%cd /content/Practical-RIFE/\n",
        "\n",
        "class Detection:\n",
        "  #def __init__(self):\n",
        "  #  pass\n",
        "  def detect_fps(input): #needs portable\n",
        "    import re\n",
        "    fps_ffprobe = !ffprobe -v error -select_streams v -of default=noprint_wrappers=1:nokey=1 -show_entries stream=avg_frame_rate $input\n",
        "    fps_unfinished = [str(i) for i in fps_ffprobe] # Converting integers into strings\n",
        "    fps_unfinishedTwo = str(\"\".join(fps_unfinished)) # Join the string values into one string\n",
        "    numbers = re.findall('[0-9]+', fps_unfinishedTwo)\n",
        "    newNum = numbers[0:1]\n",
        "    strings = [str(integer) for integer in newNum]\n",
        "    a_string = \"\".join(strings)\n",
        "    fps = int(a_string)\n",
        "    #print(\"Detected FPS is\",fps)\n",
        "    return fps\n",
        "  def detect_duration(input):  #needs portable\n",
        "    import re\n",
        "    duration_ffprobe = !ffprobe -v error -select_streams v:0 -show_entries stream=duration -of default=noprint_wrappers=1:nokey=1 $input\n",
        "    duration_unfinished = [str(i) for i in duration_ffprobe] # Converting integers into strings\n",
        "    duration_unfinishedTwo = str(\"\".join(duration_unfinished)) # Join the string values into one string\n",
        "    numbers = re.findall('[0-9]+', duration_unfinishedTwo)\n",
        "    newNum = numbers[0:1]\n",
        "    strings = [str(integer) for integer in newNum]\n",
        "    a_string = \"\".join(strings)\n",
        "    duration = float(int(a_string))\n",
        "    #print(\"Detected duration INTEGER (in seconds) is\",duration)\n",
        "    return duration\n",
        "  def exp_calc(measured_duration,target_length_RIFE): #needs portable\n",
        "    import numpy as np\n",
        "    a = measured_fps * measured_duration\n",
        "    b = fps_RIFE * target_length_RIFE\n",
        "    c = b / a\n",
        "    l = np.log(c) / np.log(2)\n",
        "    print(\"Un-rounded --exp is\",l)\n",
        "    x = round(l)\n",
        "    if x < 1:\n",
        "      x = 1\n",
        "    print(\"Rounding up to an --exp of \",x)\n",
        "    return x\n",
        "\n",
        "#----------------------------\n",
        "_import_mp4_file = False\n",
        "\n",
        "if _import_mp4_file:\n",
        "  measured_fps = Detection.detect_fps(input_RIFE)\n",
        "  print(\"\\n NOTICE: Detected average FPS of \",input_RIFE,\" is \",measured_fps)\n",
        "  measured_duration = Detection.detect_duration(input_RIFE)\n",
        "else: \n",
        "  measured_fps = Detection.detect_fps(input_RIFE)\n",
        "  print(\"\\n NOTICE: Detected average FPS of \",{input_RIFE},\" is \",measured_fps)\n",
        "  measured_duration = Detection.detect_duration(input_RIFE)\n",
        "\n",
        "print(\"\\n NOTICE: Detected duration INTEGER (in seconds) is \",measured_duration)\n",
        "\n",
        "if measured_duration < 1: #failsafe\n",
        "  print(\"\\n NOTICE: Your input appears to be less than one second... \\n\")\n",
        "  measured_duration = 1\n",
        "\n",
        "exp_value = Detection.exp_calc(measured_duration,target_length_RIFE)\n",
        "\n",
        "\n",
        "print(\"\\n NOTICE: Target duration currently rounds to the closest --exp RIFE can handle. \\n\")\n",
        "\n",
        "if (exp_value < 0.5):\n",
        "  _skip_RIFEsmoothing = False\n",
        "  print(\"\\n NOTICE: Your fps_RIFE doesn't necessitate RIFE motion smoothing. Skipping RIFE...\\n\")\n",
        "\n",
        "if _skip_RIFEsmoothing:\n",
        "  print(\"\\n NOTICE: Skipping RIFE motion smoothing...\\n\")\n",
        "else:\n",
        "  #---RUN RIFE------------------------------------\n",
        "  print(\"\\n NOTICE: Running RIFE... \\n\")\n",
        "  %cd /content/Practical-RIFE/\n",
        "  !python3 /content/Practical-RIFE/inference_video.py --fps=$fps_RIFE --exp=$exp_value  --video=$input_RIFE --scale=$target_scale_RIFE --output=$output_RIFE\n",
        "  #--exp=$exp_value\n",
        "  #---END--------------------------------------\n",
        "#!ffmpeg -y -i $input $visual_effects -c:v hevc_nvenc -rc vbr -cq $_constant_quality -qmin $_constant_quality -qmax $_constant_quality -b:v 0 $output_fullpathname\n",
        "#END OF MOTION SMOOTHING PHASE\n",
        "print(\"\\n \\033[92mEnd of RIFE interpolation phase.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8o5cSiIn6F0t"
      },
      "outputs": [],
      "source": [
        "#@title $\\color{orange} {\\large \\textsf{!BATCH! Define Params and Run RIFE!}}$\n",
        "rife_viddir = \"/content/drive/MyDrive/RIFE_Batch\" #@param{type:'string'}\n",
        "#@markdown $\\color{orange} {\\textsf{THIS IS THE PATH TO THE FOLDER WITH YOUR VIDEOS TO BE INTERPOLATED}}$\n",
        "for rife_vid in os.listdir(rife_viddir):\n",
        "    new_vid = os.path.join(rife_viddir, rife_vid)\n",
        "    print(new_vid)\n",
        "fps_RIFE = 60 # @param {type: 'number'}\n",
        "target_length_RIFE = 35 # @param {type: 'number'}\n",
        "target_scale_RIFE = 0.5 #@param ['0.25', '0.5', '1.0', '2.0', '4.0']\n",
        "_skip_RIFEsmoothing = False\n",
        "#@markdown $\\color{orange} {\\textsf{Additional Params}}$\n",
        "print(\"\\n ------------------------------------\\n\")\n",
        "print(\"\\n Beginning RIFE motion smoothing phase... \\n\")\n",
        "\n",
        "\n",
        "%cd /content/Practical-RIFE/\n",
        "\n",
        "class Detection:\n",
        "  #def __init__(self):\n",
        "  #  pass\n",
        "  def detect_fps(input): #needs portable\n",
        "    import re\n",
        "    fps_ffprobe = !ffprobe -v error -select_streams v -of default=noprint_wrappers=1:nokey=1 -show_entries stream=avg_frame_rate $input\n",
        "    fps_unfinished = [str(i) for i in fps_ffprobe] # Converting integers into strings\n",
        "    fps_unfinishedTwo = str(\"\".join(fps_unfinished)) # Join the string values into one string\n",
        "    numbers = re.findall('[0-9]+', fps_unfinishedTwo)\n",
        "    newNum = numbers[0:1]\n",
        "    strings = [str(integer) for integer in newNum]\n",
        "    a_string = \"\".join(strings)\n",
        "    fps = int(a_string)\n",
        "    #print(\"Detected FPS is\",fps)\n",
        "    return fps\n",
        "  def detect_duration(input):  #needs portable\n",
        "    import re\n",
        "    duration_ffprobe = !ffprobe -v error -select_streams v:0 -show_entries stream=duration -of default=noprint_wrappers=1:nokey=1 $input\n",
        "    duration_unfinished = [str(i) for i in duration_ffprobe] # Converting integers into strings\n",
        "    duration_unfinishedTwo = str(\"\".join(duration_unfinished)) # Join the string values into one string\n",
        "    numbers = re.findall('[0-9]+', duration_unfinishedTwo)\n",
        "    newNum = numbers[0:1]\n",
        "    strings = [str(integer) for integer in newNum]\n",
        "    a_string = \"\".join(strings)\n",
        "    duration = float(int(a_string))\n",
        "    #print(\"Detected duration INTEGER (in seconds) is\",duration)\n",
        "    return duration\n",
        "  def exp_calc(measured_duration,target_length_RIFE): #needs portable\n",
        "    import numpy as np\n",
        "    a = measured_fps * measured_duration\n",
        "    b = fps_RIFE * target_length_RIFE\n",
        "    c = b / a\n",
        "    l = np.log(c) / np.log(2)\n",
        "    print(\"Un-rounded --exp is\",l)\n",
        "    x = round(l)\n",
        "    if x < 1:\n",
        "      x = 1\n",
        "    print(\"Rounding up to an --exp of \",x)\n",
        "    return x\n",
        "\n",
        "#----------------------------\n",
        "_import_mp4_file = False\n",
        "\n",
        "if _import_mp4_file:\n",
        "  measured_fps = Detection.detect_fps(new_vid)\n",
        "  print(\"\\n NOTICE: Detected average FPS of \",new_vid,\" is \",measured_fps)\n",
        "  measured_duration = Detection.detect_duration(new_vid)\n",
        "else: \n",
        "  measured_fps = Detection.detect_fps(new_vid)\n",
        "  print(\"\\n NOTICE: Detected average FPS of \",new_vid,\" is \",measured_fps)\n",
        "  measured_duration = Detection.detect_duration(new_vid)\n",
        "\n",
        "print(\"\\n NOTICE: Detected duration INTEGER (in seconds) is \",measured_duration)\n",
        "\n",
        "if measured_duration < 1: #failsafe\n",
        "  print(\"\\n NOTICE: Your input appears to be less than one second... \\n\")\n",
        "  measured_duration = 1\n",
        "\n",
        "exp_value = Detection.exp_calc(measured_duration,target_length_RIFE)\n",
        "\n",
        "\n",
        "print(\"\\n NOTICE: Target duration currently rounds to the closest --exp RIFE can handle. \\n\")\n",
        "\n",
        "if (exp_value < 0.5):\n",
        "  _skip_RIFEsmoothing = False\n",
        "  print(\"\\n NOTICE: Your fps_RIFE doesn't necessitate RIFE motion smoothing. Skipping RIFE...\\n\")\n",
        "\n",
        "if _skip_RIFEsmoothing:\n",
        "  print(\"\\n NOTICE: Skipping RIFE motion smoothing...\\n\")\n",
        "else:\n",
        "  #---RUN RIFE------------------------------------\n",
        "  print(\"\\n ------------------------------------\\n\")\n",
        "  print(\"\\n NOTICE: Running RIFE... \\n\")\n",
        "  for rife_vid in os.listdir(rife_viddir):\n",
        "    new_vid = os.path.join(rife_viddir, rife_vid)\n",
        "    output_RIFE = f\"{os.path.splitext(new_vid)[0]}_RIFE{os.path.splitext(new_vid)[1]}\"\n",
        "    print(f\"Saving Video to: \", output_RIFE)\n",
        "    %cd /content/Practical-RIFE/\n",
        "    !python3 /content/Practical-RIFE/inference_video.py --fps=$fps_RIFE --exp=$exp_value  --video=$new_vid --scale=$target_scale_RIFE --output=$output_RIFE\n",
        "  \n",
        "  #---END--------------------------------------\n",
        "#!ffmpeg -y -i $input $visual_effects -c:v hevc_nvenc -rc vbr -cq $_constant_quality -qmin $_constant_quality -qmax $_constant_quality -b:v 0 $output_fullpathname\n",
        "#END OF MOTION SMOOTHING PHASE\n",
        "print(\"\\n \\033[92mEnd of RIFE interpolation phase.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Wb8AOszeOzxY"
      },
      "outputs": [],
      "source": [
        "#@title $\\color{orange} {\\large \\textsf {Analyze Lowres Vid Dimensions and FPS}}$ \n",
        "vcap = cv2.VideoCapture(output_RIFE)\n",
        "\n",
        "width = vcap.get(cv2.CAP_PROP_FRAME_WIDTH )\n",
        "height = vcap.get(cv2.CAP_PROP_FRAME_HEIGHT )\n",
        "fps =  vcap.get(cv2.CAP_PROP_FPS)\n",
        "print(f\"Width is: \\033[92m{width}.\")\n",
        "print(f\"Height is: \\033[92m{height}.\")\n",
        "print(f\"FPS is: \\033[92m{fps}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPN34lcSl1-d"
      },
      "source": [
        "# $\\color{blue} {\\large \\textsf{REAL-ESRGAN Single Image or Frame Folder Upscaling!}}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ROf39jOAblpZ"
      },
      "outputs": [],
      "source": [
        "#@title $\\color{blue} {\\large \\textsf { ð RUN REAL-ESRGAN UPSCALE PHASE (from frames) ð}}$\n",
        "#@markdown $\\color{blue} {\\large \\textsf {Input_dir is either a single image or a folder of images}}$\n",
        "model_name = 'realesr-general-x4v3' #@param ['realesr-general-x4v3', 'realesr-general-wdn-x4v3', 'RealESRGAN_x4plus','RealESRGAN_x4plus_anime_6B','RealESRGAN_x2plus'] {type:\"string\"}\n",
        "input_dir = '/content/drive/MyDrive/AI/StableDiffusion/2023-02/upscale/20230205235356_00002_dreamlikeart_portrait_of_LuisapDarkagepinguinstyleDarkage_hulking_herculean_extra_terrestrial_aquatic_grey_alien_horns_muted_colors_dark_gore_creepy_luisap.png' #@param{type:'string'}\n",
        "output_dir = '/content/drive/MyDrive/' #@param{type:'string'}\n",
        "outscale = \"12\" #@param ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13']\n",
        "denoising_strength = 1 #@param{type:'slider', step:0.01, min:0.01, max:1}\n",
        "fp32 = 'fp32'\n",
        "gpu_id = 0 #@param ['0', '1', '2', '3', '4']\n",
        "%cd '/content/'\n",
        "%cd '/content/Real-ESRGAN/'\n",
        "!python3 /content/Real-ESRGAN/inference_realesrgan.py --model_name $model_name --outscale $outscale -dn $denoising_strength --fp32 --gpu-id $gpu_id --input $input_dir --output $output_dir\n",
        "%cd '/content/'\n",
        "print(\"\\033[92m...Upscaling Phase Has Completed, You May Proceed...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZKH5VXEbICr",
        "outputId": "261e2bcf-550d-45fb-ce0a-dc2ec14ea477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Width: \u001b[92m6144, \u001b[0mImage Height: \u001b[92m12288\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "display_the_image = False #@param{type:'boolean'}\n",
        "display_image = \"/content/drive/MyDrive/20230205235356_00002_dreamlikeart_portrait_of_LuisapDarkagepinguinstyleDarkage_hulking_herculean_extra_terrestrial_aquatic_grey_alien_horns_muted_colors_dark_gore_creepy_luisap_out.png\" #@param{type:'string'}\n",
        "#@markdown Path to the Image you want to Display\n",
        "if display_the_image:\n",
        "    im = Image.open(display_image)\n",
        "    display.display(im)\n",
        "\n",
        "def analyze_image_dimensions(file_path):\n",
        "    with Image.open(file_path) as img:\n",
        "        width, height = img.size\n",
        "        print(f\"Image Width: \\033[92m{width}, \\033[0mImage Height: \\033[92m{height}\")\n",
        "\n",
        "analyze_image_dimensions(display_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy_mQSex2J7s"
      },
      "source": [
        "# $\\color{red} {\\large \\textsf{Disconnect Runtime}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IjlJkBoT8WCZ"
      },
      "outputs": [],
      "source": [
        "#@title $\\color{red} {\\large \\textsf{Disconnect Runtime}}$\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "MBqZ6ZUNL8xS",
        "qGAjrH5REUBm",
        "Gjk7GkzMEfBU",
        "KPN34lcSl1-d",
        "zy_mQSex2J7s"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNOP7Xvpc7hCbgo3AQW8UeK",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f05b573372de4927ab46fb894e6da8f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "danger",
            "description": "Psychedelic Artists",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_71f551f5685649c28cd3787229d45d6f",
            "style": "IPY_MODEL_738d26c3b1fd4cc6ada03d7b6da603ad",
            "tooltip": ""
          }
        },
        "71f551f5685649c28cd3787229d45d6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "738d26c3b1fd4cc6ada03d7b6da603ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "81a8bb9a77c24c6b93d0023ad12b01af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "Surrealist Artists",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_6767e12be257488092078b1e0354464a",
            "style": "IPY_MODEL_8ccdb764501843c795af6c878d257178",
            "tooltip": ""
          }
        },
        "6767e12be257488092078b1e0354464a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ccdb764501843c795af6c878d257178": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "52cc01bdae32400b9f56c737f4d4f2c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "info",
            "description": "Anime Artists",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_98ce06aeed3241c7b83745c09667b305",
            "style": "IPY_MODEL_21fd17da1e5a4f79962c87fb6d645443",
            "tooltip": ""
          }
        },
        "98ce06aeed3241c7b83745c09667b305": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21fd17da1e5a4f79962c87fb6d645443": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "beb3bfcfdd904e01b8692714b4ddbdea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "High Definition Words",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_10a529398263461e8f805245d2874e15",
            "style": "IPY_MODEL_67f00a986ca241fea9fc133d0e08e9a5",
            "tooltip": ""
          }
        },
        "10a529398263461e8f805245d2874e15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67f00a986ca241fea9fc133d0e08e9a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "96e3727d666f45739e93e4b34b6b4eba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "info",
            "description": "Art Styles",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_eb59dfdd5f3e45bb987c42e24dd2977e",
            "style": "IPY_MODEL_3c53751d212a4b958a5967dd93cbab89",
            "tooltip": ""
          }
        },
        "eb59dfdd5f3e45bb987c42e24dd2977e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c53751d212a4b958a5967dd93cbab89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "74a486bfc017495dbcc60171a74ac121": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "warning",
            "description": "Remix",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_cb68144c40164881a84b0d674ac533b6",
            "style": "IPY_MODEL_07668ff6ec5e45559c004d5cd5f99a1b",
            "tooltip": ""
          }
        },
        "cb68144c40164881a84b0d674ac533b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07668ff6ec5e45559c004d5cd5f99a1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}